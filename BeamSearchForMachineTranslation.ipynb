{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeamSearch for seq2seq model in keras that translates english to german"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After [implementing Bytepairencoding](BytepairencodingForMachineTranslation.ipynb), I'll now optimize the decoding inference mechanism. Instead of always taking the most likely next symbol when decoding, BeamSearch keeps a candidate list of `beam_width` best translations so far and expands them all for the next symbol and takes from those new candidates again the best `beam_width` ones. I will implement it by scratch in python here. It would be an alternative to use the BeamSearch method from `tensorflow` (and probably use `tf.keras` instead of `Keras`). As my main purpose here is to go step by step on my own, I'll follow the educational approach to do it myself and use https://gist.github.com/udibr/67be473cf053d8c38730 as a template.\n",
    "\n",
    "As trainings set I use the [European Parliament Proceedings Parallel Corpus 1996-2011](http://statmt.org/europarl/) German-English corpus with medium sized sentences.\n",
    "\n",
    "I've refactored the code a bit (putting the seq2seq model and utils into own packages), so here we can focus on the beam search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T05:26:32.144051Z",
     "start_time": "2018-05-19T05:26:30.414533Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janek/.local/share/virtualenvs/rosetta-WKmHhL03/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    " \n",
    "import bytepairencoding as bpe\n",
    "import seq2seq\n",
    "from utils.download import download_and_extract_resources\n",
    "from utils.linguistic import bleu_scores_europarl, read_europarl, preprocess_input_europarl as preprocess\n",
    "\n",
    "\n",
    "# Fixing random state ensure reproducible results\n",
    "RANDOM_STATE=42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "pd.set_option('max_colwidth', 60)  # easier to read texts in e.g. df.head()\n",
    "\n",
    "# technical detail so that an instance (maybe running in a different window)\n",
    "# doesn't take all the GPU memory resulting in some strange error messages\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T05:26:32.148091Z",
     "start_time": "2018-05-19T05:26:32.145434Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 50\n",
    "MAX_TARGET_LENGTH = 65\n",
    "LATENT_DIM = 512\n",
    "EMBEDDING_DIM = 300\n",
    "BPE_MERGE_OPERATIONS = 5_000  # I'd love to use 10_000 x 300, but this one is broken: https://github.com/bheinzerling/bpemb/issues/6\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "DROPOUT = 0.5\n",
    "TEST_SIZE = 500\n",
    "EMBEDDING_TRAINABLE = True  # Improves results significant and for at least it's not the most dominant training time factor (that's the output softmax layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T05:26:32.157166Z",
     "start_time": "2018-05-19T05:26:32.149629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n",
      "en.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "en.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (6.2 MB)\n",
      "de.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "de.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (5.7 MB)\n",
      "bytepairencoding_model_weights.h5 already downloaded (31.5 MB)\n",
      "bytepairencoding_inference_encoder_model_weights.h5 already downloaded (10.0 MB)\n",
      "bytepairencoding_inference_decoder_model_weights.h5 already downloaded (21.5 MB)\n"
     ]
    }
   ],
   "source": [
    "PATH = 'data'\n",
    "INPUT_LANG = 'en'\n",
    "TARGET_LANG = 'de'\n",
    "LANGUAGES = [INPUT_LANG, TARGET_LANG]\n",
    "BPE_URL = {lang: f'http://cosyne.h-its.org/bpemb/data/{lang}/' for lang in LANGUAGES}\n",
    "BPE_MODEL_NAME = {lang: f'{lang}.wiki.bpe.op{BPE_MERGE_OPERATIONS}.model' for lang in LANGUAGES}\n",
    "BPE_WORD2VEC_NAME = {lang: f'{lang}.wiki.bpe.op{BPE_MERGE_OPERATIONS}.d{EMBEDDING_DIM}.w2v.bin' for lang in LANGUAGES}\n",
    "\n",
    "EXTERNAL_RESOURCES = {\n",
    "    # Europarl Corpus\n",
    "    'de-en.tgz': 'http://statmt.org/europarl/v7/de-en.tgz',\n",
    "    \n",
    "    # Bytepairencoding subwords (_MODEL_) and pretrained embeddings (_WORD2VEC_)\n",
    "    BPE_MODEL_NAME[INPUT_LANG]: f'{BPE_URL[INPUT_LANG]}/{BPE_MODEL_NAME[INPUT_LANG]}',\n",
    "    BPE_WORD2VEC_NAME[INPUT_LANG] + '.tar.gz': f'{BPE_URL[INPUT_LANG]}/{BPE_WORD2VEC_NAME[INPUT_LANG]}' + '.tar.gz',\n",
    "    BPE_MODEL_NAME[TARGET_LANG]: f'{BPE_URL[TARGET_LANG]}/{BPE_MODEL_NAME[TARGET_LANG]}',\n",
    "    BPE_WORD2VEC_NAME[TARGET_LANG] + '.tar.gz': f'{BPE_URL[TARGET_LANG]}/{BPE_WORD2VEC_NAME[TARGET_LANG]}' + '.tar.gz',\n",
    "    \n",
    "    # Bytepairencoded model weights from BytepairencodingForMachineTranslation.ipynb\n",
    "    'bytepairencoding_model_weights.h5': 'https://drive.google.com/open?id=1xK2QVTsIpJLmphSEUZl1Unqmz85MYeQK',\n",
    "    'bytepairencoding_inference_encoder_model_weights.h5': 'https://drive.google.com/open?id=115Kp7ZIMqxu6YDvk4RhjvfYShcQdRP_o',\n",
    "    'bytepairencoding_inference_decoder_model_weights.h5': 'https://drive.google.com/open?id=1_e3DE5lDw10joIb83UFbzGJyvQrMfb8w',\n",
    "}\n",
    "\n",
    "download_and_extract_resources(fnames_and_urls=EXTERNAL_RESOURCES, dest_path=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T05:26:34.178318Z",
     "start_time": "2018-05-19T05:26:32.158619Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\n",
    "    'input_texts': read_europarl(INPUT_LANG),\n",
    "    'target_texts': read_europarl(TARGET_LANG)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T05:26:35.097484Z",
     "start_time": "2018-05-19T05:26:34.181589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr total input: 1920209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resumption of the session</td>\n",
       "      <td>wiederaufnahme der sitzungsperiode</td>\n",
       "      <td>25</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i declare resumed the session of the european parliament...</td>\n",
       "      <td>ich erkläre die am freitag, dem 0. dezember unterbrochen...</td>\n",
       "      <td>203</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>although, as you will have seen, the dreaded 'millennium...</td>\n",
       "      <td>wie sie feststellen konnten, ist der gefürchtete \"millen...</td>\n",
       "      <td>191</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you have requested a debate on this subject in the cours...</td>\n",
       "      <td>im parlament besteht der wunsch nach einer aussprache im...</td>\n",
       "      <td>105</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in the meantime, i should like to observe a minute' s si...</td>\n",
       "      <td>heute möchte ich sie bitten - das ist auch der wunsch ei...</td>\n",
       "      <td>232</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input_texts  \\\n",
       "0                                    resumption of the session   \n",
       "1  i declare resumed the session of the european parliament...   \n",
       "2  although, as you will have seen, the dreaded 'millennium...   \n",
       "3  you have requested a debate on this subject in the cours...   \n",
       "4  in the meantime, i should like to observe a minute' s si...   \n",
       "\n",
       "                                                  target_texts  input_length  \\\n",
       "0                           wiederaufnahme der sitzungsperiode            25   \n",
       "1  ich erkläre die am freitag, dem 0. dezember unterbrochen...           203   \n",
       "2  wie sie feststellen konnten, ist der gefürchtete \"millen...           191   \n",
       "3  im parlament besteht der wunsch nach einer aussprache im...           105   \n",
       "4  heute möchte ich sie bitten - das ist auch der wunsch ei...           232   \n",
       "\n",
       "   target_length  \n",
       "0             34  \n",
       "1            217  \n",
       "2            185  \n",
       "3            110  \n",
       "4            217  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Nr total input:\", len(df))\n",
    "df['input_length'] = df.input_texts.apply(len)\n",
    "df['target_length'] = df.target_texts.apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter translations (only sentences shorter than a given length)\n",
    "\n",
    "With a full working machine translation system, it's of course better to train on all data (plus maybe some augmented data). Without attention (and maybe copy mechanism, dynamic memory, ...) there's no point anyway in it, but it also reduces training time (a full training on ~2 Mio translations might take days, even with a good GPU).\n",
    "I use different length for input (english) than target (german) language as german is more verbose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T05:26:35.259615Z",
     "start_time": "2018-05-19T05:26:35.099607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences with length between (1, input=50/target=65) characters: 167211\n"
     ]
    }
   ],
   "source": [
    "non_empty = (df.input_length > 1) & (df.target_length > 1)  # there are empty phrases like '\\n' --> 'Frau Präsidentin\\n'\n",
    "short_inputs = (df.input_length < MAX_INPUT_LENGTH) & (df.target_length < MAX_TARGET_LENGTH)\n",
    "print(f'Sentences with length between (1, input={MAX_INPUT_LENGTH}/target={MAX_TARGET_LENGTH}) characters:', sum(non_empty & short_inputs))\n",
    "df = df[non_empty & short_inputs]\n",
    "gc.collect();  # df with filtered sentences is significant smaller, so time to garbage collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load (pretrained) Bytepairs\n",
    "\n",
    "I need the subwords dictionary (in `BPE_WORD2VEC_NAME`), the pretrained embeddings (in `BPE_MODEL_NAME`) and a [sentencepiece](https://github.com/google/sentencepiece) handler that can encode/decode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T05:26:35.390134Z",
     "start_time": "2018-05-19T05:26:35.261263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English subwords ['▁this', '▁is', '▁a', '▁test', '▁for', '▁pre', 'tr', 'ained', '▁by', 'te', 'pa', 'ire', 'm', 'bed', 'd', 'ings']\n",
      "German subwords ['▁d', 'as', '▁is', 't', '▁e', 'in', '▁test', '▁f', 'ür', '▁v', 'ort', 'rain', 'ier', 'te', '▁ze', 'ic', 'hen', 'gr', 'up', 'p', 'en']\n"
     ]
    }
   ],
   "source": [
    "bpe_input, bpe_target = [bpe.Bytepairencoding(\n",
    "    word2vec_fname=os.path.join(PATH, BPE_WORD2VEC_NAME[lang]),\n",
    "    sentencepiece_fname=os.path.join(PATH, BPE_MODEL_NAME[lang]),\n",
    ") for lang in [INPUT_LANG, TARGET_LANG]] \n",
    "print(\"English subwords\", bpe_input.sentencepiece.EncodeAsPieces(\"this is a test for pretrained bytepairembeddings\"))\n",
    "print(\"German subwords\", bpe_input.sentencepiece.EncodeAsPieces(\"das ist ein test für vortrainierte zeichengruppen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T05:26:42.701239Z",
     "start_time": "2018-05-19T05:26:35.391785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_sequences</th>\n",
       "      <th>target_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 344, 146, 498, 90, 6, 3, 3235, 90, 2]</td>\n",
       "      <td>[1, 247, 351, 750, 5, 934, 43, 3158, 4762, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1, 3005, 416, 77, 359, 4, 241, 4, 17, 76, 451, 782, 21,...</td>\n",
       "      <td>[1, 241, 156, 72, 3112, 54, 4, 39, 26, 95, 4739, 89, 937...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[1, 29, 140, 414, 3231, 8, 3106, 2484, 9, 451, 782, 21, ...</td>\n",
       "      <td>[1, 35, 2444, 2269, 2109, 625, 39, 26, 95, 4739, 89, 937...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[1, 1599, 134, 546, 4, 19, 9, 918, 6, 535, 5, 2]</td>\n",
       "      <td>[1, 1161, 2266, 52, 4, 132, 2232, 1516, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[1, 1599, 134, 546, 4, 19, 9, 918, 6, 535, 5, 2]</td>\n",
       "      <td>[1, 1161, 2266, 52, 4, 132, 2232, 1516, 3, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input_sequences  \\\n",
       "0                     [1, 344, 146, 498, 90, 6, 3, 3235, 90, 2]   \n",
       "5   [1, 3005, 416, 77, 359, 4, 241, 4, 17, 76, 451, 782, 21,...   \n",
       "6   [1, 29, 140, 414, 3231, 8, 3106, 2484, 9, 451, 782, 21, ...   \n",
       "7              [1, 1599, 134, 546, 4, 19, 9, 918, 6, 535, 5, 2]   \n",
       "13             [1, 1599, 134, 546, 4, 19, 9, 918, 6, 535, 5, 2]   \n",
       "\n",
       "                                               target_sequences  \n",
       "0                 [1, 247, 351, 750, 5, 934, 43, 3158, 4762, 2]  \n",
       "5   [1, 241, 156, 72, 3112, 54, 4, 39, 26, 95, 4739, 89, 937...  \n",
       "6   [1, 35, 2444, 2269, 2109, 625, 39, 26, 95, 4739, 89, 937...  \n",
       "7                 [1, 1161, 2266, 52, 4, 132, 2232, 1516, 3, 2]  \n",
       "13                [1, 1161, 2266, 52, 4, 132, 2232, 1516, 3, 2]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now encode the texts into sequences of indexes of bytepairs\n",
    "df['input_sequences'] = df.input_texts.apply(bpe_input.subword_indices)\n",
    "df['target_sequences'] = df.target_texts.apply(bpe_target.subword_indices)\n",
    "df[['input_sequences', 'target_sequences']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T05:26:42.782032Z",
     "start_time": "2018-05-19T05:26:42.703228Z"
    }
   },
   "outputs": [],
   "source": [
    "# Those will be the inputs for the seq2seq model (that needs to know how long the sequences can get)\n",
    "max_len_input = df.input_sequences.apply(len).max()\n",
    "max_len_target = df.target_sequences.apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T05:26:42.790560Z",
     "start_time": "2018-05-19T05:26:42.783696Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(df.shape[0]), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T05:26:44.164566Z",
     "start_time": "2018-05-19T05:26:42.792236Z"
    }
   },
   "outputs": [],
   "source": [
    "s2s = seq2seq.Seq2SeqWithBPE(\n",
    "    bpe_input=bpe_input,\n",
    "    bpe_target=bpe_target,\n",
    "    max_len_input=max_len_input,\n",
    "    max_len_target=max_len_target\n",
    ")\n",
    "s2s.model.load_weights(os.path.join(PATH, 'bytepairencoding_model_weights.h5'))\n",
    "s2s.inference_encoder_model.load_weights(os.path.join(PATH, 'bytepairencoding_inference_encoder_model_weights.h5'))\n",
    "s2s.inference_decoder_model.load_weights(os.path.join(PATH, 'bytepairencoding_inference_decoder_model_weights.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode Beam Search implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T05:26:44.179261Z",
     "start_time": "2018-05-19T05:26:44.166329Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_beam_search(input_seq, beam_width):\n",
    "    initial_states = s2s.inference_encoder_model.predict(input_seq)\n",
    "    \n",
    "    top_candidates = [{\n",
    "        'states': initial_states,\n",
    "        'idx_sequence': [bpe_target.start_token_idx],\n",
    "        'token_sequence': [bpe_target.start_token],\n",
    "        'score': 0.0,\n",
    "        'live': True\n",
    "    }]\n",
    "    live_k = 1\n",
    "    dead_k = 0\n",
    "    \n",
    "    for _ in range(max_len_target):\n",
    "        if not(live_k and dead_k < beam_width):\n",
    "            break\n",
    "        new_candidates = []\n",
    "        for candidate in top_candidates:\n",
    "            if not candidate['live']:\n",
    "                new_candidates.append(candidate)\n",
    "                continue\n",
    "         \n",
    "            target_seq = np.zeros((1, max_len_target - 1))\n",
    "            target_seq[0, 0] = candidate['idx_sequence'][-1]\n",
    "            output, states = s2s.inference_decoder_model.predict(\n",
    "                [target_seq, candidate['states']]\n",
    "            )\n",
    "            probs = output[0, 0, :]\n",
    "        \n",
    "            for idx in np.argsort(-probs)[:beam_width]:\n",
    "                new_candidates.append({\n",
    "                    'states': states,\n",
    "                    'idx_sequence': candidate['idx_sequence'] + [idx],\n",
    "                    'token_sequence': candidate['token_sequence'] + [bpe_target.tokens[idx]],\n",
    "                    # sum -log(prob) numerical more stable than to multiplate probs                    \n",
    "                    # goal now to minimize the score\n",
    "                    'score': candidate['score'] - np.log(probs[idx]),  \n",
    "                    'live': idx != bpe_target.stop_token_idx,\n",
    "                })\n",
    "        \n",
    "        top_candidates = sorted(\n",
    "            new_candidates, key=lambda c: c['score']\n",
    "        )[:beam_width]\n",
    "        \n",
    "        alive = np.array([c['live'] for c in top_candidates])\n",
    "        live_k = sum(alive == True)\n",
    "        dead_k = sum(alive == False)\n",
    "        \n",
    "    return bpe_target.sentencepiece.DecodePieces(top_candidates[0]['token_sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T05:26:44.186351Z",
     "start_time": "2018-05-19T05:26:44.181390Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence, beam_width=5):\n",
    "    return decode_beam_search(pad_sequences(\n",
    "        [bpe_input.subword_indices(preprocess(sentence))],\n",
    "        padding='post',\n",
    "        maxlen=max_len_input,\n",
    "    ), beam_width=beam_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter evaluation for beam_width and branch_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T06:13:07.545716Z",
     "start_time": "2018-05-19T05:26:44.187996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150489 training size\n",
      "500 validation size (hyperparam search)\n",
      "500 test size (for final BLEU score)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e30e2d6e5942b485cf58702de670bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "beam_width=1 average BLEU = 0.30830853189276136\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76912dbb6b8d4dbda63da71eea3b2271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "beam_width=2 average BLEU = 0.31319166873708537\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e901e8447b4280934048586cbef31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "beam_width=3 average BLEU = 0.3103411830919007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c0cccf680648b886394c7bfe343a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "beam_width=5 average BLEU = 0.31256598297129945\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd23682609a84a25bcf24e31fe45db32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "beam_width=10 average BLEU = 0.31267283678076774\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f260e4a777104c24a528fdc6ec7cddb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "beam_width=20 average BLEU = 0.31135477696157376\n"
     ]
    }
   ],
   "source": [
    "val_ids, test_ids = val_ids[-TEST_SIZE:], val_ids[:TEST_SIZE]\n",
    "print(len(train_ids), 'training size')\n",
    "print(len(val_ids), 'validation size (hyperparam search)')\n",
    "print(len(test_ids), 'test size (for final BLEU score)')\n",
    "\n",
    "widths = [1, 2, 3, 5, 10, 20]\n",
    "for beam_width in widths:\n",
    "    bleu = bleu_scores_europarl(\n",
    "        input_texts=df.input_texts.iloc[val_ids],\n",
    "        target_texts=df.target_texts.iloc[val_ids],\n",
    "        predict=lambda text: predict(text, beam_width=beam_width)\n",
    "    )\n",
    "    print(f'beam_width={beam_width} average BLEU = {bleu.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T06:13:13.889841Z",
     "start_time": "2018-05-19T06:13:07.547689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello.' --> 'hallam.'\n",
      "'you are welcome.' --> 'seien sie willkommen.'\n",
      "'how do you do?' --> 'was tun sie?'\n",
      "'i hate mondays.' --> 'ich habe mir mißverständnisse.'\n",
      "'i am a programmer.' --> 'ich bin ein programm.'\n",
      "'data is the new oil.' --> 'daten sind die öls.'\n",
      "'it could be worse.' --> 'das könnte besorgniserregend sein.'\n",
      "'i am on top of it.' --> 'ich gehe davon aus.'\n",
      "'n° uno' --> 'änderungsantrag 0'\n",
      "'awesome!' --> 'einverstanden!'\n",
      "'put your feet up!' --> 'fangen sie hart!'\n",
      "'from the start till the end!' --> 'mit dem zielsetzungen!'\n",
      "'from dusk till dawn.' --> 'von einem dialog.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T06:13:26.497797Z",
     "start_time": "2018-05-19T06:13:13.891681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original \"please rise, then, for this minute' s silence.\", got 'bitte lassen sie mich das wort äußern.', exp: 'ich bitte sie, sich zu einer schweigeminute zu erheben.'\n",
      "Original \"(the house rose and observed a minute' s silence)\", got '(das parlament erhebt sich zu einer schweigeminute.)', exp: '(das parlament erhebt sich zu einer schweigeminute.)'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'thank you, mr segni, i shall do so gladly.', got 'vielen dank, herr segni.', exp: 'vielen dank, herr segni, das will ich gerne tun.'\n",
      "Original 'it is the case of alexander nikitin.', got 'das ist der fall von alexander nikitin.', exp: 'das ist der fall von alexander nikitin.'\n",
      "Original 'it will, i hope, be examined in a positive light.', got 'ich hoffe, dass dort hineingekommen wird.', exp: 'ich hoffe, daß dort in ihrem sinne entschieden wird.'\n",
      "Original 'why are there no fire instructions?', got 'warum gibt es keine brandschutzbelehrungen?', exp: 'warum finden keine brandschutzbelehrungen statt?'\n",
      "Original 'mr berenguer fuster, we shall check all this.', got 'herr berriege, wir werden dies prüfen.', exp: 'lieber kollege, wir werden das prüfen.'\n",
      "Original 'we do not know what is happening.', got 'wir wissen nicht, was geschieht.', exp: 'wir wissen nicht, was passiert.'\n",
      "Original 'agenda', got 'tagesordnung', exp: 'arbeitsplan'\n",
      "Original 'relating to wednesday:', got 'zum mittwoch:', exp: 'zum mittwoch:'\n",
      "Original '(applause from the pse group)', got '(beifall von der pse-fraktion)', exp: '(beifall der pse-fraktion)'\n",
      "Original 'mr hänsch represented you on this occasion.', got 'herr seppänen hat sie vertreten.', exp: 'der kollege hänsch hat sie dort vertreten.'\n",
      "Original 'we then put it to a vote.', got 'wir haben dann zustimmen.', exp: 'wir haben dann abgestimmt.'\n",
      "Original 'there was a vote on this matter.', got 'es gab eine abstimmung.', exp: 'es gab eine abstimmung zu diesem punkt.'\n",
      "Original 'all of the others were of a different opinion.', got 'alle anderen waren anderer meinung.', exp: 'alle anderen waren anderer meinung.'\n",
      "Original 'that was the decision.', got 'das war die entscheidung.', exp: 'das war der beschluß.'\n",
      "Original 'i should now like to comment on the issue itself.', got 'jetzt möchte ich mich ansprechen.', exp: 'jetzt möchte ich zur sache selbst etwas sagen.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T06:13:39.053441Z",
     "start_time": "2018-05-19T06:13:26.499324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'a lot of inspiration is needed.', got 'eine vielversprechnung ist erforderlich.', exp: 'diese denkanstöße sind dringend nötig.'\n",
      "Original 'so what was cancún about?', got 'was ist also in cancún?', exp: 'worum ging es in cancún?'\n",
      "Original 'i now turn to another subject.', got 'ich komme nun zu einem weiteren punkt.', exp: 'ich komme jetzt zu einem anderen punkt.'\n",
      "Original 'thank you, mr prodi.', got 'vielen dank, herr prodi.', exp: 'vielen dank, herr prodi.'\n",
      "Original 'the experience acquired varied somewhat.', got 'die erfahrungen haben etwas ähnliches.', exp: 'die damit gemachten erfahrungen sind recht unterschiedlich.'\n",
      "Original 'that is what mr smith suggested.', got 'das hat smith gesagt.', exp: 'das hat herr smith vorgeschlagen.'\n",
      "Original 'it is stable internally and externally.', got 'das ist in der tat exprimierend.', exp: 'er ist stabil nach innen und außen.'\n",
      "Original 'mr president, i am wearing my irish scarf today.', got 'herr präsident, ich möchte meinen irischen bedauern.', exp: 'herr präsident, ich trage heute mein irisches tuch.'\n",
      "Original 'it will not lead to any savings.', got 'es wird keinen beitrag leisten.', exp: 'es werden keine einsparungen eintreten.'\n",
      "Original '(applause)', got '(beifall)', exp: '(beifall)'\n",
      "Original 'it will be noted in the minutes.', got 'das wird im protokoll aufgenommen.', exp: 'wir nehmen dies zu protokoll.'\n",
      "Original 'we need another great leap forward.', got 'wir müssen schneller voranbringen.', exp: 'was wir wieder brauchen, ist ein großer sprung.'\n",
      "Original 'that is the real problem.', got 'das ist das eigentliche problem.', exp: 'das ist das eigentliche problem.'\n",
      "Original '(the sitting was closed at 0.0 p.m.)', got '(die sitzung wird um 0.0 uhr geschlossen.)', exp: '(die sitzung wird um 0.0 uhr geschlossen.)0'\n",
      "Original 'how committed are you?', got 'wie stehen sie verpflichtet?', exp: 'wie engagiert sind sie wirklich?'\n",
      "Original 'what was lacking was the necessary legal force.', got 'was fehlte gelten, war notwendig.', exp: 'was fehlte, waren die gesetzlichen auflagen.'\n",
      "Original 'each of the speakers will have one minute.', got 'jede minute einer minute werden das wort ergreifen.', exp: 'jeder sprecher hat eine minute zeit.'\n",
      "Original 'but that is not enough.', got 'aber das ist nicht genug.', exp: 'aber das ist nicht genug.'\n",
      "Original 'this seems to me to be a workable solution.', got 'das halte ich für eine echte lösung.', exp: 'ich halte dieses vorgehen für angemessen.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on validation set\n",
    "test_df = df.iloc[test_ids]\n",
    "for en, de in test_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T06:19:15.800891Z",
     "start_time": "2018-05-19T06:13:39.055068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce56cbd826542aea3c8eeeaf1e776a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.32849784166287943\n"
     ]
    }
   ],
   "source": [
    "bleu = bleu_scores_europarl(\n",
    "    input_texts=df.input_texts.iloc[test_ids],\n",
    "    target_texts=df.target_texts.iloc[test_ids],\n",
    "    predict=lambda text: predict(text)\n",
    ")\n",
    "print(f'average BLEU on test set = {bleu.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.995932Z",
     "start_time": "2018-05-13T18:17:20.994111Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The texts feel more readable, allthough the BLEU score rises up only a bit ($0.328 > $0.316$).\n",
    "A lot of the problems in the translations certainly depend on the still small training set (~200k), so as next step, I'll train on a bigger sub-corpus of longer texts. This will also make the need to use an attention model more clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 294,
   "position": {
    "height": "40px",
    "left": "553px",
    "right": "192px",
    "top": "132px",
    "width": "615px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
