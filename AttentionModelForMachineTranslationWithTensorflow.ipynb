{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on large dataset with attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing [Beamsearch on a large dataset](BeamSearchOnLargeDataset.ipynb), I'll now add an attention model.\n",
    "As trainings set I use the [European Parliament Proceedings Parallel Corpus 1996-2011](http://statmt.org/europarl/).\n",
    "\n",
    "I first intented to implement it also with `Keras`. First of all, there is no built-in implementation of an attention layer or an attention decoder (it's planned atm). There are several projects like [keras-attention](https://github.com/datalogue/keras-attention) or a bit modified [monitonic-keras-attention](https://github.com/andreyzharkov/keras-monotonic-attention) (that works really better). Also there is [seq2seq project](https://github.com/farizrahman4u/seq2seq) that as lot of issues open. And a promising looking [NMT Keras](https://nmt-keras.readthedocs.io/en/latest/) that failed to install all dependency. I wouldn't mind a reimplemetation on my own (or improving one of these), just as I do the project anyway for learning purposes. After a while I really found this approach disturbing. I don't like switching around between the real high level layers of `Keras` down to `keras.backend` when I pretty much have to low level implement everything (in a different way to usual `Keras`) and in addition everything in object oriented extension of Layers, Cell and so on (with passing all parameters along, as GRU/LSTM have to be changed they also have to be reimplementend, vectorization with own time distributed layers, the masking layer won't work with further inputs like the weighted context vector, so we'll implement also Masking, ...). Just look into the projects and there is a lot of noisy code inside detracting from the original algorithm. \n",
    "\n",
    "Another idea is to use `tf.keras` instead of `Keras`. Look at the approach taken [here](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb?linkId=53292082#scrollTo=AOpGoE2T-YXS) - I found it after I've implemented this notebook, but on a first glance, it looks short and elegant. The advantage of using the tensorflow reimplementation `tf.keras` instead of `Keras` might be that we don't need to with `keras.backend` and can work directly on the computation graph with just the inputs and output of the `tf.keras.layers`. My suggestion would be that for new projects using `tf.keras` from the start can make things easier later.\n",
    "\n",
    "Attention as basic idea is pretty simple: While decoding, we'll look back to weighted (what has to be learned) encoded states where the weights depend on the current decoding position, the previous (or current) hidden state of the decoder, and maybe to the previous alignment. We create a contect vector of them and use it also as an input (beside the last generated token) to the decoder. For performance we might only look to local hidden states around an alignment prediction (that can linear in the simplest form or usually als learnable). That's not so tough to represent as direct computation, but as we never work in Keras with the computation graph directly, it's harder than it should be. As technical detail, usually we don't take the context vector as another input (as we would have to combine inputs of different dimensions), but combine the context vector with the hidden state of the decoder to a modified hidden state. That's why we'd also change a lot in Keras.\n",
    "\n",
    "With tensorflow we're closer to research and as I anyway intended to use multiple frameworks, I'll follow now the [seq2seq tutorial from tensorflow](https://www.tensorflow.org/tutorials/seq2seq). So, in this notebook there will be also a tensorflow implementation of the raw seq2seq model and Beam Search. I also refactored all the preparation work into a module.\n",
    "\n",
    "Here, I will work with Luong Attention as it is the proposal from the tensorflow seq2seq tutorial and if I understand it right, it is very connected to dynamic neuronal network memories. I plan to investigate it later separate. There shouldn't be anyway much difference in results to Bahdanau and in case it's easy to switch to another attention mechnism in tensorflow (just change the class name). I didn't implement local allignments following a [discussion from a tensorflow feature request](https://github.com/tensorflow/tensorflow/issues/10842).\n",
    "\n",
    "I still use a bidirectional encoder based on GRUs and a decoder network with GRUs with the same hyperparameters as in [BeamSearchOnLargeDataset with Keras](BeamSearchOnLargeDataset.ipynb) used. I don't use much of the most modern features of tensorflow (eager execution or tensorflow.data API or flags for hyperparameters or plug in summaries and use tensorbard in addition). I intend to try them out later. Here, I tried to work with as little as possible magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T07:52:36.852381Z",
     "start_time": "2018-06-18T07:52:36.849149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14165716698337561091\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7692746752\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14130295096720600696\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Check that there is a GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T15:38:45.738825Z",
     "start_time": "2018-06-17T15:38:45.721521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "# Check that Cuda/Cudnn/GPU works as intended\n",
    "import tensorflow as tf\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T15:38:46.540712Z",
     "start_time": "2018-06-17T15:38:45.740267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed random seed to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils.download import download_and_extract_resources\n",
    "from utils.linguistic import bleu_scores_europarl, preprocess_input_europarl as preprocess\n",
    "from utils.preparation import Europarl, RANDOM_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T15:38:46.544569Z",
     "start_time": "2018-06-17T15:38:46.541938Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 100\n",
    "MAX_TARGET_LENGTH = 125\n",
    "LATENT_DIM =  256  # was 512, but we should be able to use a smaller hidden representation as we are looking back anyway as needed\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128  # was 64, but tensorflow implementation doesn't need so much GPU memory so can increase batch size\n",
    "DROPOUT = 0.25  # Dropout on input and output for the RNN cells, so effective dropout is 0.5, but works slightly better so\n",
    "TEST_SIZE = 2500\n",
    "BEAM_WIDTH = 5\n",
    "EMBEDDING_TRAINABLE = True  # Improves results significant and for at least it's not the most dominant training time factor (that's the output softmax layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T15:38:46.556294Z",
     "start_time": "2018-06-17T15:38:46.546189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n",
      "en.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "en.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (6.2 MB)\n",
      "de.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "de.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (5.7 MB)\n"
     ]
    }
   ],
   "source": [
    "europarl = Europarl()\n",
    "download_and_extract_resources(fnames_and_urls=europarl.external_resources, dest_path=europarl.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T15:39:35.637601Z",
     "start_time": "2018-06-17T15:38:46.557829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unfiltered translations 1920209\n",
      "Filtered translations with length between (1, input=100/target=125) characters: 597331\n"
     ]
    }
   ],
   "source": [
    "europarl.load_and_preprocess(max_input_length=MAX_INPUT_LENGTH, max_target_length=MAX_TARGET_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T15:39:35.667525Z",
     "start_time": "2018-06-17T15:39:35.639253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "      <th>input_sequences</th>\n",
       "      <th>target_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resumption of the session</td>\n",
       "      <td>wiederaufnahme der sitzungsperiode</td>\n",
       "      <td>25</td>\n",
       "      <td>34</td>\n",
       "      <td>[1, 344, 146, 498, 90, 6, 3, 3235, 90, 2]</td>\n",
       "      <td>[1, 247, 351, 750, 5, 934, 43, 3158, 4762, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>please rise, then, for this minute' s silence.</td>\n",
       "      <td>ich bitte sie, sich zu einer schweigeminute zu...</td>\n",
       "      <td>46</td>\n",
       "      <td>55</td>\n",
       "      <td>[1, 3005, 416, 77, 359, 4, 241, 4, 17, 76, 451...</td>\n",
       "      <td>[1, 241, 156, 72, 3112, 54, 4, 39, 26, 95, 473...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(the house rose and observed a minute' s silence)</td>\n",
       "      <td>(das parlament erhebt sich zu einer schweigemi...</td>\n",
       "      <td>49</td>\n",
       "      <td>52</td>\n",
       "      <td>[1, 29, 140, 414, 3231, 8, 3106, 2484, 9, 451,...</td>\n",
       "      <td>[1, 35, 2444, 2269, 2109, 625, 39, 26, 95, 473...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>madam president, on a point of order.</td>\n",
       "      <td>frau präsidentin, zur geschäftsordnung.</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>[1, 1599, 134, 546, 4, 19, 9, 918, 6, 535, 5, 2]</td>\n",
       "      <td>[1, 1161, 2266, 52, 4, 132, 2232, 1516, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>if the house agrees, i shall do as mr evans ha...</td>\n",
       "      <td>wenn das haus damit einverstanden ist, werde i...</td>\n",
       "      <td>58</td>\n",
       "      <td>86</td>\n",
       "      <td>[1, 530, 3, 414, 1434, 35, 4, 305, 186, 321, 3...</td>\n",
       "      <td>[1, 835, 19, 684, 494, 21, 161, 48, 838, 30, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_texts  \\\n",
       "0                           resumption of the session   \n",
       "5      please rise, then, for this minute' s silence.   \n",
       "6   (the house rose and observed a minute' s silence)   \n",
       "7               madam president, on a point of order.   \n",
       "12  if the house agrees, i shall do as mr evans ha...   \n",
       "\n",
       "                                         target_texts  input_length  \\\n",
       "0                  wiederaufnahme der sitzungsperiode            25   \n",
       "5   ich bitte sie, sich zu einer schweigeminute zu...            46   \n",
       "6   (das parlament erhebt sich zu einer schweigemi...            49   \n",
       "7             frau präsidentin, zur geschäftsordnung.            37   \n",
       "12  wenn das haus damit einverstanden ist, werde i...            58   \n",
       "\n",
       "    target_length                                    input_sequences  \\\n",
       "0              34          [1, 344, 146, 498, 90, 6, 3, 3235, 90, 2]   \n",
       "5              55  [1, 3005, 416, 77, 359, 4, 241, 4, 17, 76, 451...   \n",
       "6              52  [1, 29, 140, 414, 3231, 8, 3106, 2484, 9, 451,...   \n",
       "7              39   [1, 1599, 134, 546, 4, 19, 9, 918, 6, 535, 5, 2]   \n",
       "12             86  [1, 530, 3, 414, 1434, 35, 4, 305, 186, 321, 3...   \n",
       "\n",
       "                                     target_sequences  \n",
       "0       [1, 247, 351, 750, 5, 934, 43, 3158, 4762, 2]  \n",
       "5   [1, 241, 156, 72, 3112, 54, 4, 39, 26, 95, 473...  \n",
       "6   [1, 35, 2444, 2269, 2109, 625, 39, 26, 95, 473...  \n",
       "7       [1, 1161, 2266, 52, 4, 132, 2232, 1516, 3, 2]  \n",
       "12  [1, 835, 19, 684, 494, 21, 161, 48, 838, 30, 4...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T15:39:35.674912Z",
     "start_time": "2018-06-17T15:39:35.669469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English subwords ['▁this', '▁is', '▁a', '▁test', '▁for', '▁pre', 'tr', 'ained', '▁by', 'te', 'pa', 'ire', 'm', 'bed', 'd', 'ings']\n",
      "German subwords ['▁das', '▁ist', '▁ein', '▁test', '▁für', '▁v', 'ort', 'rain', 'ierte', '▁zeich', 'eng', 'ruppen']\n"
     ]
    }
   ],
   "source": [
    "print(\"English subwords\", europarl.bpe_input.sentencepiece.EncodeAsPieces(\"this is a test for pretrained bytepairembeddings\"))\n",
    "print(\"German subwords\", europarl.bpe_target.sentencepiece.EncodeAsPieces(\"das ist ein test für vortrainierte zeichengruppen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T15:39:35.935946Z",
     "start_time": "2018-06-17T15:39:35.676620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 71)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Those will be the inputs for the seq2seq model (that needs to know how long the sequences can get)\n",
    "max_len_input = europarl.df.input_sequences.apply(len).max()\n",
    "max_len_target = europarl.df.target_sequences.apply(len).max()\n",
    "(max_len_input, max_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T15:39:35.964313Z",
     "start_time": "2018-06-17T15:39:35.938174Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(europarl.df.shape[0]), test_size=0.1, random_state=RANDOM_STATE)  # fixed random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T15:39:41.198189Z",
     "start_time": "2018-06-17T15:39:35.966068Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    encoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_input\n",
    "        dtype=tf.int32,\n",
    "        name='encoder_inputs' \n",
    "    )\n",
    "    batch_size = tf.shape(encoder_inputs)[0]\n",
    "    beam_width = tf.placeholder_with_default(1, shape=[])\n",
    "    dropout = tf.placeholder_with_default(tf.cast(0.0, tf.float32), shape=[])\n",
    "    keep_prob = tf.cast(1.0, tf.float32) - dropout\n",
    "\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", \n",
    "        initializer=tf.constant(europarl.bpe_input.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder,\n",
    "        encoder_inputs,\n",
    "        name=\"encoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    input_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='input_sequence_length'\n",
    "    )\n",
    "    \n",
    "    rnn_cell_type = tf.nn.rnn_cell.GRUCell\n",
    "    encoder_forward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name='encoder_forward_cell'),\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,  # state_keep_prob not set as it was not helpful here\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    encoder_backward_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name='encoder_backward_cell'),\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    encoder_bi_outputs, encoder_bi_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        encoder_forward_cell, encoder_backward_cell,\n",
    "        inputs=encoder_emb_inp,\n",
    "        sequence_length=input_sequence_length,\n",
    "        time_major=False,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    encoder_outputs = tf.concat(encoder_bi_outputs, -1)\n",
    "    encoder_state = tf.concat(encoder_bi_state, -1)\n",
    "    \n",
    "    # Regarding time_major:\n",
    "    # If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "    # If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "    # Using `time_major = True` is a bit more efficient because it avoids\n",
    "    # transposes at the beginning and end of the RNN calculation.  However,\n",
    "    # most TensorFlow data is batch-major, so by default this function\n",
    "    # accepts input and emits output in batch-major form.\n",
    "    #\n",
    "    # for simplicity I work with batch major here instead of time_major\n",
    "    # so I don't need to transpose inputs and transpose back for attention mechanism\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_inputs' \n",
    "    )\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", \n",
    "        initializer=tf.constant(europarl.bpe_target.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder,\n",
    "        decoder_inputs,\n",
    "        name=\"decoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    target_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='target_sequence_length'\n",
    "    )\n",
    "    \n",
    "    # tiling is necessary to work with BeamSearchDecoder\n",
    "    # read carefully the NOTE on constructor in\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper \n",
    "    tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(encoder_outputs, multiplier=beam_width)\n",
    "    tiled_encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width)\n",
    "    tiled_sequence_length = tf.contrib.seq2seq.tile_batch(input_sequence_length, multiplier=beam_width)\n",
    "    \n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        LATENT_DIM,\n",
    "        memory=tiled_encoder_outputs,\n",
    "        memory_sequence_length=tiled_sequence_length,\n",
    "        dtype=tf.float32,\n",
    "        name='attention_mechanism',\n",
    "    )\n",
    "    decoder_rnn_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM, name='decoder_cell'),\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        decoder_rnn_cell,\n",
    "        attention_mechanism,\n",
    "        attention_layer_size=LATENT_DIM, \n",
    "        name='attention_wrapper',\n",
    "    )\n",
    "\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        inputs=decoder_emb_inp, \n",
    "        sequence_length=target_sequence_length,\n",
    "        time_major=False,\n",
    "        name=\"decoder_training_helper\",\n",
    "    )\n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        units=len(europarl.bpe_target.tokens),\n",
    "        use_bias=False,\n",
    "        name='projection_layer',\n",
    "    )\n",
    "    \n",
    "    initial_state=decoder_cell.zero_state(dtype=tf.float32, batch_size=batch_size).clone(\n",
    "        cell_state=encoder_state\n",
    "    )\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=decoder_cell,\n",
    "        helper=training_helper,\n",
    "        initial_state=initial_state,\n",
    "        output_layer=projection_layer,\n",
    "    )\n",
    "    outputs, _final_state, _final_sequence_length = tf.contrib.seq2seq.dynamic_decode(  \n",
    "        decoder,\n",
    "        output_time_major=False,\n",
    "        impute_finished=False,\n",
    "    )\n",
    "    logits = outputs.rnn_output\n",
    "    \n",
    "    decoder_outputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_outputs',\n",
    "    )\n",
    "    target_weights = tf.cast(tf.sequence_mask(target_sequence_length), dtype=tf.float32)\n",
    "    train_loss = tf.contrib.seq2seq.sequence_loss(logits, decoder_outputs, target_weights)\n",
    "\n",
    "    params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, params)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "        t_list=gradients,\n",
    "        clip_norm=1.,\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    update_step = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "    \n",
    "    inference_decoder_initial_state = decoder_cell.zero_state(\n",
    "        dtype=tf.float32,\n",
    "        batch_size=batch_size * beam_width  # tricky and somehow unintuitive, but necessary\n",
    "    ).clone(\n",
    "        cell_state=tiled_encoder_state\n",
    "    )\n",
    "    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "        cell=decoder_cell,\n",
    "        embedding=embedding_decoder,\n",
    "        start_tokens=tf.fill([batch_size], europarl.bpe_target.start_token_idx),\n",
    "        end_token=europarl.bpe_target.stop_token_idx,\n",
    "        initial_state=inference_decoder_initial_state,\n",
    "        beam_width=BEAM_WIDTH,\n",
    "        output_layer=projection_layer,\n",
    "        length_penalty_weight=1.0,  # https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/\n",
    "    )\n",
    "\n",
    "    \n",
    "    inference_outputs, _inference_final_state, _inference_final_sequence_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inference_decoder,\n",
    "        maximum_iterations=tf.round(tf.reduce_max(input_sequence_length) * 2),  # a bit more flexible than max_len_target\n",
    "        impute_finished=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T15:39:41.210055Z",
     "start_time": "2018-06-17T15:39:41.199363Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_train_batch(batch_ids):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    pred, loss, _ = sess.run(\n",
    "        fetches=[\n",
    "            outputs, train_loss, update_step\n",
    "        ],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "            dropout: DROPOUT,\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def run_val_batch(batch_ids):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    loss = sess.run(\n",
    "        fetches=[train_loss],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def run_validation_loss():\n",
    "    return np.mean([\n",
    "        run_val_batch(ids)\n",
    "        for ids \n",
    "        in np.array_split(val_ids, np.ceil(len(val_ids) / BATCH_SIZE))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T19:18:04.759742Z",
     "start_time": "2018-06-17T15:39:41.211370Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e801d85d88e540de829705999d9d503f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 3.2934992 val_loss 2.3617246\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cdfd79b1fe147e6a3141e0bfd71a5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.5020657 val_loss 2.1203256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6a88f3092744b7a70620c6e899b2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.3381367 val_loss 2.0130618\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb74f6db3b241088e87b01ec60c56d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.2496881 val_loss 1.9461743\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dda286fc22043a08dd1aed3bce9c338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.1938696 val_loss 1.9016857\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0326f0e5680470ab2a9635e326b581f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.1531553 val_loss 1.8689809\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa0081a9e0b4f34846ae2d41f89eed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.1221595 val_loss 1.8452276\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fc2239676444c4ae37f0d388c6aacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 8', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.0934803 val_loss 1.8202693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c25787c0a404567a053fdb52f7e085c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 9', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.069828 val_loss 1.8017997\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c28e9ba5eb448d869c1b35c054ce8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 10', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.0482225 val_loss 1.7801132\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb404739dd734aa48a98912923ccf0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 11', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.0254748 val_loss 1.7643902\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b785ceccb64a7cb291e6ae9aa42351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 12', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.0004952 val_loss 1.7432646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c762e6e4af0241a3a2d8f027c2aa1d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 13', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.9714832 val_loss 1.7145725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad42bc7959a54449b34bd6a204786df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 14', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.9321318 val_loss 1.6807121\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648eaaf7a438490aba68ce76b5b7ae5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 15', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.8866256 val_loss 1.6512736\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f5c01ef0e9443dace652222bee19d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 16', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.8547937 val_loss 1.6346791\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0447f88ee7f24aba8a840da842bec27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 17', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.8338715 val_loss 1.6228687\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0e64392dfd44d1a77171190ad270fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 18', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.8179969 val_loss 1.608734\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d03b13428842f98c379a2a79b99cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 19', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.8050221 val_loss 1.6028578\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddcf60d83e54c6e879019e313f063b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 20', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.7941648 val_loss 1.5963621\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(\n",
    "    allow_soft_placement=True,  # needed as recommendation from https://github.com/tensorflow/tensorflow/issues/2292\n",
    "    log_device_placement=True,\n",
    ")\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batches_per_epoch = np.ceil(len(train_ids) / BATCH_SIZE)\n",
    "for epoch in range(EPOCHS):\n",
    "    shuffled_ids = np.random.permutation(train_ids)\n",
    "    batch_splits = np.array_split(shuffled_ids, batches_per_epoch)\n",
    "    train_losses = []\n",
    "    N = len(batch_splits)\n",
    "    with tqdm(batch_splits, desc=f\"Epoch {epoch+1}\") as t:\n",
    "        for train_batch_ids in t:\n",
    "            batch_loss = run_train_batch(train_batch_ids)\n",
    "            train_losses.append(batch_loss)\n",
    "            t.set_postfix(train_loss=np.mean(train_losses))\n",
    "        print(\"train_loss\", np.mean(train_losses), \"val_loss\", run_validation_loss())\n",
    "        \n",
    "validation_input_sequences = europarl.df.input_sequences.iloc[val_ids[:BATCH_SIZE]]\n",
    "validation_input_lengths = validation_input_sequences.apply(len)\n",
    "\n",
    "validation_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    validation_input_sequences,\n",
    "    maxlen=max_len_input,\n",
    "    dtype=int,\n",
    "    padding='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T19:18:04.766165Z",
     "start_time": "2018-06-17T19:18:04.761356Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    sequenced = europarl.bpe_input.subword_indices(preprocess(sentence))\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [sequenced],\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    \n",
    "    beam_search_output = sess.run(\n",
    "        fetches=[inference_outputs],\n",
    "        feed_dict={\n",
    "            encoder_inputs: padded,\n",
    "            input_sequence_length: [len(sequenced)],\n",
    "            beam_width: BEAM_WIDTH,\n",
    "        }\n",
    "    )[0]\n",
    "    \n",
    "    return europarl.bpe_target.sentencepiece.DecodePieces([\n",
    "        europarl.bpe_target.tokens[idx] for idx in beam_search_output.predicted_ids[0, :, 0].tolist()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T19:18:05.508198Z",
     "start_time": "2018-06-17T19:18:04.768206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/tfattentionmodel.ckpt'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'tfattentionmodel'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, f\"data/{name}.ckpt\")\n",
    "# tfattentionmodel.ckpt.index https://drive.google.com/open?id=1Xv2Qc9Gnac_Of9bIpQef5LZJnX9ij933\n",
    "# tfattentionmodel.cpkt.meta https://drive.google.com/open?id=162WY8XEjyqfvitiIBfmq9oHLC1rQhyvr\n",
    "# tfattentionmodel.cpkt.data-00000-of-00001 https://drive.google.com/open?id=1racW0agvk5nJ_xBaxifAaEWM8T9ot7FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T19:18:05.526233Z",
     "start_time": "2018-06-17T19:18:05.510282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "      <th>input_length</th>\n",
       "      <th>target_length</th>\n",
       "      <th>input_sequences</th>\n",
       "      <th>target_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resumption of the session</td>\n",
       "      <td>wiederaufnahme der sitzungsperiode</td>\n",
       "      <td>25</td>\n",
       "      <td>34</td>\n",
       "      <td>[1, 344, 146, 498, 90, 6, 3, 3235, 90, 2]</td>\n",
       "      <td>[1, 247, 351, 750, 5, 934, 43, 3158, 4762, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>please rise, then, for this minute' s silence.</td>\n",
       "      <td>ich bitte sie, sich zu einer schweigeminute zu...</td>\n",
       "      <td>46</td>\n",
       "      <td>55</td>\n",
       "      <td>[1, 3005, 416, 77, 359, 4, 241, 4, 17, 76, 451...</td>\n",
       "      <td>[1, 241, 156, 72, 3112, 54, 4, 39, 26, 95, 473...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(the house rose and observed a minute' s silence)</td>\n",
       "      <td>(das parlament erhebt sich zu einer schweigemi...</td>\n",
       "      <td>49</td>\n",
       "      <td>52</td>\n",
       "      <td>[1, 29, 140, 414, 3231, 8, 3106, 2484, 9, 451,...</td>\n",
       "      <td>[1, 35, 2444, 2269, 2109, 625, 39, 26, 95, 473...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>madam president, on a point of order.</td>\n",
       "      <td>frau präsidentin, zur geschäftsordnung.</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>[1, 1599, 134, 546, 4, 19, 9, 918, 6, 535, 5, 2]</td>\n",
       "      <td>[1, 1161, 2266, 52, 4, 132, 2232, 1516, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>if the house agrees, i shall do as mr evans ha...</td>\n",
       "      <td>wenn das haus damit einverstanden ist, werde i...</td>\n",
       "      <td>58</td>\n",
       "      <td>86</td>\n",
       "      <td>[1, 530, 3, 414, 1434, 35, 4, 305, 186, 321, 3...</td>\n",
       "      <td>[1, 835, 19, 684, 494, 21, 161, 48, 838, 30, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_texts  \\\n",
       "0                           resumption of the session   \n",
       "5      please rise, then, for this minute' s silence.   \n",
       "6   (the house rose and observed a minute' s silence)   \n",
       "7               madam president, on a point of order.   \n",
       "12  if the house agrees, i shall do as mr evans ha...   \n",
       "\n",
       "                                         target_texts  input_length  \\\n",
       "0                  wiederaufnahme der sitzungsperiode            25   \n",
       "5   ich bitte sie, sich zu einer schweigeminute zu...            46   \n",
       "6   (das parlament erhebt sich zu einer schweigemi...            49   \n",
       "7             frau präsidentin, zur geschäftsordnung.            37   \n",
       "12  wenn das haus damit einverstanden ist, werde i...            58   \n",
       "\n",
       "    target_length                                    input_sequences  \\\n",
       "0              34          [1, 344, 146, 498, 90, 6, 3, 3235, 90, 2]   \n",
       "5              55  [1, 3005, 416, 77, 359, 4, 241, 4, 17, 76, 451...   \n",
       "6              52  [1, 29, 140, 414, 3231, 8, 3106, 2484, 9, 451,...   \n",
       "7              39   [1, 1599, 134, 546, 4, 19, 9, 918, 6, 535, 5, 2]   \n",
       "12             86  [1, 530, 3, 414, 1434, 35, 4, 305, 186, 321, 3...   \n",
       "\n",
       "                                     target_sequences  \n",
       "0       [1, 247, 351, 750, 5, 934, 43, 3158, 4762, 2]  \n",
       "5   [1, 241, 156, 72, 3112, 54, 4, 39, 26, 95, 473...  \n",
       "6   [1, 35, 2444, 2269, 2109, 625, 39, 26, 95, 473...  \n",
       "7       [1, 1161, 2266, 52, 4, 132, 2232, 1516, 3, 2]  \n",
       "12  [1, 835, 19, 684, 494, 21, 161, 48, 838, 30, 4...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T19:18:06.090991Z",
     "start_time": "2018-06-17T19:18:05.528655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello.' --> 'helfen sie.'\n",
      "'you are welcome.' --> 'sie sind begrüßenswert.'\n",
      "'how do you do?' --> 'wie sieht sie tun?'\n",
      "'i hate mondays.' --> 'ich habe meine frage.'\n",
      "'i am a programmer.' --> 'ich bin ein programm.'\n",
      "'data is the new oil.' --> 'die daten ist das neue öl.'\n",
      "'it could be worse.' --> 'es könnte schlimmer sein.'\n",
      "'i am on top of it.' --> 'ich bin davon überzeugt.'\n",
      "'n° uno' --> 'nitreo'\n",
      "'awesome!' --> 'einwände!'\n",
      "'put your feet up!' --> 'stellen sie ihre füße!'\n",
      "'from the start till the end!' --> 'aus dem beginn des beginns!'\n",
      "'from dusk till dawn.' --> 'von duskulanz.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T19:18:07.075408Z",
     "start_time": "2018-06-17T19:18:06.092186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original \"please rise, then, for this minute' s silence.\", got 'ich bitte sie, diese schweigeminute zu sagen.', exp: 'ich bitte sie, sich zu einer schweigeminute zu erheben.'\n",
      "Original \"(the house rose and observed a minute' s silence)\", got '(das parlament erhebt sich zu einer schweigeminute.)', exp: '(das parlament erhebt sich zu einer schweigeminute.)'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'if the house agrees, i shall do as mr evans has suggested.', got 'wenn das haus einverstanden ist, werde ich herrn evans vorgeschlagen.', exp: 'wenn das haus damit einverstanden ist, werde ich dem vorschlag von herrn evans folgen.'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'i would like your advice about rule 0 concerning inadmissibility.', got 'ich möchte gerne ihre ratspräsidentschaft über eine regelung für die zulässigkeit der zulässigkeit gehen.', exp: 'könnten sie mir eine auskunft zu artikel 0 im zusammenhang mit der unzulässigkeit geben?'\n",
      "Original 'it says that this should be done despite the principle of relative stability.', got 'es ist sagt, dass dies trotz des prinzips der relativ stabilität geschehen sollte.', exp: 'und zwar sollen derartige strafen trotz des grundsatzes der relativen stabilität verhängt werden.'\n",
      "Original 'this is all in accordance with the principles that we have always upheld.', got 'dies ist alles in der übereinstimmung mit den prinzipien, die wir immer wieder gesagt haben.', exp: 'all dies entspricht den grundsätzen, die wir stets verteidigt haben.'\n",
      "Original 'thank you, mr segni, i shall do so gladly.', got 'vielen dank, herr segni, ich werde dies tun.', exp: 'vielen dank, herr segni, das will ich gerne tun.'\n",
      "Original 'indeed, it is quite in keeping with the positions this house has always adopted.', got 'in der tat ist es in der tat die position des parlaments, die immer angenommen wurde.', exp: 'das ist ganz im sinne der position, die wir als parlament immer vertreten haben.'\n",
      "Original 'it is the case of alexander nikitin.', got 'das ist der fall alexander nikitin.', exp: 'das ist der fall von alexander nikitin.'\n",
      "Original 'now, however, he is to go before the courts once more because the public prosecutor is appealing.', got 'jetzt muss er vor den gerichten gehen, weil der öffentlich-vertrag aufrufen.', exp: 'nun ist es aber so, daß er wieder angeklagt werden soll, weil der staatsanwalt in berufung geht.'\n",
      "Original 'but, madam president, my personal request has not been met.', got 'aber frau präsidentin, meine persönliche anfrage war nicht erfüllt worden.', exp: 'dennoch, frau präsidentin, wurde meinem wunsch nicht entsprochen.'\n",
      "Original 'i would therefore once more ask you to ensure that we get a dutch channel as well.', got 'deshalb möchte ich sie bitten, dafür zu sorgen, dass wir einen niederländischen änderungsantrag erzielen.', exp: 'deshalb möchte ich sie nochmals ersuchen, dafür sorge zu tragen, daß auch ein niederländischer sender eingespeist wird.'\n",
      "Original 'it will, i hope, be examined in a positive light.', got 'ich hoffe, dass sie in einem positiven licht geprüft werden.', exp: 'ich hoffe, daß dort in ihrem sinne entschieden wird.'\n",
      "Original 'why has no air quality test been done on this particular building since we were elected?', got 'warum hat es keine luftqualitätsbedingungen auf diesem speziellen gebäude, weil wir gewählt wurden?', exp: 'weshalb wurde die luftqualität in diesem gebäude seit unserer wahl nicht ein einziges mal überprüft?'\n",
      "Original 'why has there been no health and safety committee meeting since 0?', got 'warum hat es keine gesundheit und sicherheitsausschuß seit 0 gegeben?', exp: 'weshalb ist der arbeitsschutzausschuß seit 0 nicht ein einziges mal zusammengetreten?'\n",
      "Original 'why are there no fire instructions?', got 'warum gibt es keine feuerartaktionen?', exp: 'warum finden keine brandschutzbelehrungen statt?'\n",
      "Original 'why have the staircases not been improved since my accident?', got 'warum wurden die schwächen nicht verbessert?', exp: 'warum wurde nach meinem unfall nichts unternommen, um die treppen sicherer zu machen?'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in europarl.df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T19:18:08.270313Z",
     "start_time": "2018-06-17T19:18:07.076972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'we congratulate you on a job very well done.', got 'wir beglückwünschen ihnen zu einer arbeit sehr gut.', exp: 'wir gratulieren dir zu deiner hervorragenden arbeit.'\n",
      "Original 'in this case, i strongly disagree with what was said by the previous speaker, carl schlyter.', got 'in diesem fall stimme ich mit dem vorredner mit dem vorredner mit dem vorredner nicht zu.', exp: 'in diesem fall widerspreche ich klar dem, was der vorredner, carl schlyter, gesagt hat.'\n",
      "Original 'it only makes sense to rebuild these if the refugees who fled are coming back.', got 'es ist nur sinnvoll, dies zu wiederaufbauen, wenn die flüchtlinge zurückkehren.', exp: 'deren wiederaufbau ist nur dann von nutzen, wenn die flüchtlinge aus den betreffenden gebieten wieder zurückkehren.'\n",
      "Original 'eba: everything but arms.', got 'eba: alles, aber waffen.', exp: 'eba: everything but arms.'\n",
      "Original 'in wider terms, this directive is not ambitious enough.', got 'in diesem sinne ist diese richtlinie nicht ehrgeizig genug.', exp: 'generell gesehen fehlt es dieser richtlinie an ambitioniertheit.'\n",
      "Original 'it is an irresponsible mistake to spend public money to maintain this fleet.', got 'es ist ein unverantwortlicher fehler, das öffentliche geld für diese flotte zu erhalten.', exp: 'öffentliche gelder zur erhaltung dieser flotte zu verwenden ist ein verantwortungsloser fehler.'\n",
      "Original 'where better to ensure access to culture than through our libraries?', got 'woher besser zu den zugang zu kultur als durch unsere bibliotheken gewährleistet?', exp: 'wo ließe sich der zugang zu kultur besser sichern als durch unsere bibliotheken?'\n",
      "Original 'i would like to advise you against this.', got 'ich möchte ihnen dafür sorgen.', exp: 'hiervor möchte ich warnen.'\n",
      "Original 'in writing. - (pt) it is essential to harmonise national safety procedures in member states.', got 'schriftlich. - (pt) es ist notwendig, die nationalen sicherheitsverfahren in den mitgliedstaaten zu harmonisieren.', exp: 'schriftlich. - (pt) es kommt jetzt darauf an, die nationalen sicherheitsverfahren in den mitgliedstaaten zu harmonisieren.'\n",
      "Original 'we know that shipbuilding is the last industrial sector to be subsidised.', got 'wir wissen, dass die schiffbauindustrie den letzten industriellen sektor zu subventionieren ist.', exp: 'wir wissen, dass der schiffbau der letzte subventionierte industriesektor ist.'\n",
      "Original 'we spent 0 million euros on this project in 0, and 0 million euros in 0.', got 'wir haben 0 millionen euro für dieses projekt im jahr 0 ausgegeben und 0 millionen euro im jahr 0 ausgegeben.', exp: 'im jahr 0 haben wir 0 millionen euro für dieses projekt ausgegeben, 0 dann 0 millionen euro.'\n",
      "Original 'the eventual solution to the chinese challenge is to be found not in china, but in africa itself.', got 'die lösung der chinesischen herausforderung besteht darin, in china, aber in afrika selbst.', exp: 'letztendlich liegt die antwort auf die chinesische herausforderung nicht in china, sondern in afrika selbst.'\n",
      "Original \"we in parliament will say 'yes' to new taxes but 'no' to more taxes.\", got 'wir im parlament werden sagen: \"ja\" zu neuen steuern, sondern \"nein\" zu mehr steuern.', exp: 'wir im parlament werden neuen steuern zustimmen, zusätzlichen steuern jedoch eine klare absage erteilen.'\n",
      "Original 'however, in my view we must not rest on our laurels.', got 'meiner ansicht nach dürfen wir jedoch nicht auf unsere lobfragen reden.', exp: 'meiner ansicht nach dürfen wir uns jedoch nicht auf unseren lorbeeren ausruhen.'\n",
      "Original 'i shall confine my comments to a few basic issues.', got 'ich werde meine bemerkungen zu einigen grundlegenden fragen beschränken.', exp: 'im folgenden werde ich mich auf einige grundlegende themen konzentrieren.'\n",
      "Original 'the greatest anxiety is being aroused by the damage to the fukushima nuclear power station.', got 'die größte flexibilität wird durch den schäden des fukushima-uklearen kernkraftwerks durchgeführt.', exp: 'die größte sorge wird durch den schaden am kernkraftwerk fukushima erweckt.'\n",
      "Original 'much is only on paper and unfortunately the reality is different.', got 'vielleicht geht es nur um das papier und unglücklicherweise um die realität.', exp: 'vieles steht nur auf dem papier, die realität sieht leider anders aus.'\n",
      "Original 'this would not help anyone, neither the assistants themselves nor the members of parliament.', got 'das würde nichts helfen, weder die assistenten selbst noch die mitglieder des parlaments.', exp: 'damit wäre niemandem geholfen, weder den assistenten noch den abgeordneten hier im hause.'\n",
      "Original 'can civil society in russia expect nothing more than this kind of provocation?', got 'kann die zivilgesellschaft in russland nicht mehr als diese art von provokationen erwarten?', exp: 'hat die zivilgesellschaft in russland nur provokationen zu erwarten?'\n"
     ]
    }
   ],
   "source": [
    "# Performance on validation set\n",
    "val_df = europarl.df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T19:20:07.097908Z",
     "start_time": "2018-06-17T19:18:08.271889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743f59ef1e5b4dca99b77a0e24a5aceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.23681209357919392\n"
     ]
    }
   ],
   "source": [
    "bleu = bleu_scores_europarl(\n",
    "    input_texts=europarl.df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    target_texts=europarl.df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    predict=lambda text: predict(text)\n",
    ")\n",
    "print(f'average BLEU on test set = {bleu.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.995932Z",
     "start_time": "2018-05-13T18:17:20.994111Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The translations for the train and validations are decent. The main problem without attention was that the rnn decoder repeated itself and got confused what it intended to say (like \"leider geht es nur um die realität und die realität.\") The bleu score also improved from 0.183 to 0.237.\n",
    "\n",
    "Now, the main problems are any words out of the vocabulary (meaning not seen often enough in the training set). Some of the problems will go away if we use the whole training data set instead of a subset. But in the long run, we might need a copy mechanism, a look up method and/or a dynamic neural network memory. Also, some translations make grammatical mistakes. I can imagine that multi learning (like also learn grammatical features or entity recognition maybe in combination with some data augmentation) would help. It's also somehow funny to see how accurately the translations are for complicated sentences from the validation set, but for my very simple own examples that are different to europarlament discussion, the translations are terrible.\n",
    "\n",
    "Regarding tensorflow, allthough the code is a bit longer, it doesn't need much energy to figure what's going on. Every line manipulates the computational graph without much surprises. Allthough, debugging is still a hassle (cryptic error messages and no direct evaluation), it's not so mind blogging like working on different abstraction levels with Keras IMHO. It's also nice to see that a lot state-of-the art technology from research is already implemented in tensorflow. The computational speed went up here a lot, too. I think the main reason is that the tensorflow implementation only trains till end-of-sentence and not beyond (where Keras implementation masked them only to ignore in loss function). \n",
    "\n",
    "My next step will be to train the neural machine translation on the whole europarliament dataset (2.3 Mio > 600k) and also train for a longer time (even here the training did not converge after 20 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 294,
   "position": {
    "height": "40px",
    "left": "553px",
    "right": "192px",
    "top": "132px",
    "width": "615px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
