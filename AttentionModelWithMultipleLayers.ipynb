{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Multiple Layers for Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing [Attention](AttentionModelForMachineTranslationWithTensorflow.ipynb), I'll come back first to examine whether it's worth to implement mulitple layers (of encoders and decoders) instead of only one. Of course, Deep Learning is all about deep nets, at some point I'll have to check it.\n",
    "\n",
    "Again, I'll try to refactor the code a bit, so we can concentrate on the stacking issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:09:07.255140Z",
     "start_time": "2018-06-22T14:09:05.305212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed random seed to 42\n",
      "Availabe devices: [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17223146694929954950\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7803568128\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1236201890047166634\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Cuda/Cudnn/GPU works as intended\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils.download import download_and_extract_resources\n",
    "from utils.linguistic import bleu_scores_europarl, preprocess_input_europarl as preprocess\n",
    "from utils.preparation import check_gpu_working, Europarl, RANDOM_STATE\n",
    "\n",
    "check_gpu_working()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:09:07.260225Z",
     "start_time": "2018-06-22T14:09:07.256848Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 100\n",
    "MAX_TARGET_LENGTH = 125\n",
    "LATENT_DIM = 256  # was 512, but we should be able to use a smaller hidden representation as we are looking back anyway as needed\n",
    "LAYERS = 2\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128  # was 64, but tensorflow implementation doesn't need so much GPU memory so can increase batch size\n",
    "DROPOUT = 0.25  # Dropout on input and output for the RNN cells, so effective dropout is 0.5, but works slightly better so\n",
    "TEST_SIZE = 2500\n",
    "BEAM_WIDTH = 5\n",
    "EMBEDDING_TRAINABLE = True  # Improves results significant and for at least it's not the most dominant training time factor (that's the output softmax layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:09:55.033880Z",
     "start_time": "2018-06-22T14:09:07.261917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n",
      "en.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "en.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (6.2 MB)\n",
      "de.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "de.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (5.7 MB)\n",
      "Total number of unfiltered translations 1920209\n",
      "Filtered translations with length between (1, input=100/target=125) characters: 597331\n"
     ]
    }
   ],
   "source": [
    "europarl = Europarl()\n",
    "download_and_extract_resources(fnames_and_urls=europarl.external_resources, dest_path=europarl.path)\n",
    "europarl.load_and_preprocess(max_input_length=MAX_INPUT_LENGTH, max_target_length=MAX_TARGET_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:09:55.227882Z",
     "start_time": "2018-06-22T14:09:55.035748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resumption of the session</td>\n",
       "      <td>wiederaufnahme der sitzungsperiode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>please rise, then, for this minute' s silence.</td>\n",
       "      <td>ich bitte sie, sich zu einer schweigeminute zu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(the house rose and observed a minute' s silence)</td>\n",
       "      <td>(das parlament erhebt sich zu einer schweigemi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>madam president, on a point of order.</td>\n",
       "      <td>frau präsidentin, zur geschäftsordnung.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>if the house agrees, i shall do as mr evans ha...</td>\n",
       "      <td>wenn das haus damit einverstanden ist, werde i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_texts  \\\n",
       "0                           resumption of the session   \n",
       "5      please rise, then, for this minute' s silence.   \n",
       "6   (the house rose and observed a minute' s silence)   \n",
       "7               madam president, on a point of order.   \n",
       "12  if the house agrees, i shall do as mr evans ha...   \n",
       "\n",
       "                                         target_texts  \n",
       "0                  wiederaufnahme der sitzungsperiode  \n",
       "5   ich bitte sie, sich zu einer schweigeminute zu...  \n",
       "6   (das parlament erhebt sich zu einer schweigemi...  \n",
       "7             frau präsidentin, zur geschäftsordnung.  \n",
       "12  wenn das haus damit einverstanden ist, werde i...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl.df[['input_texts', 'target_texts']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:09:55.232933Z",
     "start_time": "2018-06-22T14:09:55.229537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English subwords ['▁this', '▁is', '▁a', '▁test', '▁for', '▁pre', 'tr', 'ained', '▁by', 'te', 'pa', 'ire', 'm', 'bed', 'd', 'ings']\n",
      "German subwords ['▁das', '▁ist', '▁ein', '▁test', '▁für', '▁v', 'ort', 'rain', 'ierte', '▁zeich', 'eng', 'ruppen']\n"
     ]
    }
   ],
   "source": [
    "print(\"English subwords\", europarl.bpe_input.sentencepiece.EncodeAsPieces(\"this is a test for pretrained bytepairembeddings\"))\n",
    "print(\"German subwords\", europarl.bpe_target.sentencepiece.EncodeAsPieces(\"das ist ein test für vortrainierte zeichengruppen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:09:55.497633Z",
     "start_time": "2018-06-22T14:09:55.235217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 71)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Those will be the inputs for the seq2seq model (that needs to know how long the sequences can get)\n",
    "max_len_input = europarl.df.input_sequences.apply(len).max()\n",
    "max_len_target = europarl.df.target_sequences.apply(len).max()\n",
    "(max_len_input, max_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:09:55.522504Z",
     "start_time": "2018-06-22T14:09:55.499295Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(europarl.df.shape[0]), test_size=0.1, random_state=RANDOM_STATE)  # fixed random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:10:02.051749Z",
     "start_time": "2018-06-22T14:09:55.524236Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    encoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_input\n",
    "        dtype=tf.int32,\n",
    "        name='encoder_inputs' \n",
    "    )\n",
    "    batch_size = tf.shape(encoder_inputs)[0]\n",
    "    beam_width = tf.placeholder_with_default(1, shape=[])\n",
    "    dropout = tf.placeholder_with_default(tf.cast(0.0, tf.float32), shape=[])\n",
    "    keep_prob = tf.cast(1.0, tf.float32) - dropout\n",
    "\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", \n",
    "        initializer=tf.constant(europarl.bpe_input.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder,\n",
    "        encoder_inputs,\n",
    "        name=\"encoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    input_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='input_sequence_length'\n",
    "    )\n",
    "    \n",
    "    rnn_cell_type = tf.nn.rnn_cell.GRUCell\n",
    "    encoder_forward_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name=f'encoder_forward_cell{layer}'),\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,  # state_keep_prob not set as it was not helpful here\n",
    "        dtype=tf.float32,\n",
    "    ) for layer in range(LAYERS)]\n",
    "    encoder_backward_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name=f'encoder_backward_cell{layer}'),\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    ) for layer in range(LAYERS)]\n",
    "    #encoder_bi_outputs, encoder_bi_state = \n",
    "    encoder_outputs, encoder_output_state_fw, encoder_output_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "        encoder_forward_cells, encoder_backward_cells,\n",
    "        inputs=encoder_emb_inp,\n",
    "        sequence_length=input_sequence_length,\n",
    "        time_major=False,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    # encoder_outputs = tf.concat(encoder_bi_outputs, -1)\n",
    "    # encoder_state = tf.concat(encoder_bi_state, -1)\n",
    "    encoder_state = tf.concat([encoder_output_state_fw[-1], encoder_output_state_bw[-1]], -1)\n",
    "    \n",
    "    # Regarding time_major:\n",
    "    # If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "    # If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "    # Using `time_major = True` is a bit more efficient because it avoids\n",
    "    # transposes at the beginning and end of the RNN calculation.  However,\n",
    "    # most TensorFlow data is batch-major, so by default this function\n",
    "    # accepts input and emits output in batch-major form.\n",
    "    #\n",
    "    # for simplicity I work with batch major here instead of time_major\n",
    "    # so I don't need to transpose inputs and transpose back for attention mechanism\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_inputs' \n",
    "    )\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", \n",
    "        initializer=tf.constant(europarl.bpe_target.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder,\n",
    "        decoder_inputs,\n",
    "        name=\"decoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    target_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='target_sequence_length'\n",
    "    )\n",
    "    \n",
    "    # tiling is necessary to work with BeamSearchDecoder\n",
    "    # read carefully the NOTE on constructor in\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper \n",
    "    tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(encoder_outputs, multiplier=beam_width)\n",
    "    tiled_encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width)\n",
    "    tiled_sequence_length = tf.contrib.seq2seq.tile_batch(input_sequence_length, multiplier=beam_width)\n",
    "    \n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        LATENT_DIM,\n",
    "        memory=tiled_encoder_outputs,\n",
    "        memory_sequence_length=tiled_sequence_length,\n",
    "        dtype=tf.float32,\n",
    "        name='attention_mechanism',\n",
    "    )\n",
    "    decoder_rnn_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM, name='decoder_cell'),\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        decoder_rnn_cell,\n",
    "        attention_mechanism,\n",
    "        attention_layer_size=LATENT_DIM, \n",
    "        name='attention_wrapper',\n",
    "    )\n",
    "\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        inputs=decoder_emb_inp, \n",
    "        sequence_length=target_sequence_length,\n",
    "        time_major=False,\n",
    "        name=\"decoder_training_helper\",\n",
    "    )\n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        units=len(europarl.bpe_target.tokens),\n",
    "        use_bias=False,\n",
    "        name='projection_layer',\n",
    "    )\n",
    "    \n",
    "    initial_state=decoder_cell.zero_state(dtype=tf.float32, batch_size=batch_size).clone(\n",
    "        cell_state=encoder_state\n",
    "    )\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=decoder_cell,\n",
    "        helper=training_helper,\n",
    "        initial_state=initial_state,\n",
    "        output_layer=projection_layer,\n",
    "    )\n",
    "    outputs, _final_state, _final_sequence_length = tf.contrib.seq2seq.dynamic_decode(  \n",
    "        decoder,\n",
    "        output_time_major=False,\n",
    "        impute_finished=False,\n",
    "    )\n",
    "    logits = outputs.rnn_output\n",
    "    \n",
    "    decoder_outputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_outputs',\n",
    "    )\n",
    "    target_weights = tf.cast(tf.sequence_mask(target_sequence_length), dtype=tf.float32)\n",
    "    train_loss = tf.contrib.seq2seq.sequence_loss(logits, decoder_outputs, target_weights)\n",
    "\n",
    "    params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, params)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "        t_list=gradients,\n",
    "        clip_norm=1.,\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    update_step = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "    \n",
    "    inference_decoder_initial_state = decoder_cell.zero_state(\n",
    "        dtype=tf.float32,\n",
    "        batch_size=batch_size * beam_width  # tricky and somehow unintuitive, but necessary\n",
    "    ).clone(\n",
    "        cell_state=tiled_encoder_state\n",
    "    )\n",
    "    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "        cell=decoder_cell,\n",
    "        embedding=embedding_decoder,\n",
    "        start_tokens=tf.fill([batch_size], europarl.bpe_target.start_token_idx),\n",
    "        end_token=europarl.bpe_target.stop_token_idx,\n",
    "        initial_state=inference_decoder_initial_state,\n",
    "        beam_width=BEAM_WIDTH,\n",
    "        output_layer=projection_layer,\n",
    "        length_penalty_weight=1.0,  # https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/\n",
    "    )\n",
    "\n",
    "    \n",
    "    inference_outputs, _inference_final_state, _inference_final_sequence_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inference_decoder,\n",
    "        maximum_iterations=tf.round(tf.reduce_max(input_sequence_length) * 2),  # a bit more flexible than max_len_target\n",
    "        impute_finished=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T14:10:02.062814Z",
     "start_time": "2018-06-22T14:10:02.053100Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_train_batch(batch_ids):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    pred, loss, _ = sess.run(\n",
    "        fetches=[\n",
    "            outputs, train_loss, update_step\n",
    "        ],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "            dropout: DROPOUT,\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def run_val_batch(batch_ids):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    loss = sess.run(\n",
    "        fetches=[train_loss],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def run_validation_loss():\n",
    "    return np.mean([\n",
    "        run_val_batch(ids)\n",
    "        for ids \n",
    "        in np.array_split(val_ids, np.ceil(len(val_ids) / BATCH_SIZE))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T18:49:18.170598Z",
     "start_time": "2018-06-22T14:10:02.064068Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b6d162b09a40a3875f608db1f6de9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 3.3002694 val_loss 2.3963087\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66da9e73cf554793ae8fc90095cbcb59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.5515401 val_loss 2.150888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483454274dfa4d0b94f794ded5c04e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.3890698 val_loss 2.0429566\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5c74cdbf2f4cd78d3b685e9d946274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.298877 val_loss 1.9747212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997e3590d8f2409e93e7f1139466a973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.2359378 val_loss 1.9244874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5da2f35e4bb4705b3f3d8d33b71a244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.185375 val_loss 1.8782232\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d3039609054c55a327ae39095e595f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.1381018 val_loss 1.8368719\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d52c27163fb4f6bb3d7f065ca0ef4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 8', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.0900717 val_loss 1.7949886\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bf61689fca40e98100b160a587a77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 9', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.0314927 val_loss 1.7475777\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535c7a6b88fd4c19854d5bb469cdc15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 10', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.974537 val_loss 1.7086394\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364f927dde2b4cb98f7d4ae2064d7751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 11', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.9354006 val_loss 1.6879371\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb8e332591a45d0bb00ad68a0913676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 12', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.9090197 val_loss 1.6691895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6056b6050c24885ac543d4033e7af43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 13', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.8886082 val_loss 1.6497431\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc12dc55aa234985944b7eae5e352b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 14', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.8718128 val_loss 1.6387889\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a279575b930e4660a70e91eba2233f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 15', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.8571445 val_loss 1.6262755\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5668d28bf9a54631a3c400e7cd36ca40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 16', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.8450264 val_loss 1.6174359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1d71437f094621840512811b76440b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 17', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.8345942 val_loss 1.6089264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb719288c884b1e8ac397760f7e2b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 18', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.8244557 val_loss 1.6019193\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2b894616d5489384d4d33fae0f80a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 19', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.8154076 val_loss 1.5957993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3863120af6ae4d07a263810a93a73570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 20', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.8077573 val_loss 1.5885834\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(\n",
    "    allow_soft_placement=True,  # needed as recommendation from https://github.com/tensorflow/tensorflow/issues/2292\n",
    "    log_device_placement=True,\n",
    ")\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batches_per_epoch = np.ceil(len(train_ids) / BATCH_SIZE)\n",
    "for epoch in range(EPOCHS):\n",
    "    shuffled_ids = np.random.permutation(train_ids)\n",
    "    batch_splits = np.array_split(shuffled_ids, batches_per_epoch)\n",
    "    train_losses = []\n",
    "    N = len(batch_splits)\n",
    "    with tqdm(batch_splits, desc=f\"Epoch {epoch+1}\") as t:\n",
    "        for train_batch_ids in t:\n",
    "            batch_loss = run_train_batch(train_batch_ids)\n",
    "            train_losses.append(batch_loss)\n",
    "            t.set_postfix(train_loss=np.mean(train_losses))\n",
    "        print(\"train_loss\", np.mean(train_losses), \"val_loss\", run_validation_loss())\n",
    "        \n",
    "validation_input_sequences = europarl.df.input_sequences.iloc[val_ids[:BATCH_SIZE]]\n",
    "validation_input_lengths = validation_input_sequences.apply(len)\n",
    "\n",
    "validation_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    validation_input_sequences,\n",
    "    maxlen=max_len_input,\n",
    "    dtype=int,\n",
    "    padding='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T18:49:18.176427Z",
     "start_time": "2018-06-22T18:49:18.171960Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    sequenced = europarl.bpe_input.subword_indices(preprocess(sentence))\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [sequenced],\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    \n",
    "    beam_search_output = sess.run(\n",
    "        fetches=[inference_outputs],\n",
    "        feed_dict={\n",
    "            encoder_inputs: padded,\n",
    "            input_sequence_length: [len(sequenced)],\n",
    "            beam_width: BEAM_WIDTH,\n",
    "        }\n",
    "    )[0]\n",
    "    \n",
    "    return europarl.bpe_target.sentencepiece.DecodePieces([\n",
    "        europarl.bpe_target.tokens[idx] for idx in beam_search_output.predicted_ids[0, :, 0].tolist()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T18:49:19.134215Z",
     "start_time": "2018-06-22T18:49:18.177783Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/tfattentionmodel_2layers.ckpt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'tfattentionmodel_2layers'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, f\"data/{name}.ckpt\")\n",
    "# tfattentionmodel.ckpt.index https://drive.google.com/open?id=1Xv2Qc9Gnac_Of9bIpQef5LZJnX9ij933\n",
    "# tfattentionmodel.cpkt.meta https://drive.google.com/open?id=162WY8XEjyqfvitiIBfmq9oHLC1rQhyvr\n",
    "# tfattentionmodel.cpkt.data-00000-of-00001 https://drive.google.com/open?id=1racW0agvk5nJ_xBaxifAaEWM8T9ot7FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T18:49:19.848332Z",
     "start_time": "2018-06-22T18:49:19.136469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello.' --> 'helfen sie.'\n",
      "'you are welcome.' --> 'sie sind begrüßenswert.'\n",
      "'how do you do?' --> 'wie tun sie?'\n",
      "'i hate mondays.' --> 'ich habe meine antwort.'\n",
      "'i am a programmer.' --> 'ich bin ein land.'\n",
      "'data is the new oil.' --> 'daten ist das neue öl.'\n",
      "'it could be worse.' --> 'sie könnte noch schlimmer sein.'\n",
      "'i am on top of it.' --> 'ich bin über das thema.'\n",
      "'n° uno' --> 'nein.'\n",
      "'awesome!' --> 'einwände!'\n",
      "'put your feet up!' --> 'machen sie ihre füßen!'\n",
      "'from the start till the end!' --> 'anfangs!'\n",
      "'from dusk till dawn.' --> 'aus drakisch gehen sie.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T18:49:21.195968Z",
     "start_time": "2018-06-22T18:49:19.850714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original \"please rise, then, for this minute' s silence.\", got 'bitte bitte ich sie, das schweigen zu sagen.', exp: 'ich bitte sie, sich zu einer schweigeminute zu erheben.'\n",
      "Original \"(the house rose and observed a minute' s silence)\", got '(das parlament erhebt sich zu einer schweigeminute.)', exp: '(das parlament erhebt sich zu einer schweigeminute.)'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'if the house agrees, i shall do as mr evans has suggested.', got 'wenn das haus abstimmt, werde ich das tun, wie herr evans vorgeschlagen hat.', exp: 'wenn das haus damit einverstanden ist, werde ich dem vorschlag von herrn evans folgen.'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'i would like your advice about rule 0 concerning inadmissibility.', got 'ich möchte gerne ihre empfehlung über artikel 0 bezüglich der zulässigkeit.', exp: 'könnten sie mir eine auskunft zu artikel 0 im zusammenhang mit der unzulässigkeit geben?'\n",
      "Original 'it says that this should be done despite the principle of relative stability.', got 'sie sagt, dass dies trotz des prinzips der relativ stabilität geschehen sollte.', exp: 'und zwar sollen derartige strafen trotz des grundsatzes der relativen stabilität verhängt werden.'\n",
      "Original 'this is all in accordance with the principles that we have always upheld.', got 'das ist alles in der zustimmung mit den prinzipien, die wir immer gesagt haben.', exp: 'all dies entspricht den grundsätzen, die wir stets verteidigt haben.'\n",
      "Original 'thank you, mr segni, i shall do so gladly.', got 'vielen dank, herr segni, ich werde dies tun.', exp: 'vielen dank, herr segni, das will ich gerne tun.'\n",
      "Original 'indeed, it is quite in keeping with the positions this house has always adopted.', got 'in der tat ist es in der tat um die positionen, die dieses parlament immer angenommen hat.', exp: 'das ist ganz im sinne der position, die wir als parlament immer vertreten haben.'\n",
      "Original 'it is the case of alexander nikitin.', got 'es ist der fall von alexander nikitin.', exp: 'das ist der fall von alexander nikitin.'\n",
      "Original 'now, however, he is to go before the courts once more because the public prosecutor is appealing.', got 'jetzt müssen er vor den gerichten vorgeschlagen werden, denn die staatsanwaltschaft wird anspruchsvoll.', exp: 'nun ist es aber so, daß er wieder angeklagt werden soll, weil der staatsanwalt in berufung geht.'\n",
      "Original 'but, madam president, my personal request has not been met.', got 'aber frau präsidentin, meine persönliche anfrage wurde nicht erfüllt.', exp: 'dennoch, frau präsidentin, wurde meinem wunsch nicht entsprochen.'\n",
      "Original 'i would therefore once more ask you to ensure that we get a dutch channel as well.', got 'deshalb möchte ich noch einmal bitten, dass wir ein niederländischer sender geben.', exp: 'deshalb möchte ich sie nochmals ersuchen, dafür sorge zu tragen, daß auch ein niederländischer sender eingespeist wird.'\n",
      "Original 'it will, i hope, be examined in a positive light.', got 'ich hoffe, es wird in einem positiven licht geprüft.', exp: 'ich hoffe, daß dort in ihrem sinne entschieden wird.'\n",
      "Original 'why has no air quality test been done on this particular building since we were elected?', got 'warum hat keine luftqualität in diesem speziellen gebäude geschehen, weil wir gewählt wurden?', exp: 'weshalb wurde die luftqualität in diesem gebäude seit unserer wahl nicht ein einziges mal überprüft?'\n",
      "Original 'why has there been no health and safety committee meeting since 0?', got 'warum gab es keine gesundheit und der sicherheitsausschuss?', exp: 'weshalb ist der arbeitsschutzausschuß seit 0 nicht ein einziges mal zusammengetreten?'\n",
      "Original 'why are there no fire instructions?', got 'warum gibt es keine feuerwerkschaften?', exp: 'warum finden keine brandschutzbelehrungen statt?'\n",
      "Original 'why have the staircases not been improved since my accident?', got 'warum sind die strengereien nicht verbessert worden?', exp: 'warum wurde nach meinem unfall nichts unternommen, um die treppen sicherer zu machen?'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in europarl.df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T18:49:22.698467Z",
     "start_time": "2018-06-22T18:49:21.197972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'we congratulate you on a job very well done.', got 'wir beglückwünschen sie auf eine gute arbeit.', exp: 'wir gratulieren dir zu deiner hervorragenden arbeit.'\n",
      "Original 'in this case, i strongly disagree with what was said by the previous speaker, carl schlyter.', got 'in diesem fall stimme ich nicht zu, was gesagt wurde, was der vorredner von der vorrednerin gesagt wurde.', exp: 'in diesem fall widerspreche ich klar dem, was der vorredner, carl schlyter, gesagt hat.'\n",
      "Original 'it only makes sense to rebuild these if the refugees who fled are coming back.', got 'es ist nur sinnvoll, dies zu wiederaufbauen, wenn die flüchtlinge zurückkommen.', exp: 'deren wiederaufbau ist nur dann von nutzen, wenn die flüchtlinge aus den betreffenden gebieten wieder zurückkehren.'\n",
      "Original 'eba: everything but arms.', got 'evba: alles, was alles bedeutet, waffen.', exp: 'eba: everything but arms.'\n",
      "Original 'in wider terms, this directive is not ambitious enough.', got 'vielleicht ist diese richtlinie nicht ehrgeizig genug.', exp: 'generell gesehen fehlt es dieser richtlinie an ambitioniertheit.'\n",
      "Original 'it is an irresponsible mistake to spend public money to maintain this fleet.', got 'es ist ein unverantwortlicher fehler, den öffentlichen geld zu halten, um dieses flotten zu halten.', exp: 'öffentliche gelder zur erhaltung dieser flotte zu verwenden ist ein verantwortungsloser fehler.'\n",
      "Original 'where better to ensure access to culture than through our libraries?', got 'wo ist besser gewährleistet, den zugang zu kultur als durch unsere bibliotheken zu gewährleisten?', exp: 'wo ließe sich der zugang zu kultur besser sichern als durch unsere bibliotheken?'\n",
      "Original 'i would like to advise you against this.', got 'ich möchte ihnen dafür stimmen.', exp: 'hiervor möchte ich warnen.'\n",
      "Original 'in writing. - (pt) it is essential to harmonise national safety procedures in member states.', got 'schriftlich. - (pt) es ist notwendig, die nationalen sicherheitsverfahren in den mitgliedstaaten zu harmonisieren.', exp: 'schriftlich. - (pt) es kommt jetzt darauf an, die nationalen sicherheitsverfahren in den mitgliedstaaten zu harmonisieren.'\n",
      "Original 'we know that shipbuilding is the last industrial sector to be subsidised.', got 'wir wissen, dass schiffbau der letzten industriellen industriektor subventioniert werden.', exp: 'wir wissen, dass der schiffbau der letzte subventionierte industriesektor ist.'\n",
      "Original 'we spent 0 million euros on this project in 0, and 0 million euros in 0.', got 'wir haben 0 millionen euro für dieses projekt 0 millionen euro für dieses projekt ausgegeben und 0 millionen euro im jahr 0 ausgegeben.', exp: 'im jahr 0 haben wir 0 millionen euro für dieses projekt ausgegeben, 0 dann 0 millionen euro.'\n",
      "Original 'the eventual solution to the chinese challenge is to be found not in china, but in africa itself.', got 'die gleiche lösung der chinesischen herausforderung ist nicht in china, sondern in afrika selbst.', exp: 'letztendlich liegt die antwort auf die chinesische herausforderung nicht in china, sondern in afrika selbst.'\n",
      "Original \"we in parliament will say 'yes' to new taxes but 'no' to more taxes.\", got 'wir im parlament werden \\'ja\\' zu neuen steuern sagen: \"ein \"nein mehr steuern\".', exp: 'wir im parlament werden neuen steuern zustimmen, zusätzlichen steuern jedoch eine klare absage erteilen.'\n",
      "Original 'however, in my view we must not rest on our laurels.', got 'aber ich meine, wir dürfen unsere lorbe nicht wiederholen.', exp: 'meiner ansicht nach dürfen wir uns jedoch nicht auf unseren lorbeeren ausruhen.'\n",
      "Original 'i shall confine my comments to a few basic issues.', got 'ich werde mich auf einige grundlegende fragen beschränken.', exp: 'im folgenden werde ich mich auf einige grundlegende themen konzentrieren.'\n",
      "Original 'the greatest anxiety is being aroused by the damage to the fukushima nuclear power station.', got 'die größte freude wird durch den schäden des fukushima atomkraftwerk gestaltet.', exp: 'die größte sorge wird durch den schaden am kernkraftwerk fukushima erweckt.'\n",
      "Original 'much is only on paper and unfortunately the reality is different.', got 'vielleicht ist es nur auf papier und bedauerlicherweise die realität anders.', exp: 'vieles steht nur auf dem papier, die realität sieht leider anders aus.'\n",
      "Original 'this would not help anyone, neither the assistants themselves nor the members of parliament.', got 'dies würde nichts helfen, weder die hilfe selbst noch die mitglieder des parlaments.', exp: 'damit wäre niemandem geholfen, weder den assistenten noch den abgeordneten hier im hause.'\n",
      "Original 'can civil society in russia expect nothing more than this kind of provocation?', got 'kann die zivilgesellschaft in russland nicht mehr als eine solche provokation erwarten?', exp: 'hat die zivilgesellschaft in russland nur provokationen zu erwarten?'\n"
     ]
    }
   ],
   "source": [
    "# Performance on validation set\n",
    "val_df = europarl.df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-22T18:51:57.786432Z",
     "start_time": "2018-06-22T18:49:22.700180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8b6161acaa44daaec8c231d09ff666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.23604389490531522\n"
     ]
    }
   ],
   "source": [
    "bleu = bleu_scores_europarl(\n",
    "    input_texts=europarl.df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    target_texts=europarl.df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    predict=lambda text: predict(text)\n",
    ")\n",
    "print(f'average BLEU on test set = {bleu.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.995932Z",
     "start_time": "2018-05-13T18:17:20.994111Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The translations for the train and validations are decent. The main problem without attention was that the rnn decoder repeated itself and got confused what it intended to say (like \"leider geht es nur um die realität und die realität.\") The bleu score also improved from 0.183 to 0.237.\n",
    "\n",
    "Now, the main problems are any words out of the vocabulary (meaning not seen often enough in the training set). Some of the problems will go away if we use the whole training data set instead of a subset. But in the long run, we might need a copy mechanism, a look up method and/or a dynamic neural network memory. Also, some translations make grammatical mistakes. I can imagine that multi learning (like also learn grammatical features or entity recognition maybe in combination with some data augmentation) would help. It's also somehow funny to see how accurately the translations are for complicated sentences from the validation set, but for my very simple own examples that are different to europarlament discussion, the translations are terrible.\n",
    "\n",
    "Regarding tensorflow, allthough the code is a bit longer, it doesn't need much energy to figure what's going on. Every line manipulates the computational graph without much surprises. Allthough, debugging is still a hassle (cryptic error messages and no direct evaluation), it's not so mind blogging like working on different abstraction levels with Keras IMHO. It's also nice to see that a lot state-of-the art technology from research is already implemented in tensorflow. The computational speed went up here a lot, too. I think the main reason is that the tensorflow implementation only trains till end-of-sentence and not beyond (where Keras implementation masked them only to ignore in loss function). \n",
    "\n",
    "My next step will be to train the neural machine translation on the whole europarliament dataset (2.3 Mio > 600k) and also train for a longer time (even here the training did not converge after 20 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 294,
   "position": {
    "height": "40px",
    "left": "553px",
    "right": "192px",
    "top": "132px",
    "width": "615px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
