{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Multiple Layers for Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing [Attention](AttentionModelForMachineTranslationWithTensorflow.ipynb), I'll come back first to examine whether it's worth to implement multiple layers (of encoders and decoders) instead of only one. Of course, Deep Learning is all about deep nets, at some point I'll have to check it.\n",
    "\n",
    "In my first attempts I rejected it as a simple 2 layer encoder didn't result in significant improvements. After reading [The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation](https://arxiv.org/abs/1804.09849) and looking into the [Google NMT](https://github.com/tensorflow/nmt) project, I realized that multiple layers are worth it if done correctly.\n",
    "\n",
    "Here I made several hyper parameter decisions to naive stacking:\n",
    "* _Residual Connections_: While I knew residuals from CNNs like [ResNet](https://arxiv.org/abs/1512.03385), I didn't realize how important they are for stacked RNNs. In CNNs they are crucial for very deep networks, but here they already matter for a two layers of RNNs. A bit disappointing is that implementing a bidirectional stacked RNN is still more difficulty than it should be. In the end I copy+pasted a `ExtendedMultiRNNCell` implementation from the Google NMT project. It's easy to make subtle mistakes where the computation graph compiles and is trainable, but in the end performs poor.\n",
    "* _L2 Regularization_: For a simple 1-layer RNN the dropout is enough regularization. When I experimented with L2 regularization for 1 layer RNN I found a decrease in performance. For multiple layers it seems useful to have a *low* l2 regularizer so the layers can learn better. As it might be counterproductive, I don't l2 regularize the embeddings (they are anyway pretrained, so they only need some fine adjustments)\n",
    "* _Learning Rate_: I preferred not to handcraft one, but here I followed the advice I found to have three different phases. A warmup phase with linear increasing learning rate but without training the predefined embeddings. A second phase where everything is trained with default learning rate. And a third phase with an exponential learning rate decay. The idea is to first find a solid initialization for the layer RNNs, then train them till a first saturation and then lower the learning rate so it can make progress (allthough a bit slower).\n",
    "* _Epochs_: I had to increase the number of epochs. Adjusting the learning rate to a lower value always needs more epochs. Also as there are much more parameters with multiple layers (that are interconnecting) it also needs more time for a saturation. So, beside needing more time anyway to train layer times more RNN parameters, we also need more epochs, so in the end we need much more time.\n",
    "* _Layers_: Google is funny. Google NMT works with 8 layers as they can run each layer on a seperate GPU instance in parallel. Well, I don't have the resources, so all I can do here is to implement 2 layers. I'd love to have >= 4 layers (so they are looking a bit above word levels), but that's not doable here for a side project.\n",
    "* _Label smoothing_: Would be nice to have. But for it, we would need to one-hot-encode the RNN outputs what would drastically increase the computational effort. Look to a [stackoverflow](https://stackoverflow.com/questions/49136472/tensorflow-sequence-loss-with-label-smoothing) entry about. The (hidden) usage of `sparse_softmax_cross_entropy` instead of `softmax_cross_entropy` might be a big reason why my tensorflow solution is much more efficient in computing time to the keras solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T09:15:08.371279Z",
     "start_time": "2018-07-01T09:15:06.369938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed random seed to 42\n",
      "Availabe devices: [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14586226358551728020\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7766337127\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14503000407706529000\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Cuda/Cudnn/GPU works as intended\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.python.util import nest\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils.download import download_and_extract_resources\n",
    "from utils.linguistic import bleu_scores_europarl, preprocess_input_europarl as preprocess\n",
    "from utils.preparation import check_gpu_working, Europarl, RANDOM_STATE\n",
    "\n",
    "check_gpu_working()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T09:15:08.375598Z",
     "start_time": "2018-07-01T09:15:08.372570Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 400\n",
    "MAX_TARGET_LENGTH = 450\n",
    "LATENT_DIM = 256\n",
    "LAYERS = 4\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "DROPOUT = 0.25  # Dropout on input and output for the RNN cells, so effective dropout is 0.5, but works slightly better so\n",
    "TEST_SIZE = 2500\n",
    "BEAM_WIDTH = 5\n",
    "EMBEDDING_TRAINABLE = True  # Improves results significant and for at least it's not the most dominant training time factor (that's the output softmax layer)\n",
    "WARMUP_EPOCHS = 5\n",
    "LEARNING_RATE_DECAY_START_EPOCH = 10\n",
    "LEARNING_RATE_DECAY_RATE = 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T09:15:57.227390Z",
     "start_time": "2018-07-01T09:15:08.377133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n",
      "en.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "en.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (6.2 MB)\n",
      "de.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "de.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (5.7 MB)\n",
      "Total number of unfiltered translations 1920209\n",
      "Filtered translations with length between (1, input=100/target=125) characters: 597331\n"
     ]
    }
   ],
   "source": [
    "europarl = Europarl()\n",
    "download_and_extract_resources(fnames_and_urls=europarl.external_resources, dest_path=europarl.path)\n",
    "europarl.load_and_preprocess(max_input_length=MAX_INPUT_LENGTH, max_target_length=MAX_TARGET_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T09:15:57.421663Z",
     "start_time": "2018-07-01T09:15:57.229392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resumption of the session</td>\n",
       "      <td>wiederaufnahme der sitzungsperiode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>please rise, then, for this minute' s silence.</td>\n",
       "      <td>ich bitte sie, sich zu einer schweigeminute zu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(the house rose and observed a minute' s silence)</td>\n",
       "      <td>(das parlament erhebt sich zu einer schweigemi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>madam president, on a point of order.</td>\n",
       "      <td>frau präsidentin, zur geschäftsordnung.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>if the house agrees, i shall do as mr evans ha...</td>\n",
       "      <td>wenn das haus damit einverstanden ist, werde i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_texts  \\\n",
       "0                           resumption of the session   \n",
       "5      please rise, then, for this minute' s silence.   \n",
       "6   (the house rose and observed a minute' s silence)   \n",
       "7               madam president, on a point of order.   \n",
       "12  if the house agrees, i shall do as mr evans ha...   \n",
       "\n",
       "                                         target_texts  \n",
       "0                  wiederaufnahme der sitzungsperiode  \n",
       "5   ich bitte sie, sich zu einer schweigeminute zu...  \n",
       "6   (das parlament erhebt sich zu einer schweigemi...  \n",
       "7             frau präsidentin, zur geschäftsordnung.  \n",
       "12  wenn das haus damit einverstanden ist, werde i...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl.df[['input_texts', 'target_texts']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T09:15:57.426636Z",
     "start_time": "2018-07-01T09:15:57.423422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English subwords ['▁this', '▁is', '▁a', '▁test', '▁for', '▁pre', 'tr', 'ained', '▁by', 'te', 'pa', 'ire', 'm', 'bed', 'd', 'ings']\n",
      "German subwords ['▁das', '▁ist', '▁ein', '▁test', '▁für', '▁v', 'ort', 'rain', 'ierte', '▁zeich', 'eng', 'ruppen']\n"
     ]
    }
   ],
   "source": [
    "print(\"English subwords\", europarl.bpe_input.sentencepiece.EncodeAsPieces(\"this is a test for pretrained bytepairembeddings\"))\n",
    "print(\"German subwords\", europarl.bpe_target.sentencepiece.EncodeAsPieces(\"das ist ein test für vortrainierte zeichengruppen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T09:15:57.699221Z",
     "start_time": "2018-07-01T09:15:57.428732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 71)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Those will be the inputs for the seq2seq model (that needs to know how long the sequences can get)\n",
    "max_len_input = europarl.df.input_sequences.apply(len).max()\n",
    "max_len_target = europarl.df.target_sequences.apply(len).max()\n",
    "(max_len_input, max_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T09:15:57.735884Z",
     "start_time": "2018-07-01T09:15:57.701214Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(europarl.df.shape[0]), test_size=0.1, random_state=RANDOM_STATE)  # fixed random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T09:15:57.749322Z",
     "start_time": "2018-07-01T09:15:57.737776Z"
    }
   },
   "outputs": [],
   "source": [
    "# copy+pasted from https://github.com/google/seq2seq/blob/7f485894d412e8d81ce0e07977831865e44309ce/seq2seq/contrib/rnn_cell.py#L39\n",
    "class ExtendedMultiRNNCell(tf.nn.rnn_cell.MultiRNNCell):\n",
    "    \"\"\"Extends the Tensorflow MultiRNNCell with residual connections\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cells,\n",
    "        residual_connections=False,\n",
    "        residual_combiner=\"add\",\n",
    "        residual_dense=False\n",
    "    ):\n",
    "        \"\"\"Create a RNN cell composed sequentially of a number of RNNCells.\n",
    "        Args:\n",
    "          cells: list of RNNCells that will be composed in this order.\n",
    "          state_is_tuple: If True, accepted and returned states are n-tuples, where\n",
    "            `n = len(cells)`.  If False, the states are all\n",
    "            concatenated along the column axis.  This latter behavior will soon be\n",
    "            deprecated.\n",
    "          residual_connections: If true, add residual connections between all cells.\n",
    "            This requires all cells to have the same output_size. Also, iff the\n",
    "            input size is not equal to the cell output size, a linear transform\n",
    "            is added before the first layer.\n",
    "          residual_combiner: One of \"add\" or \"concat\". To create inputs for layer\n",
    "            t+1 either \"add\" the inputs from the prev layer or concat them.\n",
    "          residual_dense: Densely connect each layer to all other layers\n",
    "        Raises:\n",
    "          ValueError: if cells is empty (not allowed), or at least one of the cells\n",
    "            returns a state tuple but the flag `state_is_tuple` is `False`.\n",
    "        \"\"\"\n",
    "        super(ExtendedMultiRNNCell, self).__init__(cells, state_is_tuple=True)\n",
    "        assert residual_combiner in [\"add\", \"concat\", \"mean\"]\n",
    "\n",
    "        self._residual_connections = residual_connections\n",
    "        self._residual_combiner = residual_combiner\n",
    "        self._residual_dense = residual_dense\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Run this multi-layer cell on inputs, starting from state.\"\"\"\n",
    "        if not self._residual_connections:\n",
    "            return super(ExtendedMultiRNNCell, self).__call__(\n",
    "                inputs, state, (scope or \"extended_multi_rnn_cell\")\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope(scope or \"extended_multi_rnn_cell\"):\n",
    "            # Adding Residual connections are only possible when input and output\n",
    "            # sizes are equal. Optionally transform the initial inputs to\n",
    "            # `cell[0].output_size`\n",
    "            if self._cells[0].output_size != inputs.get_shape().as_list()[1] and (self._residual_combiner in [\"add\", \"mean\"]):\n",
    "                inputs = tf.contrib.layers.fully_connected(\n",
    "                    inputs=inputs,\n",
    "                    num_outputs=self._cells[0].output_size,\n",
    "                    activation_fn=None,\n",
    "                    scope=\"input_transform\"\n",
    "                )\n",
    "\n",
    "            # Iterate through all layers (code from MultiRNNCell)\n",
    "            cur_inp = inputs\n",
    "            prev_inputs = [cur_inp]\n",
    "            new_states = []\n",
    "            for i, cell in enumerate(self._cells):\n",
    "                with tf.variable_scope(\"cell_%d\" % i):\n",
    "                    if not nest.is_sequence(state):\n",
    "                        raise ValueError(\n",
    "                            \"Expected state to be a tuple of length %d, but received: %s\" %\n",
    "                            (len(self.state_size), state)\n",
    "                        )\n",
    "                    cur_state = state[i]\n",
    "                    next_input, new_state = cell(cur_inp, cur_state)\n",
    "\n",
    "                    # Either combine all previous inputs or only the current input\n",
    "                    input_to_combine = prev_inputs[-1:]\n",
    "                    if self._residual_dense:\n",
    "                        input_to_combine = prev_inputs\n",
    "\n",
    "                    # Add Residual connection\n",
    "                    if self._residual_combiner == \"add\":\n",
    "                        next_input = next_input + sum(input_to_combine)\n",
    "                    if self._residual_combiner == \"mean\":\n",
    "                        combined_mean = tf.reduce_mean(tf.stack(input_to_combine), 0)\n",
    "                        next_input = next_input + combined_mean\n",
    "                    elif self._residual_combiner == \"concat\":\n",
    "                        next_input = tf.concat([next_input] + input_to_combine, 1)\n",
    "\n",
    "                    cur_inp = next_input\n",
    "                    prev_inputs.append(cur_inp)\n",
    "\n",
    "                    new_states.append(new_state)\n",
    "        new_states = (tuple(new_states) if self._state_is_tuple else array_ops.concat(new_states, 1))\n",
    "        return cur_inp, new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T09:16:12.726508Z",
     "start_time": "2018-07-01T09:15:57.751640Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    encoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_input\n",
    "        dtype=tf.int32,\n",
    "        name='encoder_inputs' \n",
    "    )\n",
    "    batch_size = tf.shape(encoder_inputs)[0]\n",
    "    beam_width = tf.placeholder_with_default(1, shape=[])\n",
    "    dropout = tf.placeholder_with_default(tf.cast(0.0, tf.float32), shape=[])\n",
    "    keep_prob = tf.cast(1.0, tf.float32) - dropout\n",
    "    learning_rate = tf.placeholder_with_default(tf.cast(1e-3, tf.float32), shape=[])\n",
    "\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", \n",
    "        initializer=tf.constant(europarl.bpe_input.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder,\n",
    "        encoder_inputs,\n",
    "        name=\"encoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    input_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='input_sequence_length'\n",
    "    )\n",
    "    \n",
    "    rnn_cell_type = tf.nn.rnn_cell.GRUCell\n",
    "    encoder_forward_cells = [\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name=f'encoder_forward_cell{layer}') \n",
    "        for layer in range(LAYERS)\n",
    "    ]\n",
    "    encoder_backward_cells = [\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name=f'encoder_backward_cell{layer}') \n",
    "        for layer in range(LAYERS)\n",
    "    ]\n",
    "    encoder_forward_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        cell,\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    ) for cell in encoder_forward_cells]\n",
    "    encoder_backward_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        cell,\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    ) for cell in encoder_backward_cells]\n",
    "    encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "        cells_fw = ExtendedMultiRNNCell(encoder_forward_cells, residual_connections=True)._cells,\n",
    "        cells_bw = ExtendedMultiRNNCell(encoder_backward_cells, residual_connections=True)._cells,\n",
    "        inputs=encoder_emb_inp,\n",
    "        sequence_length=input_sequence_length,\n",
    "        time_major=False,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    encoder_state = tf.concat([encoder_state_fw[-1], encoder_state_bw[-1]], -1)\n",
    "    \n",
    "    # Regarding time_major:\n",
    "    # If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "    # If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "    # Using `time_major = True` is a bit more efficient because it avoids\n",
    "    # transposes at the beginning and end of the RNN calculation.  However,\n",
    "    # most TensorFlow data is batch-major, so by default this function\n",
    "    # accepts input and emits output in batch-major form.\n",
    "    #\n",
    "    # for simplicity I work with batch major here instead of time_major\n",
    "    # so I don't need to transpose inputs and transpose back for attention mechanism\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_inputs' \n",
    "    )\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", \n",
    "        initializer=tf.constant(europarl.bpe_target.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder,\n",
    "        decoder_inputs,\n",
    "        name=\"decoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    target_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='target_sequence_length'\n",
    "    )\n",
    "    \n",
    "    # tiling is necessary to work with BeamSearchDecoder\n",
    "    # read carefully the NOTE on constructor in\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper \n",
    "    tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(encoder_outputs, multiplier=beam_width)\n",
    "    tiled_encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width)\n",
    "    tiled_sequence_length = tf.contrib.seq2seq.tile_batch(input_sequence_length, multiplier=beam_width)\n",
    "    \n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        LATENT_DIM,\n",
    "        memory=tiled_encoder_outputs,\n",
    "        memory_sequence_length=tiled_sequence_length,\n",
    "        dtype=tf.float32,\n",
    "        name='attention_mechanism',\n",
    "    )\n",
    "    decoder_rnn_cells = [rnn_cell_type(num_units=LATENT_DIM, name=f'decoder_cell{layer}') for layer in range(LAYERS)]\n",
    "    def residual_fn(inputs, outputs):\n",
    "        tf.contrib.framework.nest.assert_same_structure(inputs, outputs)\n",
    "        inputs_without_attention = tf.slice(inputs, [0, 0], [batch_size, LATENT_DIM])\n",
    "        return tf.contrib.framework.nest.map_structure(lambda inp, out: inp + out, inputs_without_attention, outputs) \n",
    "    for layer in range(1, LAYERS):\n",
    "        decoder_rnn_cells[layer] = tf.contrib.rnn.ResidualWrapper(\n",
    "            decoder_rnn_cells[layer],\n",
    "            residual_fn=residual_fn\n",
    "        )\n",
    "    decoder_rnn_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        cell,\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    ) for cell in decoder_rnn_cells]\n",
    "    attention_cells = [tf.contrib.seq2seq.AttentionWrapper(\n",
    "        cell,\n",
    "        attention_mechanism,\n",
    "        attention_layer_size=LATENT_DIM,\n",
    "        name=f'attention_wrapper{layer}',\n",
    "    ) for layer, cell in enumerate(decoder_rnn_cells)] \n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell(attention_cells)\n",
    "\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        inputs=decoder_emb_inp, \n",
    "        sequence_length=target_sequence_length,\n",
    "        time_major=False,\n",
    "        name=\"decoder_training_helper\",\n",
    "    )\n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        units=len(europarl.bpe_target.tokens),\n",
    "        use_bias=False,\n",
    "        name='projection_layer',\n",
    "    )\n",
    "    \n",
    "    initial_state = tuple(\n",
    "        attention_cells[0].zero_state(dtype=tf.float32, batch_size=batch_size).clone(\n",
    "            cell_state=encoder_state\n",
    "        )\n",
    "        for _ in range(LAYERS)\n",
    "    )\n",
    "\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=decoder_cell,\n",
    "        helper=training_helper,\n",
    "        initial_state=initial_state,\n",
    "        output_layer=projection_layer,\n",
    "    )\n",
    "    outputs, _final_state, _final_sequence_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder,\n",
    "        output_time_major=False,\n",
    "        impute_finished=False,\n",
    "    )\n",
    "    logits = outputs.rnn_output\n",
    "    \n",
    "    decoder_outputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_outputs',\n",
    "    )\n",
    "    target_weights = tf.cast(tf.sequence_mask(target_sequence_length), dtype=tf.float32)\n",
    "    l2_lambda = tf.placeholder_with_default(tf.cast(1e-5, tf.float32), shape=[])\n",
    "    loss_l2 = tf.add_n([\n",
    "        tf.nn.l2_loss(v)\n",
    "        for v in tf.trainable_variables()\n",
    "        if not re.match(r'embedding_(de|en)coder', v.name)  # don't regularize embeddings\n",
    "    ])\n",
    "    train_loss = tf.contrib.seq2seq.sequence_loss(logits, decoder_outputs, target_weights) + l2_lambda * loss_l2\n",
    "\n",
    "    params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, params)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "        t_list=gradients,\n",
    "        clip_norm=1.,\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    update_step = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "    \n",
    "    params_without_embeddings = [v for v in tf.trainable_variables() if not re.match(r'embedding_(de|en)coder', v.name)]\n",
    "    gradients_without_embeddings = tf.gradients(train_loss, params_without_embeddings)\n",
    "    clipped_gradients_without_embeddings, _ = tf.clip_by_global_norm(\n",
    "        t_list=gradients_without_embeddings,\n",
    "        clip_norm=1.,\n",
    "    )\n",
    "    update_step_without_embeddings = optimizer.apply_gradients(zip(clipped_gradients_without_embeddings, params_without_embeddings))\n",
    "    \n",
    "    inference_decoder_initial_state = tuple(\n",
    "        attention_cells[0].zero_state(\n",
    "            dtype=tf.float32,\n",
    "            batch_size=batch_size * beam_width  # tricky and somehow unintuitive, but necessary\n",
    "        ).clone(\n",
    "            cell_state=tiled_encoder_state\n",
    "        ) for _ in range(LAYERS)\n",
    "    )\n",
    "    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "        cell=decoder_cell,\n",
    "        embedding=embedding_decoder,\n",
    "        start_tokens=tf.fill([batch_size], europarl.bpe_target.start_token_idx),\n",
    "        end_token=europarl.bpe_target.stop_token_idx,\n",
    "        initial_state=inference_decoder_initial_state,\n",
    "        beam_width=BEAM_WIDTH,\n",
    "        output_layer=projection_layer,\n",
    "        length_penalty_weight=1.0,  # https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/\n",
    "    )\n",
    "\n",
    "    \n",
    "    inference_outputs, _inference_final_state, _inference_final_sequence_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inference_decoder,\n",
    "        maximum_iterations=tf.round(tf.reduce_max(input_sequence_length) * 2),  # a bit more flexible than max_len_target\n",
    "        impute_finished=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T09:16:12.739755Z",
     "start_time": "2018-07-01T09:16:12.727895Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_train_batch(batch_ids, epoch):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    base_lr = 1e-3\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        lr = base_lr * (0.5 + epoch / (2 * WARMUP_EPOCHS))\n",
    "    elif epoch < LEARNING_RATE_DECAY_START_EPOCH:\n",
    "        lr = base_lr\n",
    "    else:\n",
    "        lr = base_lr * (LEARNING_RATE_DECAY_RATE ** (epoch - LEARNING_RATE_DECAY_START_EPOCH))\n",
    "    pred, loss, _ = sess.run(\n",
    "        fetches=[\n",
    "            outputs, train_loss, update_step if epoch >= WARMUP_EPOCHS else update_step_without_embeddings\n",
    "        ],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "            dropout: DROPOUT,\n",
    "            # embedding_trainable: epoch >= WARMUP_EPOCHS,\n",
    "            learning_rate: lr\n",
    "        }\n",
    "    )\n",
    "    return loss, lr\n",
    "\n",
    "def run_val_batch(batch_ids):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    loss = sess.run(\n",
    "        fetches=[train_loss],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def run_validation_loss():\n",
    "    return np.mean([\n",
    "        run_val_batch(ids)\n",
    "        for ids \n",
    "        in np.array_split(val_ids, np.ceil(len(val_ids) / BATCH_SIZE))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T09:16:12.747961Z",
     "start_time": "2018-07-01T09:16:12.740842Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    sequenced = europarl.bpe_input.subword_indices(preprocess(sentence))\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [sequenced],\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    \n",
    "    beam_search_output = sess.run(\n",
    "        fetches=[inference_outputs],\n",
    "        feed_dict={\n",
    "            encoder_inputs: padded,\n",
    "            input_sequence_length: [len(sequenced)],\n",
    "            beam_width: BEAM_WIDTH,\n",
    "        }\n",
    "    )[0]\n",
    "    \n",
    "    return europarl.bpe_target.sentencepiece.DecodePieces([\n",
    "        europarl.bpe_target.tokens[idx] for idx in beam_search_output.predicted_ids[0, :, 0].tolist()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T21:06:36.684307Z",
     "start_time": "2018-07-01T09:16:12.749094Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6d4c31f0d447ed8dec5fdd4467af3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0005, train_loss=4.2281, val_loss=3.21933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96528a21a694a23a8741e5761363807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0006, train_loss=3.48042, val_loss=2.88056\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf93d0abe7f49c598e4da428ce1cec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0007, train_loss=3.25228, val_loss=2.70757\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3f7da683134eb788e2ddce2eba75ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0008, train_loss=3.11722, val_loss=2.60041\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be47443510384857b52aea751737b851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0009, train_loss=3.04331, val_loss=2.54378\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681879ea0ce846208129f473433442e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.15314623620703838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29411228f8b47e2b24a18185601cfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.78206, val_loss=2.29009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d818e112f154af3abaee75e5de9ef57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.62561, val_loss=2.20004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad36e74ac6c4fafbfa5cb4bc77e3db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 8', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.54937, val_loss=2.14822\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef08fad8b8644898c3f762813bb5db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 9', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.50101, val_loss=2.11275\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8efc36d6e6946719fbb189aabe925f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 10', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.46584, val_loss=2.08719\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae6f8ef41cf42fe82e78b3662727669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.1982994980981908\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1fcb88e2f747389dfe821476a1d12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 11', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.4391, val_loss=2.07611\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd26955c4f34b658c2c08126df6e1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 12', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.41793, val_loss=2.05399\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9aaf70daa704869ab7e17ccfed8895a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 13', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.40007, val_loss=2.03909\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb3902557b8418d8649c5d38516f99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 14', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.38616, val_loss=2.02864\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5dd274704c341c893ef45fd2c326b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 15', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.37427, val_loss=2.01273\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc42d9811061416fb40640f878587c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.2085501686379793\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efeead14c704c29b6cc42387978a66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 16', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000598737, train_loss=2.24964, val_loss=1.92147\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0170300a7bc4e8b9e60f118cf4d2fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 17', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0005688, train_loss=2.20364, val_loss=1.89161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a8d4668cf34dda8287f61816101551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 18', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.00054036, train_loss=2.17613, val_loss=1.86947\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5ef59452dc4a9a98498b0c883fd39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 19', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000513342, train_loss=2.15365, val_loss=1.86015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a44c0226ec4fbcafca1443b217ffd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 20', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000487675, train_loss=2.13374, val_loss=1.8484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a02f198eff490183a33b9f77c7496f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.22575404068898233\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eac5290eefa42be916b6182f637a9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 21', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000463291, train_loss=2.11625, val_loss=1.83358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2b8d67ea3a4b28bbe0abe16be5f2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 22', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000440127, train_loss=2.09912, val_loss=1.81799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c885f61dc1804f94ae708d698478f163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 23', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.00041812, train_loss=2.08414, val_loss=1.80434\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cad4ff4133f428aaf0c7c1de97cf8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 24', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000397214, train_loss=2.07035, val_loss=1.79862\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d04ab193b3a4f6db7cd4c7ae73f56a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 25', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000377354, train_loss=2.05625, val_loss=1.78298\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(\n",
    "    allow_soft_placement=True,  # needed as recommendation from https://github.com/tensorflow/tensorflow/issues/2292\n",
    "    log_device_placement=True,\n",
    ")\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batches_per_epoch = np.ceil(len(train_ids) / BATCH_SIZE)\n",
    "for epoch in range(EPOCHS):\n",
    "    shuffled_ids = np.random.permutation(train_ids)\n",
    "    batch_splits = np.array_split(shuffled_ids, batches_per_epoch)\n",
    "    train_losses = []\n",
    "    N = len(batch_splits)\n",
    "    with tqdm(batch_splits, desc=f\"Epoch {epoch+1}\") as t:\n",
    "        for train_batch_ids in t:\n",
    "            batch_loss, lr = run_train_batch(train_batch_ids, epoch=epoch)\n",
    "            train_losses.append(batch_loss)\n",
    "            t.set_postfix(train_loss=np.mean(train_losses))\n",
    "        print(f\"learning rate={lr:.6}, train_loss={np.mean(train_losses):.6}, val_loss={run_validation_loss():.6}\")\n",
    "    \n",
    "    if epoch > 0 and ((epoch+1) % 5 == 0) and (epoch+1 < EPOCHS):\n",
    "        bleu = bleu_scores_europarl(\n",
    "            input_texts=europarl.df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "            target_texts=europarl.df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "            predict=lambda text: predict(text)\n",
    "        )\n",
    "        print(f'average BLEU on test set = {bleu.mean()}')\n",
    "        \n",
    "validation_input_sequences = europarl.df.input_sequences.iloc[val_ids[:BATCH_SIZE]]\n",
    "validation_input_lengths = validation_input_sequences.apply(len)\n",
    "\n",
    "validation_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    validation_input_sequences,\n",
    "    maxlen=max_len_input,\n",
    "    dtype=int,\n",
    "    padding='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T21:06:39.103511Z",
     "start_time": "2018-07-01T21:06:36.685927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/tfattentionmodel_4layers.ckpt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = f'tfattentionmodel_{LAYERS}layers'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, f\"data/{name}.ckpt\")\n",
    "# tfattentionmodel.ckpt.index https://drive.google.com/open?id=1Xv2Qc9Gnac_Of9bIpQef5LZJnX9ij933\n",
    "# tfattentionmodel.cpkt.meta https://drive.google.com/open?id=162WY8XEjyqfvitiIBfmq9oHLC1rQhyvr\n",
    "# tfattentionmodel.cpkt.data-00000-of-00001 https://drive.google.com/open?id=1racW0agvk5nJ_xBaxifAaEWM8T9ot7FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T21:06:39.734028Z",
     "start_time": "2018-07-01T21:06:39.106042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello.' --> 'helfen.'\n",
      "'you are welcome.' --> 'sie begrüßen sie.'\n",
      "'how do you do?' --> 'wie sieht sie?'\n",
      "'i hate mondays.' --> 'ich habe meine ausführungen.'\n",
      "'i am a programmer.' --> 'ich bin ein land.'\n",
      "'data is the new oil.' --> 'daten ist das neue öl.'\n",
      "'it could be worse.' --> 'es könnte schlimmer sein.'\n",
      "'i am on top of it.' --> 'ich bin über das thema.'\n",
      "'n° uno' --> 'nizo-oo'\n",
      "'awesome!' --> 'das will!'\n",
      "'put your feet up!' --> 'lassen sie ihre füße aufmerksam machen!'\n",
      "'from the start till the end!' --> 'anfang des beginns!'\n",
      "'from dusk till dawn.' --> 'aus dusking-bericht.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T21:06:42.008749Z",
     "start_time": "2018-07-01T21:06:39.735332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original \"please rise, then, for this minute' s silence.\", got 'iitte bitte sie, diese schweigeminute zu verstehen.', exp: 'ich bitte sie, sich zu einer schweigeminute zu erheben.'\n",
      "Original \"(the house rose and observed a minute' s silence)\", got '(das parlament erhebt sich zu einer schweigeminute.)', exp: '(das parlament erhebt sich zu einer schweigeminute.)'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'if the house agrees, i shall do as mr evans has suggested.', got 'wenn das parlament einverstanden ist, werde ich tun, wie herr evans vorgeschlagen hat.', exp: 'wenn das haus damit einverstanden ist, werde ich dem vorschlag von herrn evans folgen.'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'i would like your advice about rule 0 concerning inadmissibility.', got 'ich möchte gerne für die verlegung in artikel 0 über die unverzügliche zulassung kommen.', exp: 'könnten sie mir eine auskunft zu artikel 0 im zusammenhang mit der unzulässigkeit geben?'\n",
      "Original 'it says that this should be done despite the principle of relative stability.', got 'es sagt, dass dies trotz des grundsatzes der relativen stabilität geschehen sollte.', exp: 'und zwar sollen derartige strafen trotz des grundsatzes der relativen stabilität verhängt werden.'\n",
      "Original 'this is all in accordance with the principles that we have always upheld.', got 'das ist alles mit den prinzipien, die wir immer noch immer vorgeschlagen haben.', exp: 'all dies entspricht den grundsätzen, die wir stets verteidigt haben.'\n",
      "Original 'thank you, mr segni, i shall do so gladly.', got 'vielen dank, herr segni, ich werde dies sehr freuen.', exp: 'vielen dank, herr segni, das will ich gerne tun.'\n",
      "Original 'indeed, it is quite in keeping with the positions this house has always adopted.', got 'es ist in der tat auch gut,, die position dieses hauses zu verabschieden.', exp: 'das ist ganz im sinne der position, die wir als parlament immer vertreten haben.'\n",
      "Original 'it is the case of alexander nikitin.', got 'es ist der fall alexander nikitin.', exp: 'das ist der fall von alexander nikitin.'\n",
      "Original 'now, however, he is to go before the courts once more because the public prosecutor is appealing.', got 'jetzt muss er vor den gerichten gefahren kommen, weil der staatsanwalt appelliert wird.', exp: 'nun ist es aber so, daß er wieder angeklagt werden soll, weil der staatsanwalt in berufung geht.'\n",
      "Original 'but, madam president, my personal request has not been met.', got 'frau präsidentin, mein persönliches antrag ist jedoch nicht erfüllt worden.', exp: 'dennoch, frau präsidentin, wurde meinem wunsch nicht entsprochen.'\n",
      "Original 'i would therefore once more ask you to ensure that we get a dutch channel as well.', got 'ich möchte daher bitten, dafür zu sorgen, dass wir einen niederländischen kankan haben.', exp: 'deshalb möchte ich sie nochmals ersuchen, dafür sorge zu tragen, daß auch ein niederländischer sender eingespeist wird.'\n",
      "Original 'it will, i hope, be examined in a positive light.', got 'ich hoffe, es wird in einem positiven licht geprüft.', exp: 'ich hoffe, daß dort in ihrem sinne entschieden wird.'\n",
      "Original 'why has no air quality test been done on this particular building since we were elected?', got 'warum wurde keine luftverkehrsbestandsbestandsbedars in diesem speziellen gebäude getan?', exp: 'weshalb wurde die luftqualität in diesem gebäude seit unserer wahl nicht ein einziges mal überprüft?'\n",
      "Original 'why has there been no health and safety committee meeting since 0?', got 'warum hat es keinen gesundheits- und sicherheitsausschuß seit 0 gegeben?', exp: 'weshalb ist der arbeitsschutzausschuß seit 0 nicht ein einziges mal zusammengetreten?'\n",
      "Original 'why are there no fire instructions?', got 'warum gibt es keine feueraufnahmen?', exp: 'warum finden keine brandschutzbelehrungen statt?'\n",
      "Original 'why have the staircases not been improved since my accident?', got 'warum wurden die strengeren fälle seit meinem vorfall verbessert?', exp: 'warum wurde nach meinem unfall nichts unternommen, um die treppen sicherer zu machen?'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in europarl.df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T21:06:44.348513Z",
     "start_time": "2018-07-01T21:06:42.010783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'we congratulate you on a job very well done.', got 'wir beglückwünschen sie zu einer arbeit sehr gut.', exp: 'wir gratulieren dir zu deiner hervorragenden arbeit.'\n",
      "Original 'in this case, i strongly disagree with what was said by the previous speaker, carl schlyter.', got 'in diesem fall stimme ich dem vorredner, herr schiedermeier, ganz wider.', exp: 'in diesem fall widerspreche ich klar dem, was der vorredner, carl schlyter, gesagt hat.'\n",
      "Original 'it only makes sense to rebuild these if the refugees who fled are coming back.', got 'es ist nur sinnvoll, dies zu wiederaufen, wenn die flüchtlinge zurückgekehrt werden.', exp: 'deren wiederaufbau ist nur dann von nutzen, wenn die flüchtlinge aus den betreffenden gebieten wieder zurückkehren.'\n",
      "Original 'eba: everything but arms.', got 'eba: alles, aber waffen.', exp: 'eba: everything but arms.'\n",
      "Original 'in wider terms, this directive is not ambitious enough.', got 'zudem ist diese richtlinie nicht nicht nicht nicht ehrgeizig.', exp: 'generell gesehen fehlt es dieser richtlinie an ambitioniertheit.'\n",
      "Original 'it is an irresponsible mistake to spend public money to maintain this fleet.', got 'es ist ein unverantwortliches fehler, das öffentlichen geld zu öffnen, um diese flotte zu erhalten.', exp: 'öffentliche gelder zur erhaltung dieser flotte zu verwenden ist ein verantwortungsloser fehler.'\n",
      "Original 'where better to ensure access to culture than through our libraries?', got 'wo besser ist es, den zugang zu kultur zu gewährleisten als durch unsere bibliotheken?', exp: 'wo ließe sich der zugang zu kultur besser sichern als durch unsere bibliotheken?'\n",
      "Original 'i would like to advise you against this.', got 'ich möchte ihnen dagegen vorstellen.', exp: 'hiervor möchte ich warnen.'\n",
      "Original 'in writing. - (pt) it is essential to harmonise national safety procedures in member states.', got 'schriftlich. - (pt) es ist wichtig, die nationalen sicherheitsverfahren in den mitgliedstaaten zu harmonisieren.', exp: 'schriftlich. - (pt) es kommt jetzt darauf an, die nationalen sicherheitsverfahren in den mitgliedstaaten zu harmonisieren.'\n",
      "Original 'we know that shipbuilding is the last industrial sector to be subsidised.', got 'wir wissen, dass schiffbau der letzte industriesektor subventioniert wird.', exp: 'wir wissen, dass der schiffbau der letzte subventionierte industriesektor ist.'\n",
      "Original 'we spent 0 million euros on this project in 0, and 0 million euros in 0.', got 'wir haben 0 millionen euro auf diesem projekt im jahr 0 und 0 millionen euro im jahr 0 ausgegeben.', exp: 'im jahr 0 haben wir 0 millionen euro für dieses projekt ausgegeben, 0 dann 0 millionen euro.'\n",
      "Original 'the eventual solution to the chinese challenge is to be found not in china, but in africa itself.', got 'die wahrliche lösung für die chinesische herausforderung ist nicht in china, sondern in afrika.', exp: 'letztendlich liegt die antwort auf die chinesische herausforderung nicht in china, sondern in afrika selbst.'\n",
      "Original \"we in parliament will say 'yes' to new taxes but 'no' to more taxes.\", got 'wir im parlament werden \"ja\" zu neuen steuern sagen, nein zu mehr steuern.', exp: 'wir im parlament werden neuen steuern zustimmen, zusätzlichen steuern jedoch eine klare absage erteilen.'\n",
      "Original 'however, in my view we must not rest on our laurels.', got 'aber ich glaube, wir dürfen nicht auf unsere lenume zurückkehren.', exp: 'meiner ansicht nach dürfen wir uns jedoch nicht auf unseren lorbeeren ausruhen.'\n",
      "Original 'i shall confine my comments to a few basic issues.', got 'ich werde meine bemerkungen zu einigen grundlegenden fragen beschränken.', exp: 'im folgenden werde ich mich auf einige grundlegende themen konzentrieren.'\n",
      "Original 'the greatest anxiety is being aroused by the damage to the fukushima nuclear power station.', got 'die größte besorgnis wird durch den schaden des kernkraftwerkes des fukushima-kraftwerkes geprüft.', exp: 'die größte sorge wird durch den schaden am kernkraftwerk fukushima erweckt.'\n",
      "Original 'much is only on paper and unfortunately the reality is different.', got 'vielleicht ist es nur auf papier, leider ist die realität anders.', exp: 'vieles steht nur auf dem papier, die realität sieht leider anders aus.'\n",
      "Original 'this would not help anyone, neither the assistants themselves nor the members of parliament.', got 'das würde niemand helfen, weder die assistenten selbst oder die mitglieder des parlaments.', exp: 'damit wäre niemandem geholfen, weder den assistenten noch den abgeordneten hier im hause.'\n",
      "Original 'can civil society in russia expect nothing more than this kind of provocation?', got 'kann die zivilgesellschaft in russland nicht mehr als diese provokation erwarten?', exp: 'hat die zivilgesellschaft in russland nur provokationen zu erwarten?'\n"
     ]
    }
   ],
   "source": [
    "# Performance on validation set\n",
    "val_df = europarl.df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-01T21:10:48.941539Z",
     "start_time": "2018-07-01T21:06:44.350100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c4924410e045899d77c32cb3726e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.22936980102002366\n"
     ]
    }
   ],
   "source": [
    "bleu = bleu_scores_europarl(\n",
    "    input_texts=europarl.df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    target_texts=europarl.df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    predict=lambda text: predict(text)\n",
    ")\n",
    "print(f'average BLEU on test set = {bleu.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.995932Z",
     "start_time": "2018-05-13T18:17:20.994111Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The translations for the train and validations are decent. The main problem without attention was that the rnn decoder repeated itself and got confused what it intended to say (like \"leider geht es nur um die realität und die realität.\") The bleu score also improved from 0.183 to 0.237.\n",
    "\n",
    "Now, the main problems are any words out of the vocabulary (meaning not seen often enough in the training set). Some of the problems will go away if we use the whole training data set instead of a subset. But in the long run, we might need a copy mechanism, a look up method and/or a dynamic neural network memory. Also, some translations make grammatical mistakes. I can imagine that multi learning (like also learn grammatical features or entity recognition maybe in combination with some data augmentation) would help. It's also somehow funny to see how accurately the translations are for complicated sentences from the validation set, but for my very simple own examples that are different to europarlament discussion, the translations are terrible.\n",
    "\n",
    "Regarding tensorflow, allthough the code is a bit longer, it doesn't need much energy to figure what's going on. Every line manipulates the computational graph without much surprises. Allthough, debugging is still a hassle (cryptic error messages and no direct evaluation), it's not so mind blogging like working on different abstraction levels with Keras IMHO. It's also nice to see that a lot state-of-the art technology from research is already implemented in tensorflow. The computational speed went up here a lot, too. I think the main reason is that the tensorflow implementation only trains till end-of-sentence and not beyond (where Keras implementation masked them only to ignore in loss function). \n",
    "\n",
    "My next step will be to train the neural machine translation on the whole europarliament dataset (2.3 Mio > 600k) and also train for a longer time (even here the training did not converge after 20 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 294,
   "position": {
    "height": "40px",
    "left": "553px",
    "right": "192px",
    "top": "132px",
    "width": "615px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
