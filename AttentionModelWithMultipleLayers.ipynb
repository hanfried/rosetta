{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Multiple Layers for Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing [Attention](AttentionModelForMachineTranslationWithTensorflow.ipynb), I'll come back first to examine whether it's worth to implement mulitple layers (of encoders and decoders) instead of only one. Of course, Deep Learning is all about deep nets, at some point I'll have to check it.\n",
    "\n",
    "Again, I'll try to refactor the code a bit, so we can concentrate on the stacking issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:34:40.124381Z",
     "start_time": "2018-06-28T17:34:38.000496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed random seed to 42\n",
      "Availabe devices: [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14234273424655731965\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7766772941\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14256217936594991556\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Cuda/Cudnn/GPU works as intended\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils.download import download_and_extract_resources\n",
    "from utils.linguistic import bleu_scores_europarl, preprocess_input_europarl as preprocess\n",
    "from utils.preparation import check_gpu_working, Europarl, RANDOM_STATE\n",
    "\n",
    "check_gpu_working()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:34:40.131577Z",
     "start_time": "2018-06-28T17:34:40.126338Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 10 # 100\n",
    "MAX_TARGET_LENGTH = 12 # 125\n",
    "LATENT_DIM = 256  # was 512, but we should be able to use a smaller hidden representation as we are looking back anyway as needed\n",
    "LAYERS = 2\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128  # was 64, but tensorflow implementation doesn't need so much GPU memory so can increase batch size\n",
    "DROPOUT = 0.25  # Dropout on input and output for the RNN cells, so effective dropout is 0.5, but works slightly better so\n",
    "TEST_SIZE = 2500\n",
    "BEAM_WIDTH = 5\n",
    "EMBEDDING_TRAINABLE = True  # Improves results significant and for at least it's not the most dominant training time factor (that's the output softmax layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:34:44.061921Z",
     "start_time": "2018-06-28T17:34:40.132947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n",
      "en.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "en.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (6.2 MB)\n",
      "de.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "de.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (5.7 MB)\n",
      "Total number of unfiltered translations 1920209\n",
      "Filtered translations with length between (1, input=10/target=12) characters: 2285\n"
     ]
    }
   ],
   "source": [
    "europarl = Europarl()\n",
    "download_and_extract_resources(fnames_and_urls=europarl.external_resources, dest_path=europarl.path)\n",
    "europarl.load_and_preprocess(max_input_length=MAX_INPUT_LENGTH, max_target_length=MAX_TARGET_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:34:44.089677Z",
     "start_time": "2018-06-28T17:34:44.063491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>agenda</td>\n",
       "      <td>arbeitsplan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>why?</td>\n",
       "      <td>wieso?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>no.</td>\n",
       "      <td>nein.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>why not?</td>\n",
       "      <td>warum?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6645</th>\n",
       "      <td>altener</td>\n",
       "      <td>altener</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     input_texts target_texts\n",
       "67        agenda  arbeitsplan\n",
       "1401        why?       wieso?\n",
       "1403         no.        nein.\n",
       "2003    why not?       warum?\n",
       "6645     altener      altener"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl.df[['input_texts', 'target_texts']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:34:44.096811Z",
     "start_time": "2018-06-28T17:34:44.092281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English subwords ['▁this', '▁is', '▁a', '▁test', '▁for', '▁pre', 'tr', 'ained', '▁by', 'te', 'pa', 'ire', 'm', 'bed', 'd', 'ings']\n",
      "German subwords ['▁das', '▁ist', '▁ein', '▁test', '▁für', '▁v', 'ort', 'rain', 'ierte', '▁zeich', 'eng', 'ruppen']\n"
     ]
    }
   ],
   "source": [
    "print(\"English subwords\", europarl.bpe_input.sentencepiece.EncodeAsPieces(\"this is a test for pretrained bytepairembeddings\"))\n",
    "print(\"German subwords\", europarl.bpe_target.sentencepiece.EncodeAsPieces(\"das ist ein test für vortrainierte zeichengruppen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:34:44.110763Z",
     "start_time": "2018-06-28T17:34:44.099830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Those will be the inputs for the seq2seq model (that needs to know how long the sequences can get)\n",
    "max_len_input = europarl.df.input_sequences.apply(len).max()\n",
    "max_len_target = europarl.df.target_sequences.apply(len).max()\n",
    "(max_len_input, max_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:34:44.119843Z",
     "start_time": "2018-06-28T17:34:44.113263Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(europarl.df.shape[0]), test_size=0.1, random_state=RANDOM_STATE)  # fixed random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:34:47.494600Z",
     "start_time": "2018-06-28T17:34:44.122158Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:52:51.547708Z",
     "start_time": "2018-06-28T17:52:47.569132Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    encoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_input\n",
    "        dtype=tf.int32,\n",
    "        name='encoder_inputs' \n",
    "    )\n",
    "    batch_size = tf.shape(encoder_inputs)[0]\n",
    "    beam_width = tf.placeholder_with_default(1, shape=[])\n",
    "    dropout = tf.placeholder_with_default(tf.cast(0.0, tf.float32), shape=[])\n",
    "    keep_prob = tf.cast(1.0, tf.float32) - dropout\n",
    "\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", \n",
    "        initializer=tf.constant(europarl.bpe_input.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder,\n",
    "        encoder_inputs,\n",
    "        name=\"encoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    input_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='input_sequence_length'\n",
    "    )\n",
    "    \n",
    "    rnn_cell_type = tf.nn.rnn_cell.GRUCell\n",
    "    encoder_forward_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name=f'encoder_forward_cell{layer}'),\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,  # state_keep_prob not set as it was not helpful here\n",
    "        dtype=tf.float32,\n",
    "    ) for layer in range(LAYERS)]\n",
    "    encoder_backward_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name=f'encoder_backward_cell{layer}'),\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    ) for layer in range(LAYERS)]\n",
    "    #encoder_bi_outputs, encoder_bi_state = \n",
    "    encoder_outputs, encoder_output_state_fw, encoder_output_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "        encoder_forward_cells, encoder_backward_cells,\n",
    "        inputs=encoder_emb_inp,\n",
    "        sequence_length=input_sequence_length,\n",
    "        time_major=False,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    # encoder_outputs = tf.concat(encoder_bi_outputs, -1)\n",
    "    # encoder_state = tf.concat(encoder_bi_state, -1)\n",
    "    encoder_state = tf.concat([encoder_output_state_fw[-1], encoder_output_state_bw[-1]], -1)\n",
    "    \n",
    "    # Regarding time_major:\n",
    "    # If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "    # If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "    # Using `time_major = True` is a bit more efficient because it avoids\n",
    "    # transposes at the beginning and end of the RNN calculation.  However,\n",
    "    # most TensorFlow data is batch-major, so by default this function\n",
    "    # accepts input and emits output in batch-major form.\n",
    "    #\n",
    "    # for simplicity I work with batch major here instead of time_major\n",
    "    # so I don't need to transpose inputs and transpose back for attention mechanism\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_inputs' \n",
    "    )\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", \n",
    "        initializer=tf.constant(europarl.bpe_target.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder,\n",
    "        decoder_inputs,\n",
    "        name=\"decoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    target_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='target_sequence_length'\n",
    "    )\n",
    "    \n",
    "    # tiling is necessary to work with BeamSearchDecoder\n",
    "    # read carefully the NOTE on constructor in\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper \n",
    "    tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(encoder_outputs, multiplier=beam_width)\n",
    "    tiled_encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width)\n",
    "    tiled_sequence_length = tf.contrib.seq2seq.tile_batch(input_sequence_length, multiplier=beam_width)\n",
    "    \n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        LATENT_DIM,\n",
    "        memory=tiled_encoder_outputs,\n",
    "        memory_sequence_length=tiled_sequence_length,\n",
    "        dtype=tf.float32,\n",
    "        name='attention_mechanism',\n",
    "    )\n",
    "    decoder_rnn_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM, name=f'decoder_cell{layer}'),\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    ) for layer in range(LAYERS-1)]\n",
    "    attention_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        #tf.contrib.rnn.MultiRNNCell(decoder_rnn_cells),\n",
    "        decoder_rnn_cells[0],\n",
    "        attention_mechanism,\n",
    "        attention_layer_size=LATENT_DIM,\n",
    "        name='multi_rnn_attention_wrapper',\n",
    "    )\n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell([attention_cell])\n",
    "\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        inputs=decoder_emb_inp, \n",
    "        sequence_length=target_sequence_length,\n",
    "        time_major=False,\n",
    "        name=\"decoder_training_helper\",\n",
    "    )\n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        units=len(europarl.bpe_target.tokens),\n",
    "        use_bias=False,\n",
    "        name='projection_layer',\n",
    "    )\n",
    "    \n",
    "    initial_state = (attention_cell.zero_state(dtype=tf.float32, batch_size=batch_size).clone(\n",
    "        cell_state=encoder_state\n",
    "    ), )\n",
    "    # initial_state = tuple(\n",
    "    #     zs.clone(cell_state=es)\n",
    "    #     if isinstance(zs, tf.contrib.seq2seq.AttentionWrapperState) else es\n",
    "    #     for zs, es in zip(decoder_cell.zero_state(batch_size, dtype=tf.float32), encoder_state)\n",
    "    # )\n",
    "\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=decoder_cell,\n",
    "        helper=training_helper,\n",
    "        initial_state=initial_state,\n",
    "        output_layer=projection_layer,\n",
    "    )\n",
    "    outputs, _final_state, _final_sequence_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder,\n",
    "        output_time_major=False,\n",
    "        impute_finished=False,\n",
    "    )\n",
    "    logits = outputs.rnn_output\n",
    "    \n",
    "    decoder_outputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_outputs',\n",
    "    )\n",
    "    target_weights = tf.cast(tf.sequence_mask(target_sequence_length), dtype=tf.float32)\n",
    "    l2_lambda = tf.placeholder_with_default(tf.cast(1e-5, tf.float32), shape=[])\n",
    "    loss_l2 = tf.add_n([ tf.nn.l2_loss(v) for v in tf.trainable_variables() ])\n",
    "    train_loss = tf.contrib.seq2seq.sequence_loss(logits, decoder_outputs, target_weights) + l2_lambda * loss_l2\n",
    "\n",
    "    params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, params)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "        t_list=gradients,\n",
    "        clip_norm=1.,\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    update_step = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "    \n",
    "    inference_decoder_initial_state = (attention_cell.zero_state(\n",
    "        dtype=tf.float32,\n",
    "        batch_size=batch_size * beam_width  # tricky and somehow unintuitive, but necessary\n",
    "    ).clone(\n",
    "        cell_state=tiled_encoder_state\n",
    "    ), )\n",
    "    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "        cell=decoder_cell,\n",
    "        embedding=embedding_decoder,\n",
    "        start_tokens=tf.fill([batch_size], europarl.bpe_target.start_token_idx),\n",
    "        end_token=europarl.bpe_target.stop_token_idx,\n",
    "        initial_state=inference_decoder_initial_state,\n",
    "        beam_width=BEAM_WIDTH,\n",
    "        output_layer=projection_layer,\n",
    "        length_penalty_weight=1.0,  # https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/\n",
    "    )\n",
    "\n",
    "    \n",
    "    inference_outputs, _inference_final_state, _inference_final_sequence_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inference_decoder,\n",
    "        maximum_iterations=tf.round(tf.reduce_max(input_sequence_length) * 2),  # a bit more flexible than max_len_target\n",
    "        impute_finished=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:52:51.559744Z",
     "start_time": "2018-06-28T17:52:51.548960Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_train_batch(batch_ids):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    pred, loss, _ = sess.run(\n",
    "        fetches=[\n",
    "            outputs, train_loss, update_step\n",
    "        ],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "            dropout: DROPOUT,\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def run_val_batch(batch_ids):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    loss = sess.run(\n",
    "        fetches=[train_loss],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def run_validation_loss():\n",
    "    return np.mean([\n",
    "        run_val_batch(ids)\n",
    "        for ids \n",
    "        in np.array_split(val_ids, np.ceil(len(val_ids) / BATCH_SIZE))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:53:10.995286Z",
     "start_time": "2018-06-28T17:52:51.560916Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dfa80d871840fa952dc9726ad7f514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 7.7827353 val_loss 5.515521\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1881608ec5f94b11a88f5c7936317d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 4.836562 val_loss 4.0198474\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eedd0bf24e254af9ad6231d799aba677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 3.5782146 val_loss 3.135301\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44e0915220f496b81b928cb92d7e265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.8725224 val_loss 2.6690092\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863899945e5d49049f39ee7da99b4d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.4309294 val_loss 2.330859\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384212663e8c4585a6184bc1ec86b866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 2.1134582 val_loss 2.130053\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7636595645fd40e7bf4b9fbeba977716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.8899457 val_loss 1.9817598\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad73ccc998ae45729db8bae2d0b596ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 8', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.7151786 val_loss 1.852367\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68ac059894f47ae99b2f49c9558253b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 9', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.5700188 val_loss 1.7530136\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7d411a1a6d40709ae3b8171d332c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 10', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.4373542 val_loss 1.6664379\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14bd7fb7fc204a3dbc6f3bd8b7554e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 11', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.3351543 val_loss 1.5950413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e63633304ea4ed480ef5ac0bbf3d9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 12', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.2210109 val_loss 1.5219679\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9185a11c56054514b52c68ead47c5af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 13', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.1298394 val_loss 1.4852664\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd11aaa68b904b16800977c095452622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 14', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 1.0477867 val_loss 1.4333477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9608d995b3d426f9d6dcd7e213ee12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 15', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 0.96540844 val_loss 1.3681561\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9fd55bbffd43f1a468e2cb47915b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 16', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 0.8959124 val_loss 1.3791211\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fdb624db5347978c8c979e39473b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 17', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 0.8303546 val_loss 1.3487499\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2558f08179ad44688ead8c5b2454054d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 18', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 0.7758453 val_loss 1.3317738\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705b02e34acb468f87c7ab659be3e717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 19', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 0.725662 val_loss 1.3178384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61d5f5ae1e54a0996b51e14a9c10e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 20', max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss 0.6844048 val_loss 1.2781029\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(\n",
    "    allow_soft_placement=True,  # needed as recommendation from https://github.com/tensorflow/tensorflow/issues/2292\n",
    "    log_device_placement=True,\n",
    ")\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batches_per_epoch = np.ceil(len(train_ids) / BATCH_SIZE)\n",
    "for epoch in range(EPOCHS):\n",
    "    shuffled_ids = np.random.permutation(train_ids)\n",
    "    batch_splits = np.array_split(shuffled_ids, batches_per_epoch)\n",
    "    train_losses = []\n",
    "    N = len(batch_splits)\n",
    "    with tqdm(batch_splits, desc=f\"Epoch {epoch+1}\") as t:\n",
    "        for train_batch_ids in t:\n",
    "            batch_loss = run_train_batch(train_batch_ids)\n",
    "            train_losses.append(batch_loss)\n",
    "            t.set_postfix(train_loss=np.mean(train_losses))\n",
    "        print(\"train_loss\", np.mean(train_losses), \"val_loss\", run_validation_loss())\n",
    "        \n",
    "validation_input_sequences = europarl.df.input_sequences.iloc[val_ids[:BATCH_SIZE]]\n",
    "validation_input_lengths = validation_input_sequences.apply(len)\n",
    "\n",
    "validation_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    validation_input_sequences,\n",
    "    maxlen=max_len_input,\n",
    "    dtype=int,\n",
    "    padding='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:53:11.000377Z",
     "start_time": "2018-06-28T17:53:10.996473Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    sequenced = europarl.bpe_input.subword_indices(preprocess(sentence))\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [sequenced],\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    \n",
    "    beam_search_output = sess.run(\n",
    "        fetches=[inference_outputs],\n",
    "        feed_dict={\n",
    "            encoder_inputs: padded,\n",
    "            input_sequence_length: [len(sequenced)],\n",
    "            beam_width: BEAM_WIDTH,\n",
    "        }\n",
    "    )[0]\n",
    "    \n",
    "    return europarl.bpe_target.sentencepiece.DecodePieces([\n",
    "        europarl.bpe_target.tokens[idx] for idx in beam_search_output.predicted_ids[0, :, 0].tolist()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:53:12.039800Z",
     "start_time": "2018-06-28T17:53:11.001456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/tfattentionmodel_2layers.ckpt'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'tfattentionmodel_2layers'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, f\"data/{name}.ckpt\")\n",
    "# tfattentionmodel.ckpt.index https://drive.google.com/open?id=1Xv2Qc9Gnac_Of9bIpQef5LZJnX9ij933\n",
    "# tfattentionmodel.cpkt.meta https://drive.google.com/open?id=162WY8XEjyqfvitiIBfmq9oHLC1rQhyvr\n",
    "# tfattentionmodel.cpkt.data-00000-of-00001 https://drive.google.com/open?id=1racW0agvk5nJ_xBaxifAaEWM8T9ot7FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:53:12.672211Z",
     "start_time": "2018-06-28T17:53:12.041548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello.' --> 'hallgest!'\n",
      "'you are welcome.' --> 'zizisch.'\n",
      "'how do you do?' --> 'warum auch?'\n",
      "'i hate mondays.' --> 'miragte:'\n",
      "'i am a programmer.' --> 'mir auch!'\n",
      "'data is the new oil.' --> 'genau.'\n",
      "'it could be worse.' --> 'mag sein.'\n",
      "'i am on top of it.' --> 'mir auch.'\n",
      "'n° uno' --> 'zleichte'\n",
      "'awesome!' --> 'zantisch!'\n",
      "'put your feet up!' --> 'wacht auf!'\n",
      "'from the start till the end!' --> 'hörenig!'\n",
      "'from dusk till dawn.' --> 'herr enci.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:53:13.003359Z",
     "start_time": "2018-06-28T17:53:12.673491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'why?', got 'warum?', exp: 'wieso?'\n",
      "Original 'no.', got 'nein.', exp: 'nein.'\n",
      "Original 'why not?', got 'warum?', exp: 'warum?'\n",
      "Original 'altener', got 'kch.', exp: 'altener'\n",
      "Original 'save', got 'save', exp: 'save'\n",
      "Original 'culture 0', got 'kultur 0', exp: 'kultur 0'\n",
      "Original 'agenda', got 'arbeitsplan', exp: 'arbeitsplan'\n",
      "Original 'welcome', got 'begrüßung', exp: 'begrüßung'\n",
      "Original 'scrapie', got 'scrapie', exp: 'scrapie'\n",
      "Original 'welcome', got 'begrüßung', exp: 'begrüßung'\n",
      "Original 'welcome', got 'begrüßung', exp: 'begrüßung'\n",
      "Original 'ucits', got 'weißrußland', exp: 'ogaw'\n",
      "Original 'why not?', got 'warum?', exp: 'weshalb?'\n",
      "Original 'cambodia', got 'kambodscha', exp: 'kambodscha'\n",
      "Original 'why?', got 'warum?', exp: 'weshalb?'\n",
      "Original 'why?', got 'warum?', exp: 'warum?'\n",
      "Original 'bouwman.', got 'bouu.', exp: 'bouwman.'\n",
      "Original 'no.', got 'nein.', exp: 'nein.'\n",
      "Original 'nobody.', got 'niemand.', exp: 'niemand.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in europarl.df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:53:13.306552Z",
     "start_time": "2018-06-28T17:53:13.004608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'welcome', got 'begrüßung', exp: 'begrüßung'\n",
      "Original 'why?', got 'warum?', exp: 'weshalb?'\n",
      "Original 'applause', got 'beifall', exp: 'beifall'\n",
      "Original '(0)', got '(0)', exp: '(0)'\n",
      "Original 'ok.', got 'in ordnung.', exp: 'o.k.'\n",
      "Original 'no!', got 'nein!', exp: 'nein!'\n",
      "Original 'applause', got 'beifall', exp: 'beifall'\n",
      "Original '0. (', got '0. (', exp: '0. ('\n",
      "Original 'applause', got 'beifall', exp: 'beifall'\n",
      "Original 'why?', got 'warum?', exp: 'warum?'\n",
      "Original 'applause', got 'beifall', exp: 'beifall'\n",
      "Original 'yes.', got 'ja.', exp: 'ja.'\n",
      "Original 'spain', got 'georgien', exp: 'spanien'\n",
      "Original 'turkey', got 'türkei', exp: 'türkei:'\n",
      "Original 'nothing.', got 'nichts.', exp: 'zu nichts.'\n",
      "Original 'applause', got 'beifall', exp: 'beifall'\n",
      "Original 'thursday:', got 'donnerstag:', exp: 'donnerstag:'\n",
      "Original 'applause', got 'beifall', exp: 'beifall'\n",
      "Original 'applause', got 'beifall', exp: 'beifall'\n"
     ]
    }
   ],
   "source": [
    "# Performance on validation set\n",
    "val_df = europarl.df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-28T17:53:16.801795Z",
     "start_time": "2018-06-28T17:53:13.307914Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd07fdf5f994c73bc435e03711bd363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=229), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.2936452724279626\n"
     ]
    }
   ],
   "source": [
    "bleu = bleu_scores_europarl(\n",
    "    input_texts=europarl.df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    target_texts=europarl.df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    predict=lambda text: predict(text)\n",
    ")\n",
    "print(f'average BLEU on test set = {bleu.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.995932Z",
     "start_time": "2018-05-13T18:17:20.994111Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The translations for the train and validations are decent. The main problem without attention was that the rnn decoder repeated itself and got confused what it intended to say (like \"leider geht es nur um die realität und die realität.\") The bleu score also improved from 0.183 to 0.237.\n",
    "\n",
    "Now, the main problems are any words out of the vocabulary (meaning not seen often enough in the training set). Some of the problems will go away if we use the whole training data set instead of a subset. But in the long run, we might need a copy mechanism, a look up method and/or a dynamic neural network memory. Also, some translations make grammatical mistakes. I can imagine that multi learning (like also learn grammatical features or entity recognition maybe in combination with some data augmentation) would help. It's also somehow funny to see how accurately the translations are for complicated sentences from the validation set, but for my very simple own examples that are different to europarlament discussion, the translations are terrible.\n",
    "\n",
    "Regarding tensorflow, allthough the code is a bit longer, it doesn't need much energy to figure what's going on. Every line manipulates the computational graph without much surprises. Allthough, debugging is still a hassle (cryptic error messages and no direct evaluation), it's not so mind blogging like working on different abstraction levels with Keras IMHO. It's also nice to see that a lot state-of-the art technology from research is already implemented in tensorflow. The computational speed went up here a lot, too. I think the main reason is that the tensorflow implementation only trains till end-of-sentence and not beyond (where Keras implementation masked them only to ignore in loss function). \n",
    "\n",
    "My next step will be to train the neural machine translation on the whole europarliament dataset (2.3 Mio > 600k) and also train for a longer time (even here the training did not converge after 20 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 294,
   "position": {
    "height": "40px",
    "left": "553px",
    "right": "192px",
    "top": "132px",
    "width": "615px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
