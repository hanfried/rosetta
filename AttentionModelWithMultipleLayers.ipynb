{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Multiple Layers for Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing [Attention](AttentionModelForMachineTranslationWithTensorflow.ipynb), I'll come back first to examine whether it's worth to implement mulitple layers (of encoders and decoders) instead of only one. Of course, Deep Learning is all about deep nets, at some point I'll have to check it.\n",
    "\n",
    "Again, I'll try to refactor the code a bit, so we can concentrate on the stacking issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T11:28:51.819912Z",
     "start_time": "2018-06-29T11:28:49.863754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed random seed to 42\n",
      "Availabe devices: [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4412624522425098867\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7689882829\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14476577674040062727\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Cuda/Cudnn/GPU works as intended\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils.download import download_and_extract_resources\n",
    "from utils.linguistic import bleu_scores_europarl, preprocess_input_europarl as preprocess\n",
    "from utils.preparation import check_gpu_working, Europarl, RANDOM_STATE\n",
    "\n",
    "check_gpu_working()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T11:28:51.823551Z",
     "start_time": "2018-06-29T11:28:51.821093Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 100\n",
    "MAX_TARGET_LENGTH = 125\n",
    "LATENT_DIM = 256  # was 512, but we should be able to use a smaller hidden representation as we are looking back anyway as needed\n",
    "LAYERS = 2\n",
    "EPOCHS = 25 #20\n",
    "BATCH_SIZE = 128  # was 64, but tensorflow implementation doesn't need so much GPU memory so can increase batch size\n",
    "DROPOUT = 0.25  # Dropout on input and output for the RNN cells, so effective dropout is 0.5, but works slightly better so\n",
    "TEST_SIZE = 2500\n",
    "BEAM_WIDTH = 5\n",
    "EMBEDDING_TRAINABLE = True  # Improves results significant and for at least it's not the most dominant training time factor (that's the output softmax layer)\n",
    "WARMUP_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T11:29:39.053817Z",
     "start_time": "2018-06-29T11:28:51.824953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n",
      "en.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "en.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (6.2 MB)\n",
      "de.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "de.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (5.7 MB)\n",
      "Total number of unfiltered translations 1920209\n",
      "Filtered translations with length between (1, input=100/target=125) characters: 597331\n"
     ]
    }
   ],
   "source": [
    "europarl = Europarl()\n",
    "download_and_extract_resources(fnames_and_urls=europarl.external_resources, dest_path=europarl.path)\n",
    "europarl.load_and_preprocess(max_input_length=MAX_INPUT_LENGTH, max_target_length=MAX_TARGET_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T11:29:39.228451Z",
     "start_time": "2018-06-29T11:29:39.055609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resumption of the session</td>\n",
       "      <td>wiederaufnahme der sitzungsperiode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>please rise, then, for this minute' s silence.</td>\n",
       "      <td>ich bitte sie, sich zu einer schweigeminute zu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(the house rose and observed a minute' s silence)</td>\n",
       "      <td>(das parlament erhebt sich zu einer schweigemi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>madam president, on a point of order.</td>\n",
       "      <td>frau präsidentin, zur geschäftsordnung.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>if the house agrees, i shall do as mr evans ha...</td>\n",
       "      <td>wenn das haus damit einverstanden ist, werde i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_texts  \\\n",
       "0                           resumption of the session   \n",
       "5      please rise, then, for this minute' s silence.   \n",
       "6   (the house rose and observed a minute' s silence)   \n",
       "7               madam president, on a point of order.   \n",
       "12  if the house agrees, i shall do as mr evans ha...   \n",
       "\n",
       "                                         target_texts  \n",
       "0                  wiederaufnahme der sitzungsperiode  \n",
       "5   ich bitte sie, sich zu einer schweigeminute zu...  \n",
       "6   (das parlament erhebt sich zu einer schweigemi...  \n",
       "7             frau präsidentin, zur geschäftsordnung.  \n",
       "12  wenn das haus damit einverstanden ist, werde i...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl.df[['input_texts', 'target_texts']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T11:29:39.233271Z",
     "start_time": "2018-06-29T11:29:39.230008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English subwords ['▁this', '▁is', '▁a', '▁test', '▁for', '▁pre', 'tr', 'ained', '▁by', 'te', 'pa', 'ire', 'm', 'bed', 'd', 'ings']\n",
      "German subwords ['▁das', '▁ist', '▁ein', '▁test', '▁für', '▁v', 'ort', 'rain', 'ierte', '▁zeich', 'eng', 'ruppen']\n"
     ]
    }
   ],
   "source": [
    "print(\"English subwords\", europarl.bpe_input.sentencepiece.EncodeAsPieces(\"this is a test for pretrained bytepairembeddings\"))\n",
    "print(\"German subwords\", europarl.bpe_target.sentencepiece.EncodeAsPieces(\"das ist ein test für vortrainierte zeichengruppen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T11:29:39.500212Z",
     "start_time": "2018-06-29T11:29:39.236450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 71)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Those will be the inputs for the seq2seq model (that needs to know how long the sequences can get)\n",
    "max_len_input = europarl.df.input_sequences.apply(len).max()\n",
    "max_len_target = europarl.df.target_sequences.apply(len).max()\n",
    "(max_len_input, max_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T11:29:39.535480Z",
     "start_time": "2018-06-29T11:29:39.502459Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(europarl.df.shape[0]), test_size=0.1, random_state=RANDOM_STATE)  # fixed random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T11:29:48.264449Z",
     "start_time": "2018-06-29T11:29:39.537799Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    encoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_input\n",
    "        dtype=tf.int32,\n",
    "        name='encoder_inputs' \n",
    "    )\n",
    "    batch_size = tf.shape(encoder_inputs)[0]\n",
    "    beam_width = tf.placeholder_with_default(1, shape=[])\n",
    "    dropout = tf.placeholder_with_default(tf.cast(0.0, tf.float32), shape=[])\n",
    "    keep_prob = tf.cast(1.0, tf.float32) - dropout\n",
    "    # embedding_trainable = tf.placeholder_with_default(EMBEDDING_TRAINABLE, shape=[])\n",
    "    learning_rate = tf.placeholder_with_default(tf.cast(1e-3, tf.float32), shape=[])\n",
    "\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", \n",
    "        initializer=tf.constant(europarl.bpe_input.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder,\n",
    "        encoder_inputs,\n",
    "        name=\"encoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    input_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='input_sequence_length'\n",
    "    )\n",
    "    \n",
    "    rnn_cell_type = tf.nn.rnn_cell.GRUCell\n",
    "    encoder_forward_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name=f'encoder_forward_cell{layer}'),\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,  # state_keep_prob not set as it was not helpful here\n",
    "        dtype=tf.float32,\n",
    "    ) for layer in range(LAYERS)]\n",
    "    encoder_backward_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name=f'encoder_backward_cell{layer}'),\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    ) for layer in range(LAYERS)]\n",
    "    encoder_outputs, encoder_output_state_fw, encoder_output_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "        encoder_forward_cells, encoder_backward_cells,\n",
    "        inputs=encoder_emb_inp,\n",
    "        sequence_length=input_sequence_length,\n",
    "        time_major=False,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    encoder_state = tf.concat([encoder_output_state_fw[-1], encoder_output_state_bw[-1]], -1)\n",
    "    \n",
    "    # Regarding time_major:\n",
    "    # If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "    # If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "    # Using `time_major = True` is a bit more efficient because it avoids\n",
    "    # transposes at the beginning and end of the RNN calculation.  However,\n",
    "    # most TensorFlow data is batch-major, so by default this function\n",
    "    # accepts input and emits output in batch-major form.\n",
    "    #\n",
    "    # for simplicity I work with batch major here instead of time_major\n",
    "    # so I don't need to transpose inputs and transpose back for attention mechanism\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_inputs' \n",
    "    )\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", \n",
    "        initializer=tf.constant(europarl.bpe_target.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder,\n",
    "        decoder_inputs,\n",
    "        name=\"decoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    target_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='target_sequence_length'\n",
    "    )\n",
    "    \n",
    "    # tiling is necessary to work with BeamSearchDecoder\n",
    "    # read carefully the NOTE on constructor in\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper \n",
    "    tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(encoder_outputs, multiplier=beam_width)\n",
    "    tiled_encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width)\n",
    "    tiled_sequence_length = tf.contrib.seq2seq.tile_batch(input_sequence_length, multiplier=beam_width)\n",
    "    \n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        LATENT_DIM,\n",
    "        memory=tiled_encoder_outputs,\n",
    "        memory_sequence_length=tiled_sequence_length,\n",
    "        dtype=tf.float32,\n",
    "        name='attention_mechanism',\n",
    "    )\n",
    "    decoder_rnn_cells = [rnn_cell_type(num_units=LATENT_DIM, name=f'decoder_cell{layer}') for layer in range(LAYERS)]\n",
    "    def residual_fn(inputs, outputs):\n",
    "        tf.contrib.framework.nest.assert_same_structure(inputs, outputs)\n",
    "        inputs_without_attention = tf.slice(inputs, [0, 0], [batch_size, LATENT_DIM])\n",
    "        return tf.contrib.framework.nest.map_structure(lambda inp, out: inp + out, inputs_without_attention, outputs) \n",
    "    for layer in range(1, LAYERS):\n",
    "        decoder_rnn_cells[layer] = tf.contrib.rnn.ResidualWrapper(\n",
    "            decoder_rnn_cells[layer],\n",
    "            residual_fn=residual_fn\n",
    "        )\n",
    "    decoder_rnn_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        cell,\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    ) for cell in decoder_rnn_cells]\n",
    "    attention_cells = [tf.contrib.seq2seq.AttentionWrapper(\n",
    "        cell,\n",
    "        attention_mechanism,\n",
    "        attention_layer_size=LATENT_DIM,\n",
    "        name=f'attention_wrapper{layer}',\n",
    "    ) for layer, cell in enumerate(decoder_rnn_cells)] \n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell(attention_cells)\n",
    "\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        inputs=decoder_emb_inp, \n",
    "        sequence_length=target_sequence_length,\n",
    "        time_major=False,\n",
    "        name=\"decoder_training_helper\",\n",
    "    )\n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        units=len(europarl.bpe_target.tokens),\n",
    "        use_bias=False,\n",
    "        name='projection_layer',\n",
    "    )\n",
    "    \n",
    "    initial_state = tuple(\n",
    "        attention_cells[0].zero_state(dtype=tf.float32, batch_size=batch_size).clone(\n",
    "            cell_state=encoder_state\n",
    "        )\n",
    "        for _ in range(LAYERS)\n",
    "    )\n",
    "\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=decoder_cell,\n",
    "        helper=training_helper,\n",
    "        initial_state=initial_state,\n",
    "        output_layer=projection_layer,\n",
    "    )\n",
    "    outputs, _final_state, _final_sequence_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder,\n",
    "        output_time_major=False,\n",
    "        impute_finished=False,\n",
    "    )\n",
    "    logits = outputs.rnn_output\n",
    "    \n",
    "    decoder_outputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_outputs',\n",
    "    )\n",
    "    target_weights = tf.cast(tf.sequence_mask(target_sequence_length), dtype=tf.float32)\n",
    "    l2_lambda = tf.placeholder_with_default(tf.cast(1e-5, tf.float32), shape=[])\n",
    "    loss_l2 = tf.add_n([\n",
    "        tf.nn.l2_loss(v)\n",
    "        for v in tf.trainable_variables()\n",
    "        if not re.match(r'embedding_(de|en)coder', v.name)  # don't regularize embeddings\n",
    "    ])\n",
    "    train_loss = tf.contrib.seq2seq.sequence_loss(logits, decoder_outputs, target_weights) + l2_lambda * loss_l2\n",
    "\n",
    "    params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, params)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "        t_list=gradients,\n",
    "        clip_norm=1.,\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    update_step = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "    \n",
    "    params_without_embeddings = [v for v in tf.trainable_variables() if not re.match(r'embedding_(de|en)coder', v.name)]\n",
    "    gradients_without_embeddings = tf.gradients(train_loss, params_without_embeddings)\n",
    "    clipped_gradients_without_embeddings, _ = tf.clip_by_global_norm(\n",
    "        t_list=gradients_without_embeddings,\n",
    "        clip_norm=1.,\n",
    "    )\n",
    "    update_step_without_embeddings = optimizer.apply_gradients(zip(clipped_gradients_without_embeddings, params_without_embeddings))\n",
    "    \n",
    "    inference_decoder_initial_state = tuple(\n",
    "        attention_cells[0].zero_state(\n",
    "            dtype=tf.float32,\n",
    "            batch_size=batch_size * beam_width  # tricky and somehow unintuitive, but necessary\n",
    "        ).clone(\n",
    "            cell_state=tiled_encoder_state\n",
    "        ) for _ in range(LAYERS)\n",
    "    )\n",
    "    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "        cell=decoder_cell,\n",
    "        embedding=embedding_decoder,\n",
    "        start_tokens=tf.fill([batch_size], europarl.bpe_target.start_token_idx),\n",
    "        end_token=europarl.bpe_target.stop_token_idx,\n",
    "        initial_state=inference_decoder_initial_state,\n",
    "        beam_width=BEAM_WIDTH,\n",
    "        output_layer=projection_layer,\n",
    "        length_penalty_weight=1.0,  # https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/\n",
    "    )\n",
    "\n",
    "    \n",
    "    inference_outputs, _inference_final_state, _inference_final_sequence_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inference_decoder,\n",
    "        maximum_iterations=tf.round(tf.reduce_max(input_sequence_length) * 2),  # a bit more flexible than max_len_target\n",
    "        impute_finished=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T11:29:48.276646Z",
     "start_time": "2018-06-29T11:29:48.265788Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_train_batch(batch_ids, epoch):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    lr = 1e-3 / 2 + (epoch / WARMUP_EPOCHS) * (1e-3 / 2) if epoch <= WARMUP_EPOCHS else 1e-3 * (0.96 ** (epoch - WARMUP_EPOCHS))\n",
    "    pred, loss, _ = sess.run(\n",
    "        fetches=[\n",
    "            outputs, train_loss, update_step if epoch >= WARMUP_EPOCHS else update_step_without_embeddings\n",
    "        ],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "            dropout: DROPOUT,\n",
    "            # embedding_trainable: epoch >= WARMUP_EPOCHS,\n",
    "            learning_rate: lr\n",
    "        }\n",
    "    )\n",
    "    return loss, lr\n",
    "\n",
    "def run_val_batch(batch_ids):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    loss = sess.run(\n",
    "        fetches=[train_loss],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def run_validation_loss():\n",
    "    return np.mean([\n",
    "        run_val_batch(ids)\n",
    "        for ids \n",
    "        in np.array_split(val_ids, np.ceil(len(val_ids) / BATCH_SIZE))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T11:29:48.284915Z",
     "start_time": "2018-06-29T11:29:48.278468Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    sequenced = europarl.bpe_input.subword_indices(preprocess(sentence))\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [sequenced],\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    \n",
    "    beam_search_output = sess.run(\n",
    "        fetches=[inference_outputs],\n",
    "        feed_dict={\n",
    "            encoder_inputs: padded,\n",
    "            input_sequence_length: [len(sequenced)],\n",
    "            beam_width: BEAM_WIDTH,\n",
    "        }\n",
    "    )[0]\n",
    "    \n",
    "    return europarl.bpe_target.sentencepiece.DecodePieces([\n",
    "        europarl.bpe_target.tokens[idx] for idx in beam_search_output.predicted_ids[0, :, 0].tolist()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T18:22:38.169833Z",
     "start_time": "2018-06-29T11:29:48.286374Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e85ec350e64a58b368f5342bf79362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0005, train_loss=4.01746, val_loss=3.01409\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea11c6650a2c402e9eefaafbc051b61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.00055, train_loss=3.2367, val_loss=2.69308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a698b3653cc34de69094863c22eff5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0006, train_loss=3.02766, val_loss=2.53387\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6245b670b98b4287840debf5eab261b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.00065, train_loss=2.90406, val_loss=2.43222\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be300a9723649d29a2ce605df736f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0007, train_loss=2.80922, val_loss=2.3543\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8911819c02441681eaad7124acabff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.1688193598369874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a4875147634f3784ef02357a131c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.00075, train_loss=2.74052, val_loss=2.30671\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ade646f50142eca6fa597b117ba58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0008, train_loss=2.70086, val_loss=2.27955\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf6ddd2724048acb57ae2c497a8ae97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 8', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.00085, train_loss=2.67775, val_loss=2.25711\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfe189bd35f41c7b02399009910735c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 9', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0009, train_loss=2.66522, val_loss=2.24242\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa89609d76cc408e821d70181000c583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 10', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.00095, train_loss=2.65815, val_loss=2.23937\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39843d8eaceb4878b3f2d00e269dcb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.1858996685256661\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd50fd72a4694d698f9e20cb0ca39ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 11', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.43898, val_loss=2.02336\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a877b205a84c929f1ee43726658d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 12', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.00096, train_loss=2.29811, val_loss=1.95121\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79010d422e34cb8b322a4d5424ade2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 13', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0009216, train_loss=2.2262, val_loss=1.89936\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d21138e0d6541fba6c8512f09500d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 14', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000884736, train_loss=2.17744, val_loss=1.87125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f8ae809975406499e6ec66dcb580a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 15', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000849347, train_loss=2.13882, val_loss=1.84166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d745dd52091f454b9bc10b2bf827b006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.22332697100624174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72fd259a76741cdb643141f96fc760a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 16', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000815373, train_loss=2.1078, val_loss=1.82361\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e803d8e89e47209ee8daa355395d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 17', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000782758, train_loss=2.08192, val_loss=1.79983\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa0e6a480ff4846b8e3ebce8fbb1f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 18', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000751447, train_loss=2.05919, val_loss=1.78248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b02017c2494021aa6cde1606b4149f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 19', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.00072139, train_loss=2.03902, val_loss=1.77332\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cee32f383e49e483e80432163560f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 20', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000692534, train_loss=2.02011, val_loss=1.75984\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3707ee8f1d784b189fee67537b9e50b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.23244944010035007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7c0d4c61d04a478320aaa0d469e2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 21', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000664833, train_loss=2.00344, val_loss=1.74195\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21026aec22df4205b11ce9f77b498aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 22', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000638239, train_loss=1.98763, val_loss=1.73377\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d9acaa0ffc4abd95016959b433645d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 23', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.00061271, train_loss=1.97277, val_loss=1.72126\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486df6c4d4e141d48af3ec093be83450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 24', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000588201, train_loss=1.95886, val_loss=1.71279\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a61441cc57442c08b977259c32d77f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 25', max=4200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000564673, train_loss=1.94618, val_loss=1.70131\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(\n",
    "    allow_soft_placement=True,  # needed as recommendation from https://github.com/tensorflow/tensorflow/issues/2292\n",
    "    log_device_placement=True,\n",
    ")\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batches_per_epoch = np.ceil(len(train_ids) / BATCH_SIZE)\n",
    "for epoch in range(EPOCHS):\n",
    "    shuffled_ids = np.random.permutation(train_ids)\n",
    "    batch_splits = np.array_split(shuffled_ids, batches_per_epoch)\n",
    "    train_losses = []\n",
    "    N = len(batch_splits)\n",
    "    with tqdm(batch_splits, desc=f\"Epoch {epoch+1}\") as t:\n",
    "        for train_batch_ids in t:\n",
    "            batch_loss, lr = run_train_batch(train_batch_ids, epoch=epoch)\n",
    "            train_losses.append(batch_loss)\n",
    "            t.set_postfix(train_loss=np.mean(train_losses))\n",
    "        print(f\"learning rate={lr:.6}, train_loss={np.mean(train_losses):.6}, val_loss={run_validation_loss():.6}\")\n",
    "    \n",
    "    if epoch > 0 and ((epoch+1) % 5 == 0) and (epoch+1 < EPOCHS):\n",
    "        bleu = bleu_scores_europarl(\n",
    "            input_texts=europarl.df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "            target_texts=europarl.df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "            predict=lambda text: predict(text)\n",
    "        )\n",
    "        print(f'average BLEU on test set = {bleu.mean()}')\n",
    "        \n",
    "validation_input_sequences = europarl.df.input_sequences.iloc[val_ids[:BATCH_SIZE]]\n",
    "validation_input_lengths = validation_input_sequences.apply(len)\n",
    "\n",
    "validation_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    validation_input_sequences,\n",
    "    maxlen=max_len_input,\n",
    "    dtype=int,\n",
    "    padding='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T18:22:39.549353Z",
     "start_time": "2018-06-29T18:22:38.171239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/tfattentionmodel_2layers.ckpt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'tfattentionmodel_2layers'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, f\"data/{name}.ckpt\")\n",
    "# tfattentionmodel.ckpt.index https://drive.google.com/open?id=1Xv2Qc9Gnac_Of9bIpQef5LZJnX9ij933\n",
    "# tfattentionmodel.cpkt.meta https://drive.google.com/open?id=162WY8XEjyqfvitiIBfmq9oHLC1rQhyvr\n",
    "# tfattentionmodel.cpkt.data-00000-of-00001 https://drive.google.com/open?id=1racW0agvk5nJ_xBaxifAaEWM8T9ot7FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T18:22:39.967696Z",
     "start_time": "2018-06-29T18:22:39.551096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello.' --> 'helfen.'\n",
      "'you are welcome.' --> 'sie sind begrüßenswert.'\n",
      "'how do you do?' --> 'wie sieht sie?'\n",
      "'i hate mondays.' --> 'ich habe meine antwort.'\n",
      "'i am a programmer.' --> 'ich bin ein programm.'\n",
      "'data is the new oil.' --> 'daten ist das neue öl.'\n",
      "'it could be worse.' --> 'es könnte schlimmer sein.'\n",
      "'i am on top of it.' --> 'ich bin über das thema.'\n",
      "'n° uno' --> 'nitoo'\n",
      "'awesome!' --> 'ein ergebnis!'\n",
      "'put your feet up!' --> 'ihre füße!'\n",
      "'from the start till the end!' --> 'aus dem beginn der begeisterung!'\n",
      "'from dusk till dawn.' --> 'aus tuskanna.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T18:22:41.481557Z",
     "start_time": "2018-06-29T18:22:39.969043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original \"please rise, then, for this minute' s silence.\", got 'bitte bitte sie, die schweigeminute zu setzen.', exp: 'ich bitte sie, sich zu einer schweigeminute zu erheben.'\n",
      "Original \"(the house rose and observed a minute' s silence)\", got '(das parlament erhebt sich zu einer schweigeminute.)', exp: '(das parlament erhebt sich zu einer schweigeminute.)'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'if the house agrees, i shall do as mr evans has suggested.', got 'wenn das haus eineinht, werde ich tun, wie herr evans vorgeschlagen hat.', exp: 'wenn das haus damit einverstanden ist, werde ich dem vorschlag von herrn evans folgen.'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin, zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'i would like your advice about rule 0 concerning inadmissibility.', got 'ich möchte gerne den rat über artikel 0 betreffen, die in der vermittlung zu tun haben.', exp: 'könnten sie mir eine auskunft zu artikel 0 im zusammenhang mit der unzulässigkeit geben?'\n",
      "Original 'it says that this should be done despite the principle of relative stability.', got 'es sagt, dass dies trotz des grundsatzes der relativ stabilität geleistet wird.', exp: 'und zwar sollen derartige strafen trotz des grundsatzes der relativen stabilität verhängt werden.'\n",
      "Original 'this is all in accordance with the principles that we have always upheld.', got 'das ist alles, was wir immer wieder haben.', exp: 'all dies entspricht den grundsätzen, die wir stets verteidigt haben.'\n",
      "Original 'thank you, mr segni, i shall do so gladly.', got 'i danke, herr segni, ich werde das gerne tun.', exp: 'vielen dank, herr segni, das will ich gerne tun.'\n",
      "Original 'indeed, it is quite in keeping with the positions this house has always adopted.', got 'in der tat ist es in der tat, dass die position dieses hauses immer angenommen hat.', exp: 'das ist ganz im sinne der position, die wir als parlament immer vertreten haben.'\n",
      "Original 'it is the case of alexander nikitin.', got 'das ist der fall alexander nikitin.', exp: 'das ist der fall von alexander nikitin.'\n",
      "Original 'now, however, he is to go before the courts once more because the public prosecutor is appealing.', got 'jetzt ist er jetzt vor der gerichte, denn die staatsanwaltschaft ist berufung.', exp: 'nun ist es aber so, daß er wieder angeklagt werden soll, weil der staatsanwalt in berufung geht.'\n",
      "Original 'but, madam president, my personal request has not been met.', got 'aber frau präsidentin, meine persönlichen anfrage sind nicht erfüllt worden.', exp: 'dennoch, frau präsidentin, wurde meinem wunsch nicht entsprochen.'\n",
      "Original 'i would therefore once more ask you to ensure that we get a dutch channel as well.', got 'ich möchte daher bitten, sicherzustellen, dass wir einen niederländischen änderungsantrag haben.', exp: 'deshalb möchte ich sie nochmals ersuchen, dafür sorge zu tragen, daß auch ein niederländischer sender eingespeist wird.'\n",
      "Original 'it will, i hope, be examined in a positive light.', got 'ich hoffe, daß sie in einem positiven licht geprüft wird.', exp: 'ich hoffe, daß dort in ihrem sinne entschieden wird.'\n",
      "Original 'why has no air quality test been done on this particular building since we were elected?', got 'warum hat kein luftqualitätsbedare auf diesem speziellen gebäude in diesem speziellen gebäude getan?', exp: 'weshalb wurde die luftqualität in diesem gebäude seit unserer wahl nicht ein einziges mal überprüft?'\n",
      "Original 'why has there been no health and safety committee meeting since 0?', got 'warum hat es keinen gesundheits- und sicherheitsausschuss für 0?', exp: 'weshalb ist der arbeitsschutzausschuß seit 0 nicht ein einziges mal zusammengetreten?'\n",
      "Original 'why are there no fire instructions?', got 'warum gibt es keine feuerwehr?', exp: 'warum finden keine brandschutzbelehrungen statt?'\n",
      "Original 'why have the staircases not been improved since my accident?', got 'warum wurde die strengere verbesserung seit meinem unfall nicht verbessert?', exp: 'warum wurde nach meinem unfall nichts unternommen, um die treppen sicherer zu machen?'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in europarl.df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T18:22:43.103048Z",
     "start_time": "2018-06-29T18:22:41.483176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'we congratulate you on a job very well done.', got 'wir beglückwünschen ihnen zu einer guten arbeit.', exp: 'wir gratulieren dir zu deiner hervorragenden arbeit.'\n",
      "Original 'in this case, i strongly disagree with what was said by the previous speaker, carl schlyter.', got 'in diesem fall stimme ich dem vorredner, herr kommissar, stinren zu.', exp: 'in diesem fall widerspreche ich klar dem, was der vorredner, carl schlyter, gesagt hat.'\n",
      "Original 'it only makes sense to rebuild these if the refugees who fled are coming back.', got 'es ist nur sinnvoll, wenn die flüchtlinge zurückgekehrt werden.', exp: 'deren wiederaufbau ist nur dann von nutzen, wenn die flüchtlinge aus den betreffenden gebieten wieder zurückkehren.'\n",
      "Original 'eba: everything but arms.', got 'eba: alles, was aber waffen ist.', exp: 'eba: everything but arms.'\n",
      "Original 'in wider terms, this directive is not ambitious enough.', got 'die richtlinie ist in dieser hinsicht nicht ehrgeizig.', exp: 'generell gesehen fehlt es dieser richtlinie an ambitioniertheit.'\n",
      "Original 'it is an irresponsible mistake to spend public money to maintain this fleet.', got 'es ist ein unverantwortlicher fehler, das öffentliche geld zu erhalten, um diese flotte zu erhalten.', exp: 'öffentliche gelder zur erhaltung dieser flotte zu verwenden ist ein verantwortungsloser fehler.'\n",
      "Original 'where better to ensure access to culture than through our libraries?', got 'wo ist besser zu gewährleisten, dass zugang zu kultur als durch unsere bibliotheken?', exp: 'wo ließe sich der zugang zu kultur besser sichern als durch unsere bibliotheken?'\n",
      "Original 'i would like to advise you against this.', got 'ich möchte sie dazu gernen.', exp: 'hiervor möchte ich warnen.'\n",
      "Original 'in writing. - (pt) it is essential to harmonise national safety procedures in member states.', got 'schriftlich. - (pt) es ist wichtig, die nationalen sicherheitsverfahren in den mitgliedstaaten zu harmonisieren.', exp: 'schriftlich. - (pt) es kommt jetzt darauf an, die nationalen sicherheitsverfahren in den mitgliedstaaten zu harmonisieren.'\n",
      "Original 'we know that shipbuilding is the last industrial sector to be subsidised.', got 'wir wissen, dass der schiffbau der letzte industriewirtschaftielle industrie subventventioniert.', exp: 'wir wissen, dass der schiffbau der letzte subventionierte industriesektor ist.'\n",
      "Original 'we spent 0 million euros on this project in 0, and 0 million euros in 0.', got 'wir haben 0 millionen euro für dieses projekt im jahr 0 und 0 millionen euro im jahr 0 ausgegeben.', exp: 'im jahr 0 haben wir 0 millionen euro für dieses projekt ausgegeben, 0 dann 0 millionen euro.'\n",
      "Original 'the eventual solution to the chinese challenge is to be found not in china, but in africa itself.', got 'die voraussetzung für die chinesische herausforderung ist nicht in china, sondern in afrika selbst.', exp: 'letztendlich liegt die antwort auf die chinesische herausforderung nicht in china, sondern in afrika selbst.'\n",
      "Original \"we in parliament will say 'yes' to new taxes but 'no' to more taxes.\", got 'wir im parlament werden mit neuen steuern sagen: \"nein\" zu mehr steuern.', exp: 'wir im parlament werden neuen steuern zustimmen, zusätzlichen steuern jedoch eine klare absage erteilen.'\n",
      "Original 'however, in my view we must not rest on our laurels.', got 'meiner meinung nach dürfen wir uns jedoch nicht auf unsere lormen beschränken.', exp: 'meiner ansicht nach dürfen wir uns jedoch nicht auf unseren lorbeeren ausruhen.'\n",
      "Original 'i shall confine my comments to a few basic issues.', got 'ich werde meine bemerkungen zu einigen grundlegenden fragen beschränken.', exp: 'im folgenden werde ich mich auf einige grundlegende themen konzentrieren.'\n",
      "Original 'the greatest anxiety is being aroused by the damage to the fukushima nuclear power station.', got 'die größte ernährung wird durch den schdenden an den kern des fukusima kernkraftwerkes geändert.', exp: 'die größte sorge wird durch den schaden am kernkraftwerk fukushima erweckt.'\n",
      "Original 'much is only on paper and unfortunately the reality is different.', got 'es geht nur um das papier, und leider ist die realität anders.', exp: 'vieles steht nur auf dem papier, die realität sieht leider anders aus.'\n",
      "Original 'this would not help anyone, neither the assistants themselves nor the members of parliament.', got 'das würde niemanden helfen, weder die assistenten selbst und die mitglieder des parlaments.', exp: 'damit wäre niemandem geholfen, weder den assistenten noch den abgeordneten hier im hause.'\n",
      "Original 'can civil society in russia expect nothing more than this kind of provocation?', got 'kann die zivilgesellschaft in russland nicht mehr als eine solche provokation erwarten?', exp: 'hat die zivilgesellschaft in russland nur provokationen zu erwarten?'\n"
     ]
    }
   ],
   "source": [
    "# Performance on validation set\n",
    "val_df = europarl.df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T18:25:23.499162Z",
     "start_time": "2018-06-29T18:22:43.105032Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cafcb6a76d4dd28a7a96078c191c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.23929180571629935\n"
     ]
    }
   ],
   "source": [
    "bleu = bleu_scores_europarl(\n",
    "    input_texts=europarl.df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    target_texts=europarl.df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    predict=lambda text: predict(text)\n",
    ")\n",
    "print(f'average BLEU on test set = {bleu.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.995932Z",
     "start_time": "2018-05-13T18:17:20.994111Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The translations for the train and validations are decent. The main problem without attention was that the rnn decoder repeated itself and got confused what it intended to say (like \"leider geht es nur um die realität und die realität.\") The bleu score also improved from 0.183 to 0.237.\n",
    "\n",
    "Now, the main problems are any words out of the vocabulary (meaning not seen often enough in the training set). Some of the problems will go away if we use the whole training data set instead of a subset. But in the long run, we might need a copy mechanism, a look up method and/or a dynamic neural network memory. Also, some translations make grammatical mistakes. I can imagine that multi learning (like also learn grammatical features or entity recognition maybe in combination with some data augmentation) would help. It's also somehow funny to see how accurately the translations are for complicated sentences from the validation set, but for my very simple own examples that are different to europarlament discussion, the translations are terrible.\n",
    "\n",
    "Regarding tensorflow, allthough the code is a bit longer, it doesn't need much energy to figure what's going on. Every line manipulates the computational graph without much surprises. Allthough, debugging is still a hassle (cryptic error messages and no direct evaluation), it's not so mind blogging like working on different abstraction levels with Keras IMHO. It's also nice to see that a lot state-of-the art technology from research is already implemented in tensorflow. The computational speed went up here a lot, too. I think the main reason is that the tensorflow implementation only trains till end-of-sentence and not beyond (where Keras implementation masked them only to ignore in loss function). \n",
    "\n",
    "My next step will be to train the neural machine translation on the whole europarliament dataset (2.3 Mio > 600k) and also train for a longer time (even here the training did not converge after 20 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 294,
   "position": {
    "height": "40px",
    "left": "553px",
    "right": "192px",
    "top": "132px",
    "width": "615px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
