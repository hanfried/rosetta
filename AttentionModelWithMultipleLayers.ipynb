{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Multiple Layers for Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing [Attention](AttentionModelForMachineTranslationWithTensorflow.ipynb), I'll come back first to examine whether it's worth to implement multiple layers (of encoders and decoders) instead of only one. Of course, Deep Learning is all about deep nets, at some point I'll have to check it.\n",
    "\n",
    "In my first attempts I rejected it as a simple 2 layer encoder didn't result in significant improvements. After reading [The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation](https://arxiv.org/abs/1804.09849) and looking into the [Google NMT](https://github.com/tensorflow/nmt) project, I realized that multiple layers are worth it if done correctly.\n",
    "\n",
    "Here I made several hyper parameter decisions to naive stacking:\n",
    "* _Residual Connections_: While I knew residuals from CNNs like [ResNet](https://arxiv.org/abs/1512.03385), I didn't realize how important they are for stacked RNNs. In CNNs they are crucial for very deep networks, but here they already matter for two layers of RNNs. A bit disappointing is that implementing a bidirectional stacked RNN is still more difficulty than it should be. In the end I copy+pasted a `ExtendedMultiRNNCell` implementation from the Google NMT project. It's easy to make subtle mistakes where the computation graph compiles and is trainable, but in the end performs poor.\n",
    "* _L2 Regularization/Weight decay_: For a simple 1-layer RNN the dropout is enough regularization. When I experimented with L2 regularization for 1 layer RNN I found a decrease in performance. For multiple layers it seems useful to have a *low* l2 regularizer so the layers can learn better. As it might be counterproductive, I don't l2 regularize the embeddings (they are anyway pretrained, so they only need some fine adjustments). *Note:* There is a subtle, but important difference between l2 regularization via adapting l2 loss as implemented here and implmenentig weight decay in combination with the Adam Optimizer used here. Read more about this _Bug_ from [Boris Babenko](https://bbabenko.github.io/weight-decay/) or from [fast.ai](http://www.fast.ai/2018/07/02/adam-weight-decay/). It would be important to implement weight decay, that is *wrong* here and might be the reason for the poor performance. I'll rerun it with weight decay instead of l2 loss.\n",
    "* _Learning Rate_: I preferred not to handcraft one, but here I followed the advice I found to have three different phases. A warmup phase with linear increasing learning rate but without training the predefined embeddings. A second phase where everything is trained with default learning rate. And a third phase with an exponential learning rate decay. The idea is to first find a solid initialization for the layer RNNs, then train them till a first saturation and then lower the learning rate so it can make progress (allthough a bit slower).\n",
    "* _Epochs_: I had to increase the number of epochs. Adjusting the learning rate to a lower value always needs some more epochs. Also as there are much more parameters with multiple layers (that are interconnecting) it also needs more time for a saturation. So, beside needing more time anyway to train layer times more RNN parameters, we also need more epochs, so in the end we need much more time.\n",
    "* _Layers_: Google is funny. Google NMT works with 8 layers as they can run each layer on a seperate GPU instance in parallel - of course, having 8 high performance GPU in a computer is great. Well, I don't have the resources, so all I can do here is to implement 2 layers. I'd love to have >= 4 layers (so they are looking a bit above word levels), but that's not doable here for a side project.\n",
    "* _Label smoothing_: Would be nice to have. But for it, we would need to one-hot-encode the RNN outputs what would drastically increase the computational effort. Look to a [stackoverflow](https://stackoverflow.com/questions/49136472/tensorflow-sequence-loss-with-label-smoothing) entry about. The (hidden) usage of `sparse_softmax_cross_entropy` instead of `softmax_cross_entropy` might be a big reason why my tensorflow solution is much more efficient in computing time to the keras solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T09:36:08.241173Z",
     "start_time": "2018-07-02T09:36:06.208430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed random seed to 42\n",
      "Availabe devices: [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15806180947444488576\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7679859098\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3682821903106788990\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Cuda/Cudnn/GPU works as intended\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.python.util import nest\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils.download import download_and_extract_resources\n",
    "from utils.linguistic import bleu_scores_europarl, preprocess_input_europarl as preprocess\n",
    "from utils.preparation import check_gpu_working, Europarl, RANDOM_STATE\n",
    "\n",
    "check_gpu_working()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T09:36:08.245344Z",
     "start_time": "2018-07-02T09:36:08.242532Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 400\n",
    "MAX_TARGET_LENGTH = 450\n",
    "LATENT_DIM = 256\n",
    "LAYERS = 2\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "DROPOUT = 0.25  # Dropout on input and output for the RNN cells, so effective dropout is 0.5, but works slightly better so\n",
    "TEST_SIZE = 2500\n",
    "BEAM_WIDTH = 5\n",
    "EMBEDDING_TRAINABLE = True  # after warmup phase\n",
    "WARMUP_EPOCHS = 5\n",
    "LEARNING_RATE_DECAY_START_EPOCH = 10\n",
    "LEARNING_RATE_DECAY_RATE = 0.965"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T09:43:26.907161Z",
     "start_time": "2018-07-02T09:36:08.246863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n",
      "en.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "en.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (6.2 MB)\n",
      "de.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "de.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (5.7 MB)\n",
      "Total number of unfiltered translations 1920209\n",
      "Filtered translations with length between (1, input=400/target=450) characters: 1864679\n"
     ]
    }
   ],
   "source": [
    "europarl = Europarl()\n",
    "download_and_extract_resources(fnames_and_urls=europarl.external_resources, dest_path=europarl.path)\n",
    "europarl.load_and_preprocess(max_input_length=MAX_INPUT_LENGTH, max_target_length=MAX_TARGET_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T09:43:27.423303Z",
     "start_time": "2018-07-02T09:43:26.909045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resumption of the session</td>\n",
       "      <td>wiederaufnahme der sitzungsperiode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i declare resumed the session of the european ...</td>\n",
       "      <td>ich erkläre die am freitag, dem 0. dezember un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>although, as you will have seen, the dreaded '...</td>\n",
       "      <td>wie sie feststellen konnten, ist der gefürchte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you have requested a debate on this subject in...</td>\n",
       "      <td>im parlament besteht der wunsch nach einer aus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in the meantime, i should like to observe a mi...</td>\n",
       "      <td>heute möchte ich sie bitten - das ist auch der...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         input_texts  \\\n",
       "0                          resumption of the session   \n",
       "1  i declare resumed the session of the european ...   \n",
       "2  although, as you will have seen, the dreaded '...   \n",
       "3  you have requested a debate on this subject in...   \n",
       "4  in the meantime, i should like to observe a mi...   \n",
       "\n",
       "                                        target_texts  \n",
       "0                 wiederaufnahme der sitzungsperiode  \n",
       "1  ich erkläre die am freitag, dem 0. dezember un...  \n",
       "2  wie sie feststellen konnten, ist der gefürchte...  \n",
       "3  im parlament besteht der wunsch nach einer aus...  \n",
       "4  heute möchte ich sie bitten - das ist auch der...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl.df[['input_texts', 'target_texts']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T09:43:27.429003Z",
     "start_time": "2018-07-02T09:43:27.425421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English subwords ['▁this', '▁is', '▁a', '▁test', '▁for', '▁pre', 'tr', 'ained', '▁by', 'te', 'pa', 'ire', 'm', 'bed', 'd', 'ings']\n",
      "German subwords ['▁das', '▁ist', '▁ein', '▁test', '▁für', '▁v', 'ort', 'rain', 'ierte', '▁zeich', 'eng', 'ruppen']\n"
     ]
    }
   ],
   "source": [
    "print(\"English subwords\", europarl.bpe_input.sentencepiece.EncodeAsPieces(\"this is a test for pretrained bytepairembeddings\"))\n",
    "print(\"German subwords\", europarl.bpe_target.sentencepiece.EncodeAsPieces(\"das ist ein test für vortrainierte zeichengruppen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T09:43:28.173049Z",
     "start_time": "2018-07-02T09:43:27.430594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 171)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Those will be the inputs for the seq2seq model (that needs to know how long the sequences can get)\n",
    "max_len_input = europarl.df.input_sequences.apply(len).max()\n",
    "max_len_target = europarl.df.target_sequences.apply(len).max()\n",
    "(max_len_input, max_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T09:43:28.255621Z",
     "start_time": "2018-07-02T09:43:28.174587Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(europarl.df.shape[0]), test_size=0.1, random_state=RANDOM_STATE)  # fixed random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T09:43:28.268623Z",
     "start_time": "2018-07-02T09:43:28.257377Z"
    }
   },
   "outputs": [],
   "source": [
    "# copy+pasted from https://github.com/google/seq2seq/blob/7f485894d412e8d81ce0e07977831865e44309ce/seq2seq/contrib/rnn_cell.py#L39\n",
    "class ExtendedMultiRNNCell(tf.nn.rnn_cell.MultiRNNCell):\n",
    "    \"\"\"Extends the Tensorflow MultiRNNCell with residual connections\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cells,\n",
    "        residual_connections=False,\n",
    "        residual_combiner=\"add\",\n",
    "        residual_dense=False\n",
    "    ):\n",
    "        \"\"\"Create a RNN cell composed sequentially of a number of RNNCells.\n",
    "        Args:\n",
    "          cells: list of RNNCells that will be composed in this order.\n",
    "          state_is_tuple: If True, accepted and returned states are n-tuples, where\n",
    "            `n = len(cells)`.  If False, the states are all\n",
    "            concatenated along the column axis.  This latter behavior will soon be\n",
    "            deprecated.\n",
    "          residual_connections: If true, add residual connections between all cells.\n",
    "            This requires all cells to have the same output_size. Also, iff the\n",
    "            input size is not equal to the cell output size, a linear transform\n",
    "            is added before the first layer.\n",
    "          residual_combiner: One of \"add\" or \"concat\". To create inputs for layer\n",
    "            t+1 either \"add\" the inputs from the prev layer or concat them.\n",
    "          residual_dense: Densely connect each layer to all other layers\n",
    "        Raises:\n",
    "          ValueError: if cells is empty (not allowed), or at least one of the cells\n",
    "            returns a state tuple but the flag `state_is_tuple` is `False`.\n",
    "        \"\"\"\n",
    "        super(ExtendedMultiRNNCell, self).__init__(cells, state_is_tuple=True)\n",
    "        assert residual_combiner in [\"add\", \"concat\", \"mean\"]\n",
    "\n",
    "        self._residual_connections = residual_connections\n",
    "        self._residual_combiner = residual_combiner\n",
    "        self._residual_dense = residual_dense\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Run this multi-layer cell on inputs, starting from state.\"\"\"\n",
    "        if not self._residual_connections:\n",
    "            return super(ExtendedMultiRNNCell, self).__call__(\n",
    "                inputs, state, (scope or \"extended_multi_rnn_cell\")\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope(scope or \"extended_multi_rnn_cell\"):\n",
    "            # Adding Residual connections are only possible when input and output\n",
    "            # sizes are equal. Optionally transform the initial inputs to\n",
    "            # `cell[0].output_size`\n",
    "            if self._cells[0].output_size != inputs.get_shape().as_list()[1] and (self._residual_combiner in [\"add\", \"mean\"]):\n",
    "                inputs = tf.contrib.layers.fully_connected(\n",
    "                    inputs=inputs,\n",
    "                    num_outputs=self._cells[0].output_size,\n",
    "                    activation_fn=None,\n",
    "                    scope=\"input_transform\"\n",
    "                )\n",
    "\n",
    "            # Iterate through all layers (code from MultiRNNCell)\n",
    "            cur_inp = inputs\n",
    "            prev_inputs = [cur_inp]\n",
    "            new_states = []\n",
    "            for i, cell in enumerate(self._cells):\n",
    "                with tf.variable_scope(\"cell_%d\" % i):\n",
    "                    if not nest.is_sequence(state):\n",
    "                        raise ValueError(\n",
    "                            \"Expected state to be a tuple of length %d, but received: %s\" %\n",
    "                            (len(self.state_size), state)\n",
    "                        )\n",
    "                    cur_state = state[i]\n",
    "                    next_input, new_state = cell(cur_inp, cur_state)\n",
    "\n",
    "                    # Either combine all previous inputs or only the current input\n",
    "                    input_to_combine = prev_inputs[-1:]\n",
    "                    if self._residual_dense:\n",
    "                        input_to_combine = prev_inputs\n",
    "\n",
    "                    # Add Residual connection\n",
    "                    if self._residual_combiner == \"add\":\n",
    "                        next_input = next_input + sum(input_to_combine)\n",
    "                    if self._residual_combiner == \"mean\":\n",
    "                        combined_mean = tf.reduce_mean(tf.stack(input_to_combine), 0)\n",
    "                        next_input = next_input + combined_mean\n",
    "                    elif self._residual_combiner == \"concat\":\n",
    "                        next_input = tf.concat([next_input] + input_to_combine, 1)\n",
    "\n",
    "                    cur_inp = next_input\n",
    "                    prev_inputs.append(cur_inp)\n",
    "\n",
    "                    new_states.append(new_state)\n",
    "        new_states = (tuple(new_states) if self._state_is_tuple else array_ops.concat(new_states, 1))\n",
    "        return cur_inp, new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T09:43:38.225213Z",
     "start_time": "2018-07-02T09:43:28.270342Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    encoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_input\n",
    "        dtype=tf.int32,\n",
    "        name='encoder_inputs' \n",
    "    )\n",
    "    batch_size = tf.shape(encoder_inputs)[0]\n",
    "    beam_width = tf.placeholder_with_default(1, shape=[])\n",
    "    dropout = tf.placeholder_with_default(tf.cast(0.0, tf.float32), shape=[])\n",
    "    keep_prob = tf.cast(1.0, tf.float32) - dropout\n",
    "    learning_rate = tf.placeholder_with_default(tf.cast(1e-3, tf.float32), shape=[])\n",
    "\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", \n",
    "        initializer=tf.constant(europarl.bpe_input.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder,\n",
    "        encoder_inputs,\n",
    "        name=\"encoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    input_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='input_sequence_length'\n",
    "    )\n",
    "    \n",
    "    rnn_cell_type = tf.nn.rnn_cell.GRUCell\n",
    "    encoder_forward_cells = [\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name=f'encoder_forward_cell{layer}') \n",
    "        for layer in range(LAYERS)\n",
    "    ]\n",
    "    encoder_backward_cells = [\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name=f'encoder_backward_cell{layer}') \n",
    "        for layer in range(LAYERS)\n",
    "    ]\n",
    "    encoder_forward_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        cell,\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    ) for cell in encoder_forward_cells]\n",
    "    encoder_backward_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        cell,\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    ) for cell in encoder_backward_cells]\n",
    "    encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "        cells_fw = ExtendedMultiRNNCell(encoder_forward_cells, residual_connections=True)._cells,\n",
    "        cells_bw = ExtendedMultiRNNCell(encoder_backward_cells, residual_connections=True)._cells,\n",
    "        inputs=encoder_emb_inp,\n",
    "        sequence_length=input_sequence_length,\n",
    "        time_major=False,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    encoder_state = tf.concat([encoder_state_fw[-1], encoder_state_bw[-1]], -1)\n",
    "    \n",
    "    # Regarding time_major:\n",
    "    # If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "    # If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "    # Using `time_major = True` is a bit more efficient because it avoids\n",
    "    # transposes at the beginning and end of the RNN calculation.  However,\n",
    "    # most TensorFlow data is batch-major, so by default this function\n",
    "    # accepts input and emits output in batch-major form.\n",
    "    #\n",
    "    # for simplicity I work with batch major here instead of time_major\n",
    "    # so I don't need to transpose inputs and transpose back for attention mechanism\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_inputs' \n",
    "    )\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", \n",
    "        initializer=tf.constant(europarl.bpe_target.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder,\n",
    "        decoder_inputs,\n",
    "        name=\"decoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    target_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='target_sequence_length'\n",
    "    )\n",
    "    \n",
    "    # tiling is necessary to work with BeamSearchDecoder\n",
    "    # read carefully the NOTE on constructor in\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper \n",
    "    tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(encoder_outputs, multiplier=beam_width)\n",
    "    tiled_encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width)\n",
    "    tiled_sequence_length = tf.contrib.seq2seq.tile_batch(input_sequence_length, multiplier=beam_width)\n",
    "    \n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        LATENT_DIM,\n",
    "        memory=tiled_encoder_outputs,\n",
    "        memory_sequence_length=tiled_sequence_length,\n",
    "        dtype=tf.float32,\n",
    "        name='attention_mechanism',\n",
    "    )\n",
    "    decoder_rnn_cells = [rnn_cell_type(num_units=LATENT_DIM, name=f'decoder_cell{layer}') for layer in range(LAYERS)]\n",
    "    def residual_fn(inputs, outputs):\n",
    "        tf.contrib.framework.nest.assert_same_structure(inputs, outputs)\n",
    "        inputs_without_attention = tf.slice(inputs, [0, 0], [batch_size, LATENT_DIM])\n",
    "        return tf.contrib.framework.nest.map_structure(lambda inp, out: inp + out, inputs_without_attention, outputs) \n",
    "    for layer in range(1, LAYERS):\n",
    "        decoder_rnn_cells[layer] = tf.contrib.rnn.ResidualWrapper(\n",
    "            decoder_rnn_cells[layer],\n",
    "            residual_fn=residual_fn\n",
    "        )\n",
    "    decoder_rnn_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        cell,\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    ) for cell in decoder_rnn_cells]\n",
    "    attention_cells = [tf.contrib.seq2seq.AttentionWrapper(\n",
    "        cell,\n",
    "        attention_mechanism,\n",
    "        attention_layer_size=LATENT_DIM,\n",
    "        name=f'attention_wrapper{layer}',\n",
    "    ) for layer, cell in enumerate(decoder_rnn_cells)] \n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell(attention_cells)\n",
    "\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        inputs=decoder_emb_inp, \n",
    "        sequence_length=target_sequence_length,\n",
    "        time_major=False,\n",
    "        name=\"decoder_training_helper\",\n",
    "    )\n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        units=len(europarl.bpe_target.tokens),\n",
    "        use_bias=False,\n",
    "        name='projection_layer',\n",
    "    )\n",
    "    \n",
    "    initial_state = tuple(\n",
    "        attention_cells[0].zero_state(dtype=tf.float32, batch_size=batch_size).clone(\n",
    "            cell_state=encoder_state\n",
    "        )\n",
    "        for _ in range(LAYERS)\n",
    "    )\n",
    "\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=decoder_cell,\n",
    "        helper=training_helper,\n",
    "        initial_state=initial_state,\n",
    "        output_layer=projection_layer,\n",
    "    )\n",
    "    outputs, _final_state, _final_sequence_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder,\n",
    "        output_time_major=False,\n",
    "        impute_finished=False,\n",
    "    )\n",
    "    logits = outputs.rnn_output\n",
    "    \n",
    "    decoder_outputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_outputs',\n",
    "    )\n",
    "    target_weights = tf.cast(tf.sequence_mask(target_sequence_length), dtype=tf.float32)\n",
    "    l2_lambda = tf.placeholder_with_default(tf.cast(1e-5, tf.float32), shape=[])\n",
    "    loss_l2 = tf.add_n([\n",
    "        tf.nn.l2_loss(v)\n",
    "        for v in tf.trainable_variables()\n",
    "        if not re.match(r'embedding_(de|en)coder', v.name)  # don't regularize embeddings\n",
    "    ])\n",
    "    train_loss = tf.contrib.seq2seq.sequence_loss(logits, decoder_outputs, target_weights) + l2_lambda * loss_l2\n",
    "\n",
    "    params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, params)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "        t_list=gradients,\n",
    "        clip_norm=1.,\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    update_step = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "    \n",
    "    params_without_embeddings = [v for v in tf.trainable_variables() if not re.match(r'embedding_(de|en)coder', v.name)]\n",
    "    gradients_without_embeddings = tf.gradients(train_loss, params_without_embeddings)\n",
    "    clipped_gradients_without_embeddings, _ = tf.clip_by_global_norm(\n",
    "        t_list=gradients_without_embeddings,\n",
    "        clip_norm=1.,\n",
    "    )\n",
    "    update_step_without_embeddings = optimizer.apply_gradients(zip(clipped_gradients_without_embeddings, params_without_embeddings))\n",
    "    \n",
    "    inference_decoder_initial_state = tuple(\n",
    "        attention_cells[0].zero_state(\n",
    "            dtype=tf.float32,\n",
    "            batch_size=batch_size * beam_width  # tricky and somehow unintuitive, but necessary\n",
    "        ).clone(\n",
    "            cell_state=tiled_encoder_state\n",
    "        ) for _ in range(LAYERS)\n",
    "    )\n",
    "    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "        cell=decoder_cell,\n",
    "        embedding=embedding_decoder,\n",
    "        start_tokens=tf.fill([batch_size], europarl.bpe_target.start_token_idx),\n",
    "        end_token=europarl.bpe_target.stop_token_idx,\n",
    "        initial_state=inference_decoder_initial_state,\n",
    "        beam_width=BEAM_WIDTH,\n",
    "        output_layer=projection_layer,\n",
    "        length_penalty_weight=1.0,  # https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/\n",
    "    )\n",
    "\n",
    "    \n",
    "    inference_outputs, _inference_final_state, _inference_final_sequence_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inference_decoder,\n",
    "        maximum_iterations=tf.round(tf.reduce_max(input_sequence_length) * 2),  # a bit more flexible than max_len_target\n",
    "        impute_finished=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T09:43:38.237560Z",
     "start_time": "2018-07-02T09:43:38.226461Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_train_batch(batch_ids, epoch):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    base_lr = 1e-3\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        lr = base_lr * (0.5 + epoch / (2 * WARMUP_EPOCHS))\n",
    "    elif epoch < LEARNING_RATE_DECAY_START_EPOCH:\n",
    "        lr = base_lr\n",
    "    else:\n",
    "        lr = base_lr * (LEARNING_RATE_DECAY_RATE ** (epoch - LEARNING_RATE_DECAY_START_EPOCH))\n",
    "    pred, loss, _ = sess.run(\n",
    "        fetches=[\n",
    "            outputs, train_loss, update_step if epoch >= WARMUP_EPOCHS else update_step_without_embeddings\n",
    "        ],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "            dropout: DROPOUT,\n",
    "            # embedding_trainable: epoch >= WARMUP_EPOCHS,\n",
    "            learning_rate: lr\n",
    "        }\n",
    "    )\n",
    "    return loss, lr\n",
    "\n",
    "def run_val_batch(batch_ids):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    loss = sess.run(\n",
    "        fetches=[train_loss],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def run_validation_loss():\n",
    "    return np.mean([\n",
    "        run_val_batch(ids)\n",
    "        for ids \n",
    "        in np.array_split(val_ids, np.ceil(len(val_ids) / BATCH_SIZE))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T09:43:38.245733Z",
     "start_time": "2018-07-02T09:43:38.238730Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    sequenced = europarl.bpe_input.subword_indices(preprocess(sentence))\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [sequenced],\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    \n",
    "    beam_search_output = sess.run(\n",
    "        fetches=[inference_outputs],\n",
    "        feed_dict={\n",
    "            encoder_inputs: padded,\n",
    "            input_sequence_length: [len(sequenced)],\n",
    "            beam_width: BEAM_WIDTH,\n",
    "        }\n",
    "    )[0]\n",
    "    \n",
    "    return europarl.bpe_target.sentencepiece.DecodePieces([\n",
    "        europarl.bpe_target.tokens[idx] for idx in beam_search_output.predicted_ids[0, :, 0].tolist()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T20:44:09.636047Z",
     "start_time": "2018-07-02T09:43:38.247003Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2fc632bad046eeac22b4e4d2e0e067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0005, train_loss=3.58372, val_loss=2.6351\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22693d1b1d5949aaad95c179e4342a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0006, train_loss=2.96221, val_loss=2.43479\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1231cfeb49d44ea89a96402b438190f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0007, train_loss=2.85717, val_loss=2.37503\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b4f3dfc14f403682738ca258b6692d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0008, train_loss=2.82346, val_loss=2.35887\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6764e6f32b24b8a9d219d6a1943b76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0009, train_loss=2.81353, val_loss=2.35356\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8576a1f9cc004e17b7d6ac3d6856b146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.14099431381207564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a71f538e6d84446be0d3062f6c8b247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.46987, val_loss=2.02201\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46929cd0c2f4b0993df16503ef81f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.34115, val_loss=1.96864\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b25b20e9c5a495391622ffd0ab34604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 8', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.29909, val_loss=1.94105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a8b93fcae549e785270e2e0163f88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 9', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.27621, val_loss=1.92889\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec56e5eb65e4aa6b1e4efeb484c3b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 10', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.26106, val_loss=1.91814\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12360e3a1a64a70a28d6300cf47d5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.18268257125838908\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc70d24a55a4b5f9f953c35390b3f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 11', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.24983, val_loss=1.90511\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd4bd2390dd413596011662d699ddea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 12', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000965, train_loss=2.23226, val_loss=1.89239\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4c846677f644859b2765316e39b2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 13', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000931225, train_loss=2.21638, val_loss=1.88021\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e32229219ba4653981722b6d2987bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 14', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000898632, train_loss=2.20148, val_loss=1.86752\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2408a454f8a341edac64d8d2316c8d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 15', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.00086718, train_loss=2.18754, val_loss=1.85285\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3ab6bed4a44c29aab2d2308d41f93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.18906517656931873\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675621767a244482a5a5d7f6c85339fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 16', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000836829, train_loss=2.17578, val_loss=1.85149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6153fa893a04954b36e4e410a4865a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 17', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.00080754, train_loss=2.16364, val_loss=1.8408\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9478bfa4c9e74cb48a6f48c6e3ae0cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 18', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000779276, train_loss=2.15279, val_loss=1.83038\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6067a566acc6456da3802167b750b203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 19', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000752001, train_loss=2.14217, val_loss=1.82086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8fa8da383e4318b2cf5bf8b56b5f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 20', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000725681, train_loss=2.13224, val_loss=1.81306\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afbec78d7574b9abd1123dc71500e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.19385107675206542\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af13f00c5b14d59b8e694ea8816f490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 21', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000700282, train_loss=2.12291, val_loss=1.80654\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c37ecb5b7874347916decfe669acdee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 22', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000675772, train_loss=2.11356, val_loss=1.80375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a304807680524c2eb2461eeda8fa0ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 23', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.00065212, train_loss=2.10532, val_loss=1.79419\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f270a1bcb429406bbeb2d12c8b59308b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 24', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000629296, train_loss=2.09716, val_loss=1.78314\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b97c959540b452f921abb4a5a2d3bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 25', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000607271, train_loss=2.08919, val_loss=1.7774\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec10ee43cd44bdba55a2ef6de5b3d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.19723175702052279\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930e24da5ca14d55b86bebb7b424924b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 26', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000586016, train_loss=2.08126, val_loss=1.77253\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5eb371742c4485684d161338c97165d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 27', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000565506, train_loss=2.07389, val_loss=1.76668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd36369e8f34e4f9dab57fc3e9b9ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 28', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000545713, train_loss=2.06696, val_loss=1.76386\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb505eb47284b588522ad85bde35ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 29', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000526613, train_loss=2.06003, val_loss=1.75887\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10825a0cf73442f9087f55cd6007fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 30', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000508182, train_loss=2.05353, val_loss=1.75075\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(\n",
    "    allow_soft_placement=True,  # needed as recommendation from https://github.com/tensorflow/tensorflow/issues/2292\n",
    "    log_device_placement=True,\n",
    ")\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batches_per_epoch = np.ceil(len(train_ids) / BATCH_SIZE)\n",
    "for epoch in range(EPOCHS):\n",
    "    shuffled_ids = np.random.permutation(train_ids)\n",
    "    batch_splits = np.array_split(shuffled_ids, batches_per_epoch)\n",
    "    train_losses = []\n",
    "    N = len(batch_splits)\n",
    "    with tqdm(batch_splits, desc=f\"Epoch {epoch+1}\") as t:\n",
    "        for train_batch_ids in t:\n",
    "            batch_loss, lr = run_train_batch(train_batch_ids, epoch=epoch)\n",
    "            train_losses.append(batch_loss)\n",
    "            t.set_postfix(train_loss=np.mean(train_losses))\n",
    "        print(f\"learning rate={lr:.6}, train_loss={np.mean(train_losses):.6}, val_loss={run_validation_loss():.6}\")\n",
    "    \n",
    "    if epoch > 0 and ((epoch+1) % 5 == 0) and (epoch+1 < EPOCHS):\n",
    "        bleu = bleu_scores_europarl(\n",
    "            input_texts=europarl.df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "            target_texts=europarl.df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "            predict=lambda text: predict(text)\n",
    "        )\n",
    "        print(f'average BLEU on test set = {bleu.mean()}')\n",
    "        \n",
    "validation_input_sequences = europarl.df.input_sequences.iloc[val_ids[:BATCH_SIZE]]\n",
    "validation_input_lengths = validation_input_sequences.apply(len)\n",
    "\n",
    "validation_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    validation_input_sequences,\n",
    "    maxlen=max_len_input,\n",
    "    dtype=int,\n",
    "    padding='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T20:44:11.204988Z",
     "start_time": "2018-07-06T20:44:09.637402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/tfattentionmodel_2layers.ckpt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = f'tfattentionmodel_{LAYERS}layers'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, f\"data/{name}.ckpt\")\n",
    "# tfattentionmodel_2layers.ckpt.index https://drive.google.com/open?id=1t5f7vbI6sdBqlTJ3DguUnJwKjx2NInC4 \n",
    "# tfattentionmodel_2layers.cpkt.meta https://drive.google.com/open?id=1Ikp266cw7c93S6mCYHkE0SaBfI1WZtmF \n",
    "# tfattentionmodel_2layers.cpkt.data-00000-of-00001 https://drive.google.com/open?id=1QMT_5nA7dOHe5G8FdCOY0MDvwh5b5_-A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T20:44:11.680754Z",
     "start_time": "2018-07-06T20:44:11.206904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello.' --> 'helfen sie.'\n",
      "'you are welcome.' --> 'sie sind begrüßenswert.'\n",
      "'how do you do?' --> 'wie tun sie?'\n",
      "'i hate mondays.' --> 'ich habe versprochen.'\n",
      "'i am a programmer.' --> 'ich bin ein programm.'\n",
      "'data is the new oil.' --> 'daten ist das neue öl.'\n",
      "'it could be worse.' --> 'es könnte schlimmer sein.'\n",
      "'i am on top of it.' --> 'ich bin zu diesem thema.'\n",
      "'n° uno' --> 'niedrigo'\n",
      "'awesome!' --> 'das ist!'\n",
      "'put your feet up!' --> 'ich hüte sie!'\n",
      "'from the start till the end!' --> 'aus beginn des starts!'\n",
      "'from dusk till dawn.' --> 'aus dem dusus.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T20:44:14.584434Z",
     "start_time": "2018-07-06T20:44:11.682040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'i declare resumed the session of the european parliament adjourned on friday 0 december 0, and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.', got 'ich erkläre die sitzungsperiode des europäischen parlaments für freitag 0. dezember 0 wieder aufgenommen, und ich möchte ihnen einmal noch einmal ein glückliches neues jahr in der hoffnung wünschen, dass sie eine freude feiern.', exp: 'ich erkläre die am freitag, dem 0. dezember unterbrochene sitzungsperiode des europäischen parlaments für wiederaufgenommen, wünsche ihnen nochmals alles gute zum jahreswechsel und hoffe, daß sie schöne ferien hatten.'\n",
      "Original \"although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\", got 'obwohl sie sich gesehen haben, hat das berühmte \"millennium-verbug\" nicht materialiert, wie die menschen in einer reihe von ländern eine reihe von naturkatastrophen leiden, die wirklich schrecklich waren.', exp: 'wie sie feststellen konnten, ist der gefürchtete \"millenium-bug \" nicht eingetreten. doch sind bürger einiger unserer mitgliedstaaten opfer von schrecklichen naturkatastrophen geworden.'\n",
      "Original 'you have requested a debate on this subject in the course of the next few days, during this part-session.', got 'sie haben in dieser sitzung eine debatte über dieses thema gebeten.', exp: 'im parlament besteht der wunsch nach einer aussprache im verlauf dieser sitzungsperiode in den nächsten tagen.'\n",
      "Original \"in the meantime, i should like to observe a minute' s silence, as a number of members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the european union.\", got 'in der zwischenzeit möchte ich eine schweigeminute beobachten, wie einige abgeordnete im namen aller opfer, insbesondere denen der schrecklichen stärke, in den verschiedenen ländern der europäischen union gebeten haben.', exp: 'heute möchte ich sie bitten - das ist auch der wunsch einiger kolleginnen und kollegen -, allen opfern der stürme, insbesondere in den verschiedenen ländern der europäischen union, in einer schweigeminute zu gedenken.'\n",
      "Original \"please rise, then, for this minute' s silence.\", got 'bitte bitte ich um diese schweigeminute.', exp: 'ich bitte sie, sich zu einer schweigeminute zu erheben.'\n",
      "Original \"(the house rose and observed a minute' s silence)\", got '(das parlament erhebt sich zu einer schweiminminute.)', exp: '(das parlament erhebt sich zu einer schweigeminute.)'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin!', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka.', got 'sie werden von der presse und dem fernsehen bekannt sein, dass es eine reihe von bombenverbaten und töen in sri lanka gab.', exp: 'wie sie sicher aus der presse und dem fernsehen wissen, gab es in sri lanka mehrere bombenexplosionen mit zahlreichen toten.'\n",
      "Original 'one of the people assassinated very recently in sri lanka was mr kumar ponnambalam, who had visited the european parliament just a few months ago.', got 'eine der menschen, die in sri lanka in sri lanka in sri lanka einbehalten, war herr kolley ponyalamam, der das europäische parlament vor einigen monaten besucht hatte.', exp: 'zu den attentatsopfern, die es in jüngster zeit in sri lanka zu beklagen gab, zählt auch herr kumar ponnambalam, der dem europäischen parlament erst vor wenigen monaten einen besuch abgestattet hatte.'\n",
      "Original \"would it be appropriate for you, madam president, to write a letter to the sri lankan president expressing parliament's regret at his and the other violent deaths in sri lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\", got 'wäre es angebracht, frau präsidentin, ein schreiben an den präsidenten sri lankas zu schreiben, das das bedauern des parlaments zu seiner und anderen gewaltigen todesfälle in sri lanka ausdrücklich ausdrücklich ausdrücklich ausdrücklich ausdrücklich ausdrücklich ausdrücklich auf eine friedliche versöhnung für eine sehr schwierige situation süht.', exp: 'wäre es angemessen, wenn sie, frau präsidentin, der präsidentin von sri lanka in einem schreiben das bedauern des parlaments zum gewaltsamen tod von herrn ponnambalam und anderen bürgern von sri lanka übermitteln und sie auffordern würden, alles in ihrem kräften stehende zu tun, um nach einer friedlichen lösung dieser sehr schwierigen situation zu suchen?'\n",
      "Original 'yes, mr evans, i feel an initiative of the type you have just suggested would be entirely appropriate.', got 'ja, herr evans, ich halte eine initiative der art, die sie vorgeschlagen haben, wäre völlig angemessen.', exp: 'ja, herr evans, ich denke, daß eine derartige initiative durchaus angebracht ist.'\n",
      "Original 'if the house agrees, i shall do as mr evans has suggested.', got 'wenn das parlament einverstanden ist, werde ich tun, wie herr evans vorgeschlagen hat.', exp: 'wenn das haus damit einverstanden ist, werde ich dem vorschlag von herrn evans folgen.'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin!', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'i would like your advice about rule 0 concerning inadmissibility.', got 'ich möchte ihre ratifizierung über artikel 0 über die zulässigkeit.', exp: 'könnten sie mir eine auskunft zu artikel 0 im zusammenhang mit der unzulässigkeit geben?'\n",
      "Original 'my question relates to something that will come up on thursday and which i will then raise again.', got 'meine frage bezieht sich auf etwas, das am donnerstag kommen wird, und ich werde noch einmal wiederholen.', exp: 'meine frage betrifft eine angelegenheit, die am donnerstag zur sprache kommen wird und auf die ich dann erneut verweisen werde.'\n",
      "Original 'the cunha report on multiannual guidance programmes comes before parliament on thursday and contains a proposal in paragraph 0 that a form of quota penalties should be introduced for countries which fail to meet their fleet reduction targets annually.', got 'der bericht cunha über mehrjährige reitlinienprogramme wird vor dem parlament am donnerstag kommen und einen vorschlag in ziffer 0 enthält, dass eine form von quotenstrafen für länder eingeführt werden sollte, die ihre flottenziele jährlich nicht erfüllen.', exp: 'das parlament wird sich am donnerstag mit dem cunha-bericht über mehrjährige ausrichtungsprogramme befassen, der in absatz 0 vorschlägt, daß länder, die ihr soll zur flottenverkleinerung nicht erfüllen, jährlich mit einer art quotenstrafe belegt werden sollen.'\n",
      "Original 'it says that this should be done despite the principle of relative stability.', got 'es sagt, das sollte trotz des prinzips der relativen stabilität erfolten.', exp: 'und zwar sollen derartige strafen trotz des grundsatzes der relativen stabilität verhängt werden.'\n",
      "Original 'i believe that the principle of relative stability is a fundamental legal principle of the common fisheries policy and a proposal to subvert it would be legally inadmissible.', got 'ich glaube, dass der grundsatz der relativen stabilität ein grundlegender rechtsgrundsatz der gemeinsamen fischereipolitik ist und einen vorschlag zur werbung sein würde.', exp: 'ich meine, daß der grundsatz der relativen stabilität einen elementaren rechtsgrundsatz der gemeinsamen fischereipolitik darstellt und ein vorschlag, diesen zu unterlaufen, rechtlich unzulässig wäre.'\n",
      "Original 'i want to know whether one can raise an objection of that kind to what is merely a report, not a legislative proposal, and whether that is something i can competently do on thursday.', got 'ich möchte wissen, ob ein solches ziel, was nur einen bericht ist, nicht einen legislativvorschlag, und ob das, was ich am donnerstag habe, zu tun ist.', exp: 'ich möchte wissen, ob es möglich ist, einen einwand gegen ein dokument zu erheben, bei dem es sich lediglich um einen bericht und keinen legislativvorschlag handelt, und ob ich befugt bin, dies am donnerstag zu tun.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in europarl.df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T20:44:17.302694Z",
     "start_time": "2018-07-06T20:44:14.585963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'it is important not to underestimate the work involved.', got 'es ist wichtig, die arbeit zu unterschätzen.', exp: 'das sollte man nicht unterschätzen.'\n",
      "Original 'mr vanhanen, you were mr calm, and i think you, mr tuomioja, were mr collected.', got 'herr vanhanen, sie waren herrn ruh, und ich denke, sie, herr tuomioja, herr kollege.', exp: 'herr vanhanen, sie waren mr. calm, und, ich denke, sie, herr tuomioja, waren mr. collected.'\n",
      "Original \"most members of this parliament are aware of the commission's efforts to make sure that the european support for the palestinian authority is money that is properly spent, well spent and spent in ways that help to promote pluralism, the rule of law and clean government in the palestinian territories.\", got 'die meisten mitglieder dieses parlaments sind die bemühungen der kommission, sicherzustellen, dass die europäische unterstützung für die palästinensische autonomiebehörde geld ausgegeben wird, die ordnungsgemäß ausgegeben wird, ausgegeben und ausgegeben wird, um den pluralismus, die rechtsstaatlichkeit und saubere regierung in den palästinensischen gebieten zu fördern.', exp: 'die meisten abgeordneten dieses parlaments wissen um die bemühungen der kommission, dafür zu sorgen, dass die hilfe der eu für die palästinensische autonomiebehörde gut angelegt ist, dass die bereitgestellten mittel für zwecke ausgegeben werden, die der förderung des pluralismus, der rechtsstaatlichkeit und der bekämpfung der korruption in den palästinensischen gebieten dienen.'\n",
      "Original 'i therefore advocate that the eu and russia must step up negotiations on a new partnership and cooperation agreement based on the mutual interdependence of the eu and russia.', got 'ich befürworte daher, dass die eu und russland verhandlungen über ein neues partnerschafts- und kooperationsabkommen zur gegenseitigen gegenseitigen abhängigkeit der eu und russland schließen müssen.', exp: 'deswegen befürworte ich, dass die eu und russland ihre verhandlungen über ein neues partnerschafts- und kooperationsabkommen auf der grundlage der gegenseitigen abhängigkeit der eu und russland intensivieren.'\n",
      "Original 'the work to be done on logistics does not involve regulating this rapidly growing sector, but ensuring that it has a sustainable future by allowing it to mobilise the efficiency potential that still exists in the transport business.', got 'die arbeit, die auf logistik zu tun ist, nicht mit der regelung dieses rates wachsenden sektor, sondern sicherzustellen, dass es eine nachhaltige zukunft gibt, wenn es ermöglicht, das effizienzspotenzial zu mobilisieren, das noch im verkehrsunternehmen existiert.', exp: 'ziel der anstrengungen auf dem gebiet der logistik ist es nicht, diesen wachstumssektor zu reglementieren, sondern ihm eine nachhaltige zukunft zu sichern, indem er die möglichkeit erhält, das noch im verkehrssektor vorhandene effizienzpotenzial zu mobilisieren.'\n",
      "Original 'nevertheless a comparison of national legislations would be an important factor when we adopt common rules at european level.', got 'dennoch wäre ein vergleich der nationalen rechtsvorschriften ein wichtiger faktor, wenn wir gemeinsame regeln auf europäischer ebene annehmen.', exp: 'trotzdem wäre ein vergleich der nationalen rechtsvorschriften ein wichtiger faktor, wenn auf europäischer ebene gemeinsame regelungen verabschiedet werden.'\n",
      "Original \"from this point of view, we accept the commission proposal in that - and this should be made very clear here - the genuine political debate will come about when we see what the council makes of the commission's preliminary draft budget.\", got 'aus diesem grund akzeptieren wir den vorschlag der kommission, und dies sollte deutlich gemacht werden - die echte politische debatte wird darüber kommen, wenn wir sehen, was der rat vor dem haushaltsentwurf der kommission macht.', exp: 'aus dieser sicht heraus machen wir uns den vorschlag der kommission in dem bewußtsein - und das wollen wir hier klarstellen - zu eigen, daß die wirkliche politische debatte erst dann beginnt, wenn feststeht, wie der rat den haushaltsvorentwurf der kommission aufnimmt.'\n",
      "Original 'this is also the main reason why the council took so long to adopt this framework decision.', got 'das ist auch der hauptgrund, weshalb der rat diesen rahmenbeschluss so lange angenommen hat.', exp: 'das ist auch der hauptgrund dafür, weshalb die annahme des rahmenbeschlusses durch den rat so lange gedauert hat.'\n",
      "Original '(the oral amendment was adopted)', got '(der mündliche änderungsantrag wurde angenommen.)', exp: '(der mündliche änderungsantrag wird angenommen)'\n",
      "Original 'the adoption of this text now enables us to express our initial position.', got 'die annahme dieses textes ermöglicht uns unser ursprüngliche standpunkt.', exp: 'wenn wir den text jetzt verabschieden, können wir unsere anfängliche haltung zum ausdruck bringen.'\n",
      "Original 'if new priorities are to be added each year, further resources must also be provided.', got 'wenn neue prioritäten zu einem jahr kommen, müssen weitere ressourcen bereitgestellt werden.', exp: 'wenn wir jedes jahr neue prioritäten hinzufügen wollen, müssen wir auch neue ressourcen zur verfügung stellen.'\n",
      "Original 'for the sake of peace and for europe, i very much hope that it can be put into practice and that it will be successful.', got 'für den frieden und für europa hoffe ich sehr, daß es in der praxis umgesetzt werden kann und dass es erfolgreich sein wird.', exp: 'für den frieden und für europa ist es mein dringlicher wunsch, daß dieses beispiel konkrete gestalt annehmen und zu einem erfolg führen möge.'\n",
      "Original 'sixty per cent of them said that they want the uk to renegotiate our relationship with the european union to a simple free trade agreement and no more than that.', got 'sechs prozent davon sagten, dass das vereinigte königreich unsere beziehungen mit der europäischen union auf ein einfaches freihandelsabkommen verhandelt und nicht mehr als dies.', exp: 'sechzig prozent der befragten firmen forderten, dass das vereinigte königreich seine beziehungen zur europäischen union dahin gehend ändern soll, dass nur noch ein einfaches freihandelsabkommen mit ihr besteht und nicht mehr.'\n",
      "Original 'over and again, we have said that it needs to be done by relaunching the lisbon strategy, but that strategy will continue to fade away if we do not invest seriously and forcefully in industrial policy.', got 'und wir haben bereits gesagt, dass es durch die wiederbelebung der lissabon-strategie getan werden muss, aber diese strategie wird weiterhin uns weiterentwickeln, wenn wir in der industriepolitik nicht ernsthaft investieren.', exp: 'wir haben immer wieder betont, dass das durch eine wiederbelebung der lissabon-strategie geschehen muss, doch diese strategie wird weiter dahinschwinden, wenn wir nicht ernsthaft und energisch in die industriepolitik investieren.'\n",
      "Original 'secondly, we gave this matter very careful consideration at the time we tabled the resolution and we are on no account prepared to withdraw it.', got 'zweitens haben wir diese angelegenheit sehr sorgfältig über die zeit gegeben, die wir den entschließungsantrag vorgelegt haben, und wir sind nicht bereit, ihn zurückzuziehen.', exp: 'zweitens haben wir den zeitpunkt, zu dem wir diese sache eingebracht haben, sorgfältig erwogen und sind auf keinen fall bereit, die entschließung zurückzuziehen.'\n",
      "Original 'this is a country which has enormous potential - and our readiness to engage with iran constructively has been made over and over.', got 'das ist ein land, das enorme potenzial besitzt - und unsere bereitschaft, mit dem iran konstruktiv zu handelen.', exp: 'dies ist ein land, welches über ein enormes potenzial verfügt - und unsere bereitschaft, uns konstruktiv auf den iran einzulassen, ist immer wieder gezeigt worden.'\n",
      "Original 'sixthly, the rules which mr solana imposed last year in august without any consultation now fall under this regulation and will also be verified against the same.', got 'sechstens, die vorschriften, die herr solana im august im august verschlossen hat, ohne jede konsultation jetzt unter dieser verordnung zu fallen, wird auch gegen das gleiche verfahren geprüft werden.', exp: 'sechstens: die vorschriften, die herr solana im august vergangenen jahres ohne rücksprache durchgesetzt hat, fallen nunmehr unter diese verordnung und werden auch daran geprüft.'\n",
      "Original 'the debate is not therefore whether we are in favour of or opposed to the alternative methods.', got 'die aussprache ist daher nicht, ob wir für die alternativen methoden stimmen.', exp: 'es geht also nicht darum, ob wir für oder gegen alternativmethoden sind.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'in conclusion, i would say that only a realistic policy, appropriate to the needs of the population, the environment and an increasingly high-quality market, is capable of achieving the objectives which we think the european union should be aiming at in terms of viticultural policy.', got 'abschließend möchte ich sagen, dass nur eine realistische politik, die den bedürfnissen der bevölkerung, der umwelt und einem zunehmenden hochwertigen markt angemessen ist, in der lage ist, die ziele zu erreichen, die wir denken, dass die europäische union in bezug auf die vitolitikpolitik zielen sollte.', exp: 'abschließend möchte ich feststellen, daß nur eine realistische, auf die bedürfnisse der menschen, der umwelt und den zunehmend qualitätsbetonten markt abgestimmte politik geeignet ist, die ziele, die wir im bereich des weinbaus in der europäischen union politisch verankern wollen, zu verwirklichen.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on validation set\n",
    "val_df = europarl.df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T20:50:32.825084Z",
     "start_time": "2018-07-06T20:44:17.304214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ffed96508940ec9676e78826647bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.2023468449059349\n"
     ]
    }
   ],
   "source": [
    "bleu = bleu_scores_europarl(\n",
    "    input_texts=europarl.df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    target_texts=europarl.df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    predict=lambda text: predict(text)\n",
    ")\n",
    "print(f'average BLEU on test set = {bleu.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.995932Z",
     "start_time": "2018-05-13T18:17:20.994111Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "There is only a marginal improvement ($0.202 > 0.199$) while the model needed twice as much time to train. And it's not clear that even the tiny improvement is not a product of the hyperparameter adjustments. I'll need to rerun this experiment with weight decay to check whether 2 layers can be at least a significant (even if it is small improvement).\n",
    "\n",
    "Of course, even small improvements could add to a solid improvement. Google uses for its NMT model 8 layers (that can in parallel on one machine with 8 GPUs), so there might be potential anyway. But of course for a side project with only one GPU access, that would not worth to go on in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 294,
   "position": {
    "height": "40px",
    "left": "553px",
    "right": "192px",
    "top": "132px",
    "width": "615px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
