{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Multiple Layers for Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing [Attention](AttentionModelForMachineTranslationWithTensorflow.ipynb), I'll come back first to examine whether it's worth to implement multiple layers (of encoders and decoders) instead of only one. Of course, Deep Learning is all about deep nets, at some point I'll have to check it.\n",
    "\n",
    "In my first attempts I rejected it as a simple 2 layer encoder didn't result in significant improvements. After reading [The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation](https://arxiv.org/abs/1804.09849) and looking into the [Google NMT](https://github.com/tensorflow/nmt) project, I realized that multiple layers are worth it if done correctly.\n",
    "\n",
    "Here I made several hyper parameter decisions to naive stacking:\n",
    "* _Residual Connections_: While I knew residuals from CNNs like [ResNet](https://arxiv.org/abs/1512.03385), I didn't realize how important they are for stacked RNNs. In CNNs they are crucial for very deep networks, but here they already matter for two layers of RNNs. A bit disappointing is that implementing a bidirectional stacked RNN is still more difficulty than it should be. In the end I copy+pasted a `ExtendedMultiRNNCell` implementation from the Google NMT project. It's easy to make subtle mistakes where the computation graph compiles and is trainable, but in the end performs poor.\n",
    "* _L2 Regularization/Weight decay_: For a simple 1-layer RNN the dropout is enough regularization. When I experimented with L2 regularization for 1 layer RNN I found a decrease in performance. For multiple layers it seems useful to have a *low* l2 regularizer so the layers can learn better. As it might be counterproductive, I don't l2 regularize the embeddings (they are anyway pretrained, so they only need some fine adjustments). *Note:* There is a subtle, but important difference between l2 regularization via adapting l2 loss as implemented here and implmenentig weight decay in combination with the Adam Optimizer used here. Read more about this _Bug_ from [Boris Babenko](https://bbabenko.github.io/weight-decay/) or from [fast.ai](http://www.fast.ai/2018/07/02/adam-weight-decay/) or the direct [arxiv paper about](https://arxiv.org/abs/1711.05101). It would be important to implement weight decay, that is *wrong* here and might be the reason for the poor performance. I'll rerun it with weight decay instead of l2 loss.\n",
    "* _Learning Rate_: I preferred not to handcraft one, but here I followed the advice I found to have three different phases. A warmup phase with linear increasing learning rate but without training the predefined embeddings. A second phase where everything is trained with default learning rate. And a third phase with an exponential learning rate decay. The idea is to first find a solid initialization for the layer RNNs, then train them till a first saturation and then lower the learning rate so it can make progress (allthough a bit slower).\n",
    "* _Epochs_: I had to increase the number of epochs. Adjusting the learning rate to a lower value always needs some more epochs. Also as there are much more parameters with multiple layers (that are interconnecting) it also needs more time for a saturation. So, beside needing more time anyway to train layer times more RNN parameters, we also need more epochs, so in the end we need much more time.\n",
    "* _Layers_: Google is funny. Google NMT works with 8 layers as they can run each layer on a seperate GPU instance in parallel - of course, having 8 high performance GPU in a computer is great. Well, I don't have the resources, so all I can do here is to implement 2 layers. I'd love to have >= 4 layers (so they are looking a bit above word levels), but that's not doable here for a side project.\n",
    "* _Label smoothing_: Would be nice to have. But for it, we would need to one-hot-encode the RNN outputs what would drastically increase the computational effort. Look to a [stackoverflow](https://stackoverflow.com/questions/49136472/tensorflow-sequence-loss-with-label-smoothing) entry about. The (hidden) usage of `sparse_softmax_cross_entropy` instead of `softmax_cross_entropy` might be a big reason why my tensorflow solution is much more efficient in computing time to the keras solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-07T20:53:03.550254Z",
     "start_time": "2018-07-07T20:53:01.609826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed random seed to 42\n",
      "Availabe devices: [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2047769553909087464\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7769512346\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17468272896898241021\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Cuda/Cudnn/GPU works as intended\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.python.util import nest\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from utils.download import download_and_extract_resources\n",
    "from utils.linguistic import bleu_scores_europarl, preprocess_input_europarl as preprocess\n",
    "from utils.preparation import check_gpu_working, Europarl, RANDOM_STATE\n",
    "\n",
    "check_gpu_working()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-07T20:53:03.554774Z",
     "start_time": "2018-07-07T20:53:03.551738Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 400\n",
    "MAX_TARGET_LENGTH = 450\n",
    "LATENT_DIM = 256\n",
    "LAYERS = 2\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 128\n",
    "DROPOUT = 0.25  # Dropout on input and output for the RNN cells, so effective dropout is 0.5, but works slightly better so\n",
    "TEST_SIZE = 2500\n",
    "BEAM_WIDTH = 5\n",
    "EMBEDDING_TRAINABLE = True  # after warmup phase\n",
    "WARMUP_EPOCHS = 5\n",
    "LEARNING_RATE_DECAY_START_EPOCH = 10\n",
    "LEARNING_RATE_DECAY_RATE = 0.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T13:03:31.674082Z",
     "start_time": "2018-05-08T13:03:31.670919Z"
    }
   },
   "source": [
    "## Download and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-07T20:59:57.685952Z",
     "start_time": "2018-07-07T20:53:03.556135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-en.tgz already downloaded (188.6 MB)\n",
      "en.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "en.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (6.2 MB)\n",
      "de.wiki.bpe.op5000.model already downloaded (0.3 MB)\n",
      "de.wiki.bpe.op5000.d300.w2v.bin.tar.gz already downloaded (5.7 MB)\n",
      "Total number of unfiltered translations 1920209\n",
      "Filtered translations with length between (1, input=400/target=450) characters: 1864679\n"
     ]
    }
   ],
   "source": [
    "europarl = Europarl()\n",
    "download_and_extract_resources(fnames_and_urls=europarl.external_resources, dest_path=europarl.path)\n",
    "europarl.load_and_preprocess(max_input_length=MAX_INPUT_LENGTH, max_target_length=MAX_TARGET_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-07T20:59:58.251562Z",
     "start_time": "2018-07-07T20:59:57.688399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_texts</th>\n",
       "      <th>target_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resumption of the session</td>\n",
       "      <td>wiederaufnahme der sitzungsperiode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i declare resumed the session of the european ...</td>\n",
       "      <td>ich erkläre die am freitag, dem 0. dezember un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>although, as you will have seen, the dreaded '...</td>\n",
       "      <td>wie sie feststellen konnten, ist der gefürchte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you have requested a debate on this subject in...</td>\n",
       "      <td>im parlament besteht der wunsch nach einer aus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in the meantime, i should like to observe a mi...</td>\n",
       "      <td>heute möchte ich sie bitten - das ist auch der...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         input_texts  \\\n",
       "0                          resumption of the session   \n",
       "1  i declare resumed the session of the european ...   \n",
       "2  although, as you will have seen, the dreaded '...   \n",
       "3  you have requested a debate on this subject in...   \n",
       "4  in the meantime, i should like to observe a mi...   \n",
       "\n",
       "                                        target_texts  \n",
       "0                 wiederaufnahme der sitzungsperiode  \n",
       "1  ich erkläre die am freitag, dem 0. dezember un...  \n",
       "2  wie sie feststellen konnten, ist der gefürchte...  \n",
       "3  im parlament besteht der wunsch nach einer aus...  \n",
       "4  heute möchte ich sie bitten - das ist auch der...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl.df[['input_texts', 'target_texts']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-07T20:59:58.258164Z",
     "start_time": "2018-07-07T20:59:58.253989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English subwords ['▁this', '▁is', '▁a', '▁test', '▁for', '▁pre', 'tr', 'ained', '▁by', 'te', 'pa', 'ire', 'm', 'bed', 'd', 'ings']\n",
      "German subwords ['▁das', '▁ist', '▁ein', '▁test', '▁für', '▁v', 'ort', 'rain', 'ierte', '▁zeich', 'eng', 'ruppen']\n"
     ]
    }
   ],
   "source": [
    "print(\"English subwords\", europarl.bpe_input.sentencepiece.EncodeAsPieces(\"this is a test for pretrained bytepairembeddings\"))\n",
    "print(\"German subwords\", europarl.bpe_target.sentencepiece.EncodeAsPieces(\"das ist ein test für vortrainierte zeichengruppen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-07T20:59:59.021431Z",
     "start_time": "2018-07-07T20:59:58.260258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 171)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Those will be the inputs for the seq2seq model (that needs to know how long the sequences can get)\n",
    "max_len_input = europarl.df.input_sequences.apply(len).max()\n",
    "max_len_target = europarl.df.target_sequences.apply(len).max()\n",
    "(max_len_input, max_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-07T20:59:59.099714Z",
     "start_time": "2018-07-07T20:59:59.023748Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, val_ids = train_test_split(np.arange(europarl.df.shape[0]), test_size=0.1, random_state=RANDOM_STATE)  # fixed random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-07T20:59:59.115899Z",
     "start_time": "2018-07-07T20:59:59.101720Z"
    }
   },
   "outputs": [],
   "source": [
    "# copy+pasted from https://github.com/google/seq2seq/blob/7f485894d412e8d81ce0e07977831865e44309ce/seq2seq/contrib/rnn_cell.py#L39\n",
    "class ExtendedMultiRNNCell(tf.nn.rnn_cell.MultiRNNCell):\n",
    "    \"\"\"Extends the Tensorflow MultiRNNCell with residual connections\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cells,\n",
    "        residual_connections=False,\n",
    "        residual_combiner=\"add\",\n",
    "        residual_dense=False\n",
    "    ):\n",
    "        \"\"\"Create a RNN cell composed sequentially of a number of RNNCells.\n",
    "        Args:\n",
    "          cells: list of RNNCells that will be composed in this order.\n",
    "          state_is_tuple: If True, accepted and returned states are n-tuples, where\n",
    "            `n = len(cells)`.  If False, the states are all\n",
    "            concatenated along the column axis.  This latter behavior will soon be\n",
    "            deprecated.\n",
    "          residual_connections: If true, add residual connections between all cells.\n",
    "            This requires all cells to have the same output_size. Also, iff the\n",
    "            input size is not equal to the cell output size, a linear transform\n",
    "            is added before the first layer.\n",
    "          residual_combiner: One of \"add\" or \"concat\". To create inputs for layer\n",
    "            t+1 either \"add\" the inputs from the prev layer or concat them.\n",
    "          residual_dense: Densely connect each layer to all other layers\n",
    "        Raises:\n",
    "          ValueError: if cells is empty (not allowed), or at least one of the cells\n",
    "            returns a state tuple but the flag `state_is_tuple` is `False`.\n",
    "        \"\"\"\n",
    "        super(ExtendedMultiRNNCell, self).__init__(cells, state_is_tuple=True)\n",
    "        assert residual_combiner in [\"add\", \"concat\", \"mean\"]\n",
    "\n",
    "        self._residual_connections = residual_connections\n",
    "        self._residual_combiner = residual_combiner\n",
    "        self._residual_dense = residual_dense\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Run this multi-layer cell on inputs, starting from state.\"\"\"\n",
    "        if not self._residual_connections:\n",
    "            return super(ExtendedMultiRNNCell, self).__call__(\n",
    "                inputs, state, (scope or \"extended_multi_rnn_cell\")\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope(scope or \"extended_multi_rnn_cell\"):\n",
    "            # Adding Residual connections are only possible when input and output\n",
    "            # sizes are equal. Optionally transform the initial inputs to\n",
    "            # `cell[0].output_size`\n",
    "            if self._cells[0].output_size != inputs.get_shape().as_list()[1] and (self._residual_combiner in [\"add\", \"mean\"]):\n",
    "                inputs = tf.contrib.layers.fully_connected(\n",
    "                    inputs=inputs,\n",
    "                    num_outputs=self._cells[0].output_size,\n",
    "                    activation_fn=None,\n",
    "                    scope=\"input_transform\"\n",
    "                )\n",
    "\n",
    "            # Iterate through all layers (code from MultiRNNCell)\n",
    "            cur_inp = inputs\n",
    "            prev_inputs = [cur_inp]\n",
    "            new_states = []\n",
    "            for i, cell in enumerate(self._cells):\n",
    "                with tf.variable_scope(\"cell_%d\" % i):\n",
    "                    if not nest.is_sequence(state):\n",
    "                        raise ValueError(\n",
    "                            \"Expected state to be a tuple of length %d, but received: %s\" %\n",
    "                            (len(self.state_size), state)\n",
    "                        )\n",
    "                    cur_state = state[i]\n",
    "                    next_input, new_state = cell(cur_inp, cur_state)\n",
    "\n",
    "                    # Either combine all previous inputs or only the current input\n",
    "                    input_to_combine = prev_inputs[-1:]\n",
    "                    if self._residual_dense:\n",
    "                        input_to_combine = prev_inputs\n",
    "\n",
    "                    # Add Residual connection\n",
    "                    if self._residual_combiner == \"add\":\n",
    "                        next_input = next_input + sum(input_to_combine)\n",
    "                    if self._residual_combiner == \"mean\":\n",
    "                        combined_mean = tf.reduce_mean(tf.stack(input_to_combine), 0)\n",
    "                        next_input = next_input + combined_mean\n",
    "                    elif self._residual_combiner == \"concat\":\n",
    "                        next_input = tf.concat([next_input] + input_to_combine, 1)\n",
    "\n",
    "                    cur_inp = next_input\n",
    "                    prev_inputs.append(cur_inp)\n",
    "\n",
    "                    new_states.append(new_state)\n",
    "        new_states = (tuple(new_states) if self._state_is_tuple else array_ops.concat(new_states, 1))\n",
    "        return cur_inp, new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-07T20:59:59.149788Z",
     "start_time": "2018-07-07T20:59:59.117709Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copy+pasted from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/opt/python/training/weight_decay_optimizers.py\n",
    "# would need tensorflow >= 1.9.2, here I use 1.8\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import resource_variable_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.training import adam\n",
    "from tensorflow.python.training import momentum as momentum_opt\n",
    "from tensorflow.python.training import optimizer\n",
    "from tensorflow.python.util.tf_export import tf_export\n",
    "\n",
    "\n",
    "class DecoupledWeightDecayExtension(object):\n",
    "  \"\"\"This class allows to extend optimizers with decoupled weight decay.\n",
    "  It implements the decoupled weight decay described by Loshchilov & Hutter\n",
    "  (https://arxiv.org/pdf/1711.05101.pdf), in which the weight decay is\n",
    "  decoupled from the optimization steps w.r.t. to the loss function.\n",
    "  For SGD variants, this simplifies hyperparameter search since it decouples\n",
    "  the settings of weight decay and learning rate.\n",
    "  For adaptive gradient algorithms, it regularizes variables with large\n",
    "  gradients more than L2 regularization would, which was shown to yield better\n",
    "  training loss and generalization error in the paper above.\n",
    "  This class alone is not an optimizer but rather extends existing\n",
    "  optimizers with decoupled weight decay. We explicitly define the two examples\n",
    "  used in the above paper (SGDW and AdamW), but in general this can extend\n",
    "  any OptimizerX by using\n",
    "  `extend_with_weight_decay(OptimizerX, weight_decay=weight_decay)`.\n",
    "  In order for it to work, it must be the first class the Optimizer with\n",
    "  weight decay inherits from, e.g.\n",
    "  ```python\n",
    "  class AdamWOptimizer(DecoupledWeightDecayExtension, adam.AdamOptimizer):\n",
    "    def __init__(self, weight_decay, *args, **kwargs):\n",
    "      super(AdamWOptimizer, self).__init__(weight_decay, *args, **kwargs).\n",
    "  ```\n",
    "  Note that this extension decays weights BEFORE applying the update based\n",
    "  on the gradient, i.e. this extension only has the desired behaviour for\n",
    "  optimizers which do not depend on the value of'var' in the update step!\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, weight_decay, **kwargs):\n",
    "    \"\"\"Construct the extension class that adds weight decay to an optimizer.\n",
    "    Args:\n",
    "      weight_decay: A `Tensor` or a floating point value, the factor by which\n",
    "        a variable is decayed in the update step.\n",
    "      **kwargs: Optional list or tuple or set of `Variable` objects to\n",
    "        decay.\n",
    "    \"\"\"\n",
    "    self._decay_var_list = None  # is set in minimize or apply_gradients\n",
    "    self._weight_decay = weight_decay\n",
    "    # The tensors are initialized in call to _prepare\n",
    "    self._weight_decay_tensor = None\n",
    "    super(DecoupledWeightDecayExtension, self).__init__(**kwargs)\n",
    "\n",
    "  def minimize(self, loss, global_step=None, var_list=None,\n",
    "               gate_gradients=optimizer.Optimizer.GATE_OP,\n",
    "               aggregation_method=None, colocate_gradients_with_ops=False,\n",
    "               name=None, grad_loss=None, decay_var_list=None):\n",
    "    \"\"\"Add operations to minimize `loss` by updating `var_list` with decay.\n",
    "    This function is the same as Optimizer.minimize except that it allows to\n",
    "    specify the variables that should be decayed using decay_var_list.\n",
    "    If decay_var_list is None, all variables in var_list are decayed.\n",
    "    For more information see the documentation of Optimizer.minimize.\n",
    "    Args:\n",
    "      loss: A `Tensor` containing the value to minimize.\n",
    "      global_step: Optional `Variable` to increment by one after the\n",
    "        variables have been updated.\n",
    "      var_list: Optional list or tuple of `Variable` objects to update to\n",
    "        minimize `loss`.  Defaults to the list of variables collected in\n",
    "        the graph under the key `GraphKeys.TRAINABLE_VARIABLES`.\n",
    "      gate_gradients: How to gate the computation of gradients.  Can be\n",
    "        `GATE_NONE`, `GATE_OP`, or  `GATE_GRAPH`.\n",
    "      aggregation_method: Specifies the method used to combine gradient terms.\n",
    "        Valid values are defined in the class `AggregationMethod`.\n",
    "      colocate_gradients_with_ops: If True, try colocating gradients with\n",
    "        the corresponding op.\n",
    "      name: Optional name for the returned operation.\n",
    "      grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
    "      decay_var_list: Optional list of decay variables.\n",
    "    Returns:\n",
    "      An Operation that updates the variables in `var_list`.  If `global_step`\n",
    "      was not `None`, that operation also increments `global_step`.\n",
    "    \"\"\"\n",
    "    self._decay_var_list = set(decay_var_list) if decay_var_list else False\n",
    "    return super(DecoupledWeightDecayExtension, self).minimize(\n",
    "        loss, global_step=global_step, var_list=var_list,\n",
    "        gate_gradients=gate_gradients, aggregation_method=aggregation_method,\n",
    "        colocate_gradients_with_ops=colocate_gradients_with_ops, name=name,\n",
    "        grad_loss=grad_loss)\n",
    "\n",
    "  def apply_gradients(self, grads_and_vars, global_step=None, name=None,\n",
    "                      decay_var_list=None):\n",
    "    \"\"\"Apply gradients to variables and decay the variables.\n",
    "    This function is the same as Optimizer.apply_gradients except that it\n",
    "    allows to specify the variables that should be decayed using\n",
    "    decay_var_list. If decay_var_list is None, all variables in var_list\n",
    "    are decayed.\n",
    "    For more information see the documentation of Optimizer.apply_gradients.\n",
    "    Args:\n",
    "      grads_and_vars: List of (gradient, variable) pairs as returned by\n",
    "        `compute_gradients()`.\n",
    "      global_step: Optional `Variable` to increment by one after the\n",
    "        variables have been updated.\n",
    "      name: Optional name for the returned operation.  Default to the\n",
    "        name passed to the `Optimizer` constructor.\n",
    "      decay_var_list: Optional list of decay variables.\n",
    "    Returns:\n",
    "      An `Operation` that applies the specified gradients. If `global_step`\n",
    "      was not None, that operation also increments `global_step`.\n",
    "    \"\"\"\n",
    "    self._decay_var_list = set(decay_var_list) if decay_var_list else False\n",
    "    return super(DecoupledWeightDecayExtension, self).apply_gradients(\n",
    "        grads_and_vars, global_step=global_step, name=name)\n",
    "\n",
    "  def _prepare(self):\n",
    "    weight_decay = self._weight_decay\n",
    "    if callable(weight_decay):\n",
    "      weight_decay = weight_decay()\n",
    "    self._weight_decay_tensor = ops.convert_to_tensor(\n",
    "        weight_decay, name=\"weight_decay\")\n",
    "    # Call the optimizers _prepare function.\n",
    "    super(DecoupledWeightDecayExtension, self)._prepare()\n",
    "\n",
    "  def _decay_weights_op(self, var):\n",
    "    if not self._decay_var_list or var in self._decay_var_list:\n",
    "      return var.assign_sub(self._weight_decay * var, self._use_locking)\n",
    "    return control_flow_ops.no_op()\n",
    "\n",
    "  def _decay_weights_sparse_op(self, var, indices, scatter_add):\n",
    "    if not self._decay_var_list or var in self._decay_var_list:\n",
    "      return scatter_add(var, indices, -self._weight_decay * var,\n",
    "                         self._use_locking)\n",
    "    return control_flow_ops.no_op()\n",
    "\n",
    "  # Here, we overwrite the apply functions that the base optimizer calls.\n",
    "  # super().apply_x resolves to the apply_x function of the BaseOptimizer.\n",
    "  def _apply_dense(self, grad, var):\n",
    "    with ops.control_dependencies([self._decay_weights_op(var)]):\n",
    "      return super(DecoupledWeightDecayExtension, self)._apply_dense(grad, var)\n",
    "\n",
    "  def _resource_apply_dense(self, grad, var):\n",
    "    with ops.control_dependencies([self._decay_weights_op(var)]):\n",
    "      return super(DecoupledWeightDecayExtension, self)._resource_apply_dense(\n",
    "          grad, var)\n",
    "\n",
    "  def _apply_sparse(self, grad, var):\n",
    "    scatter_add = state_ops.scatter_add\n",
    "    decay_op = self._decay_weights_sparse_op(var, grad.indices, scatter_add)\n",
    "    with ops.control_dependencies([decay_op]):\n",
    "      return super(DecoupledWeightDecayExtension, self)._apply_sparse(\n",
    "          grad, var)\n",
    "\n",
    "  def _resource_scatter_add(self, x, i, v, _=None):\n",
    "    # last argument allows for one overflow argument, to have the same function\n",
    "    # signature as state_ops.scatter_add\n",
    "    with ops.control_dependencies(\n",
    "        [resource_variable_ops.resource_scatter_add(x.handle, i, v)]):\n",
    "      return x.value()\n",
    "\n",
    "  def _resource_apply_sparse(self, grad, var, indices):\n",
    "    scatter_add = self._resource_scatter_add\n",
    "    decay_op = self._decay_weights_sparse_op(var, indices, scatter_add)\n",
    "    with ops.control_dependencies([decay_op]):\n",
    "      return super(DecoupledWeightDecayExtension, self)._resource_apply_sparse(\n",
    "          grad, var, indices)\n",
    "\n",
    "\n",
    "def extend_with_decoupled_weight_decay(base_optimizer):\n",
    "  \"\"\"Factory function returning an optimizer class with decoupled weight decay.\n",
    "  Returns an optimizer class. An instance of the returned class computes the\n",
    "  update step of `base_optimizer` and additionally decays the weights.\n",
    "  E.g., the class returned by\n",
    "  `extend_with_decoupled_weight_decay(tf.train.AdamOptimizer)` is equivalent to\n",
    "  `tf.contrib.opt.AdamWOptimizer`.\n",
    "  The API of the new optimizer class slightly differs from the API of the\n",
    "  base optimizer:\n",
    "  - The first argument to the constructor is the weight decay rate.\n",
    "  - `minimize` and `apply_gradients` accept the optional keyword argument\n",
    "    `decay_var_list`, which specifies the variables that should be decayed.\n",
    "    If `None`, all variables that are optimized are decayed.\n",
    "  Usage example:\n",
    "  ```python\n",
    "  # MyAdamW is a new class\n",
    "  MyAdamW = extend_with_decoupled_weight_decay(tf.train.AdamOptimizer)\n",
    "  # Create a MyAdamW object\n",
    "  optimizer = MyAdamW(weight_decay=0.001, learning_rate=0.001)\n",
    "  sess.run(optimizer.minimize(loss, decay_variables=[var1, var2]))\n",
    "  Note that this extension decays weights BEFORE applying the update based\n",
    "  on the gradient, i.e. this extension only has the desired behaviour for\n",
    "  optimizers which do not depend on the value of'var' in the update step!\n",
    "  ```\n",
    "  Args:\n",
    "    base_optimizer: An optimizer class that inherits from tf.train.Optimizer.\n",
    "  Returns:\n",
    "    A new optimizer class that inherits from DecoupledWeightDecayExtension\n",
    "    and base_optimizer.\n",
    "  \"\"\"\n",
    "\n",
    "  class OptimizerWithDecoupledWeightDecay(DecoupledWeightDecayExtension,\n",
    "                                          base_optimizer):\n",
    "    \"\"\"Base_optimizer with decoupled weight decay.\n",
    "    This class computes the update step of `base_optimizer` and\n",
    "    additionally decays the variable with the weight decay being decoupled from\n",
    "    the optimization steps w.r.t. to the loss function, as described by\n",
    "    Loshchilov & Hutter (https://arxiv.org/pdf/1711.05101.pdf).\n",
    "    For SGD variants, this simplifies hyperparameter search since\n",
    "    it decouples the settings of weight decay and learning rate.\n",
    "    For adaptive gradient algorithms, it regularizes variables with large\n",
    "    gradients more than L2 regularization would, which was shown to yield\n",
    "    better training loss and generalization error in the paper above.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight_decay, *args, **kwargs):\n",
    "      # super delegation is necessary here\n",
    "      # pylint: disable=useless-super-delegation\n",
    "      super(OptimizerWithDecoupledWeightDecay, self).__init__(\n",
    "          weight_decay, *args, **kwargs)\n",
    "      # pylint: enable=useless-super-delegation\n",
    "\n",
    "  return OptimizerWithDecoupledWeightDecay\n",
    "\n",
    "\n",
    "@tf_export(\"contrib.opt.MomentumWOptimizer\")\n",
    "class MomentumWOptimizer(DecoupledWeightDecayExtension,\n",
    "                         momentum_opt.MomentumOptimizer):\n",
    "  \"\"\"Optimizer that implements the Momentum algorithm with weight_decay.\n",
    "  This is an implementation of the SGDW optimizer described in \"Fixing\n",
    "  Weight Decay Regularization in Adam\" by Loshchilov & Hutter\n",
    "  (https://arxiv.org/abs/1711.05101)\n",
    "  ([pdf])(https://arxiv.org/pdf/1711.05101.pdf).\n",
    "  It computes the update step of `train.MomentumOptimizer` and additionally\n",
    "  decays the variable. Note that this is different from adding\n",
    "  L2 regularization on the variables to the loss. Decoupling the weight decay\n",
    "  from other hyperparameters (in particular the learning rate) simplifies\n",
    "  hyperparameter search.\n",
    "  For further information see the documentation of the Momentum Optimizer.\n",
    "  Note that this optimizer can also be instantiated as\n",
    "  ```python\n",
    "  extend_with_weight_decay(tf.train.MomentumOptimizer,\n",
    "                           weight_decay=weight_decay)\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, weight_decay, learning_rate, momentum,\n",
    "               use_locking=False, name=\"MomentumW\", use_nesterov=False):\n",
    "    \"\"\"Construct a new MomentumW optimizer.\n",
    "    For further information see the documentation of the Momentum Optimizer.\n",
    "    Args:\n",
    "      weight_decay:  A `Tensor` or a floating point value.  The weight decay.\n",
    "      learning_rate: A `Tensor` or a floating point value.  The learning rate.\n",
    "      momentum: A `Tensor` or a floating point value.  The momentum.\n",
    "      use_locking: If `True` use locks for update operations.\n",
    "      name: Optional name prefix for the operations created when applying\n",
    "        gradients.  Defaults to \"Momentum\".\n",
    "      use_nesterov: If `True` use Nesterov Momentum.\n",
    "        See [Sutskever et al., 2013](\n",
    "        http://jmlr.org/proceedings/papers/v28/sutskever13.pdf).\n",
    "        This implementation always computes gradients at the value of the\n",
    "        variable(s) passed to the optimizer. Using Nesterov Momentum makes the\n",
    "        variable(s) track the values called `theta_t + mu*v_t` in the paper.\n",
    "    @compatibility(eager)\n",
    "    When eager execution is enabled, learning_rate, weight_decay and momentum\n",
    "    can each be a callable that takes no arguments and returns the actual value\n",
    "    to use. This can be useful for changing these values across different\n",
    "    invocations of optimizer functions.\n",
    "    @end_compatibility\n",
    "    \"\"\"\n",
    "    super(MomentumWOptimizer, self).__init__(\n",
    "        weight_decay, learning_rate=learning_rate, momentum=momentum,\n",
    "        use_locking=use_locking, name=name, use_nesterov=use_nesterov)\n",
    "\n",
    "\n",
    "# @tf_export(\"contrib.opt.AdamWOptimizer\")\n",
    "class AdamWOptimizer(DecoupledWeightDecayExtension, adam.AdamOptimizer):\n",
    "  \"\"\"Optimizer that implements the Adam algorithm with weight decay.\n",
    "  This is an implementation of the AdamW optimizer described in \"Fixing\n",
    "  Weight Decay Regularization in Adam\" by Loshchilov & Hutter\n",
    "  (https://arxiv.org/abs/1711.05101)\n",
    "  ([pdf])(https://arxiv.org/pdf/1711.05101.pdf).\n",
    "  It computes the update step of `train.AdamOptimizer` and additionally decays\n",
    "  the variable. Note that this is different from adding L2 regularization on\n",
    "  the variables to the loss: it regularizes variables with large\n",
    "  gradients more than L2 regularization would, which was shown to yield better\n",
    "  training loss and generalization error in the paper above.\n",
    "  For further information see the documentation of the Adam Optimizer.\n",
    "  Note that this optimizer can also be instantiated as\n",
    "  ```python\n",
    "  extend_with_weight_decay(tf.train.AdamOptimizer, weight_decay=weight_decay)\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, weight_decay, learning_rate=0.001, beta1=0.9, beta2=0.999,\n",
    "               epsilon=1e-8, use_locking=False, name=\"AdamW\"):\n",
    "    \"\"\"Construct a new AdamW optimizer.\n",
    "    For further information see the documentation of the Adam Optimizer.\n",
    "    Args:\n",
    "      weight_decay:  A `Tensor` or a floating point value.  The weight decay.\n",
    "      learning_rate: A Tensor or a floating point value.  The learning rate.\n",
    "      beta1: A float value or a constant float tensor.\n",
    "        The exponential decay rate for the 1st moment estimates.\n",
    "      beta2: A float value or a constant float tensor.\n",
    "        The exponential decay rate for the 2nd moment estimates.\n",
    "      epsilon: A small constant for numerical stability. This epsilon is\n",
    "        \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
    "        Section 2.1), not the epsilon in Algorithm 1 of the paper.\n",
    "      use_locking: If True use locks for update operations.\n",
    "      name: Optional name for the operations created when applying gradients.\n",
    "        Defaults to \"Adam\".\n",
    "    \"\"\"\n",
    "    super(AdamWOptimizer, self).__init__(\n",
    "        weight_decay, learning_rate=learning_rate, beta1=beta1, beta2=beta2,\n",
    "epsilon=epsilon, use_locking=use_locking, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-07T21:00:08.864337Z",
     "start_time": "2018-07-07T20:59:59.151425Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    encoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_input\n",
    "        dtype=tf.int32,\n",
    "        name='encoder_inputs' \n",
    "    )\n",
    "    batch_size = tf.shape(encoder_inputs)[0]\n",
    "    beam_width = tf.placeholder_with_default(1, shape=[])\n",
    "    dropout = tf.placeholder_with_default(tf.cast(0.0, tf.float32), shape=[])\n",
    "    keep_prob = tf.cast(1.0, tf.float32) - dropout\n",
    "    learning_rate = tf.placeholder_with_default(tf.cast(1e-3, tf.float32), shape=[])\n",
    "\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", \n",
    "        initializer=tf.constant(europarl.bpe_input.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder,\n",
    "        encoder_inputs,\n",
    "        name=\"encoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    input_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='input_sequence_length'\n",
    "    )\n",
    "    \n",
    "    rnn_cell_type = tf.nn.rnn_cell.GRUCell\n",
    "    encoder_forward_cells = [\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name=f'encoder_forward_cell{layer}') \n",
    "        for layer in range(LAYERS)\n",
    "    ]\n",
    "    encoder_backward_cells = [\n",
    "        rnn_cell_type(num_units=LATENT_DIM // 2, name=f'encoder_backward_cell{layer}') \n",
    "        for layer in range(LAYERS)\n",
    "    ]\n",
    "    encoder_forward_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        cell,\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    ) for cell in encoder_forward_cells]\n",
    "    encoder_backward_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        cell,\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    ) for cell in encoder_backward_cells]\n",
    "    encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "        cells_fw = ExtendedMultiRNNCell(encoder_forward_cells, residual_connections=True)._cells,\n",
    "        cells_bw = ExtendedMultiRNNCell(encoder_backward_cells, residual_connections=True)._cells,\n",
    "        inputs=encoder_emb_inp,\n",
    "        sequence_length=input_sequence_length,\n",
    "        time_major=False,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    encoder_state = tf.concat([encoder_state_fw[-1], encoder_state_bw[-1]], -1)\n",
    "    \n",
    "    # Regarding time_major:\n",
    "    # If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "    # If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "    # Using `time_major = True` is a bit more efficient because it avoids\n",
    "    # transposes at the beginning and end of the RNN calculation.  However,\n",
    "    # most TensorFlow data is batch-major, so by default this function\n",
    "    # accepts input and emits output in batch-major form.\n",
    "    #\n",
    "    # for simplicity I work with batch major here instead of time_major\n",
    "    # so I don't need to transpose inputs and transpose back for attention mechanism\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_inputs' \n",
    "    )\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", \n",
    "        initializer=tf.constant(europarl.bpe_target.embedding_matrix),\n",
    "        trainable=EMBEDDING_TRAINABLE,\n",
    "    )\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder,\n",
    "        decoder_inputs,\n",
    "        name=\"decoder_emb_inp\"\n",
    "    )\n",
    "    \n",
    "    target_sequence_length = tf.placeholder(\n",
    "        shape=(None, ),\n",
    "        dtype=tf.int32,\n",
    "        name='target_sequence_length'\n",
    "    )\n",
    "    \n",
    "    # tiling is necessary to work with BeamSearchDecoder\n",
    "    # read carefully the NOTE on constructor in\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper \n",
    "    tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(encoder_outputs, multiplier=beam_width)\n",
    "    tiled_encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width)\n",
    "    tiled_sequence_length = tf.contrib.seq2seq.tile_batch(input_sequence_length, multiplier=beam_width)\n",
    "    \n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        LATENT_DIM,\n",
    "        memory=tiled_encoder_outputs,\n",
    "        memory_sequence_length=tiled_sequence_length,\n",
    "        dtype=tf.float32,\n",
    "        name='attention_mechanism',\n",
    "    )\n",
    "    decoder_rnn_cells = [rnn_cell_type(num_units=LATENT_DIM, name=f'decoder_cell{layer}') for layer in range(LAYERS)]\n",
    "    def residual_fn(inputs, outputs):\n",
    "        tf.contrib.framework.nest.assert_same_structure(inputs, outputs)\n",
    "        inputs_without_attention = tf.slice(inputs, [0, 0], [batch_size, LATENT_DIM])\n",
    "        return tf.contrib.framework.nest.map_structure(lambda inp, out: inp + out, inputs_without_attention, outputs) \n",
    "    for layer in range(1, LAYERS):\n",
    "        decoder_rnn_cells[layer] = tf.contrib.rnn.ResidualWrapper(\n",
    "            decoder_rnn_cells[layer],\n",
    "            residual_fn=residual_fn\n",
    "        )\n",
    "    decoder_rnn_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "        cell,\n",
    "        input_keep_prob=keep_prob,\n",
    "        output_keep_prob=keep_prob,\n",
    "        dtype=tf.float32,\n",
    "    ) for cell in decoder_rnn_cells]\n",
    "    attention_cells = [tf.contrib.seq2seq.AttentionWrapper(\n",
    "        cell,\n",
    "        attention_mechanism,\n",
    "        attention_layer_size=LATENT_DIM,\n",
    "        name=f'attention_wrapper{layer}',\n",
    "    ) for layer, cell in enumerate(decoder_rnn_cells)] \n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell(attention_cells)\n",
    "\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        inputs=decoder_emb_inp, \n",
    "        sequence_length=target_sequence_length,\n",
    "        time_major=False,\n",
    "        name=\"decoder_training_helper\",\n",
    "    )\n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        units=len(europarl.bpe_target.tokens),\n",
    "        use_bias=False,\n",
    "        name='projection_layer',\n",
    "    )\n",
    "    \n",
    "    initial_state = tuple(\n",
    "        attention_cells[0].zero_state(dtype=tf.float32, batch_size=batch_size).clone(\n",
    "            cell_state=encoder_state\n",
    "        )\n",
    "        for _ in range(LAYERS)\n",
    "    )\n",
    "\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=decoder_cell,\n",
    "        helper=training_helper,\n",
    "        initial_state=initial_state,\n",
    "        output_layer=projection_layer,\n",
    "    )\n",
    "    outputs, _final_state, _final_sequence_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder,\n",
    "        output_time_major=False,\n",
    "        impute_finished=False,\n",
    "    )\n",
    "    logits = outputs.rnn_output\n",
    "    \n",
    "    decoder_outputs = tf.placeholder(\n",
    "        shape=(None, None),  # batch_size x max_len_target\n",
    "        dtype=tf.int32,\n",
    "        name='decoder_outputs',\n",
    "    )\n",
    "    target_weights = tf.cast(tf.sequence_mask(target_sequence_length), dtype=tf.float32)\n",
    "    weight_decay = tf.placeholder_with_default(tf.cast(1e-5, tf.float32), shape=[])\n",
    "    train_loss = tf.contrib.seq2seq.sequence_loss(logits, decoder_outputs, target_weights) # + l2_lambda * loss_l2\n",
    "    params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, params)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "        t_list=gradients,\n",
    "        clip_norm=1.,\n",
    "    )\n",
    "    \n",
    "    params_without_embeddings = [v for v in tf.trainable_variables() if not re.match(r'embedding_(de|en)coder', v.name)]\n",
    "    \n",
    "    optimizer = AdamWOptimizer(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    update_step = optimizer.apply_gradients(zip(clipped_gradients, params), decay_var_list=params_without_embeddings)\n",
    "    \n",
    "    gradients_without_embeddings = tf.gradients(train_loss, params_without_embeddings)\n",
    "    clipped_gradients_without_embeddings, _ = tf.clip_by_global_norm(\n",
    "        t_list=gradients_without_embeddings,\n",
    "        clip_norm=1.,\n",
    "    )\n",
    "    update_step_without_embeddings = optimizer.apply_gradients(zip(clipped_gradients_without_embeddings, params_without_embeddings))\n",
    "    \n",
    "    inference_decoder_initial_state = tuple(\n",
    "        attention_cells[0].zero_state(\n",
    "            dtype=tf.float32,\n",
    "            batch_size=batch_size * beam_width  # tricky and somehow unintuitive, but necessary\n",
    "        ).clone(\n",
    "            cell_state=tiled_encoder_state\n",
    "        ) for _ in range(LAYERS)\n",
    "    )\n",
    "    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "        cell=decoder_cell,\n",
    "        embedding=embedding_decoder,\n",
    "        start_tokens=tf.fill([batch_size], europarl.bpe_target.start_token_idx),\n",
    "        end_token=europarl.bpe_target.stop_token_idx,\n",
    "        initial_state=inference_decoder_initial_state,\n",
    "        beam_width=BEAM_WIDTH,\n",
    "        output_layer=projection_layer,\n",
    "        length_penalty_weight=1.0,  # https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/\n",
    "    )\n",
    "\n",
    "    \n",
    "    inference_outputs, _inference_final_state, _inference_final_sequence_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inference_decoder,\n",
    "        maximum_iterations=tf.round(tf.reduce_max(input_sequence_length) * 2),  # a bit more flexible than max_len_target\n",
    "        impute_finished=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-07T21:00:08.876992Z",
     "start_time": "2018-07-07T21:00:08.865667Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_train_batch(batch_ids, epoch):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    base_lr = 1e-3\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        lr = base_lr * (0.5 + epoch / (2 * WARMUP_EPOCHS))\n",
    "    elif epoch < LEARNING_RATE_DECAY_START_EPOCH:\n",
    "        lr = base_lr\n",
    "    else:\n",
    "        lr = base_lr * (LEARNING_RATE_DECAY_RATE ** (epoch - LEARNING_RATE_DECAY_START_EPOCH))\n",
    "    pred, loss, _ = sess.run(\n",
    "        fetches=[\n",
    "            outputs, train_loss, update_step if epoch >= WARMUP_EPOCHS else update_step_without_embeddings\n",
    "        ],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "            dropout: DROPOUT,\n",
    "            # embedding_trainable: epoch >= WARMUP_EPOCHS,\n",
    "            learning_rate: lr\n",
    "        }\n",
    "    )\n",
    "    return loss, lr\n",
    "\n",
    "def run_val_batch(batch_ids):\n",
    "    batch_input_sequences = europarl.df.input_sequences.iloc[batch_ids]\n",
    "    batch_input_lengths = batch_input_sequences.apply(len)\n",
    "    batch_target_sequences = europarl.df.target_sequences.iloc[batch_ids]\n",
    "    batch_target_lengths = batch_target_sequences.apply(len) - 1\n",
    "\n",
    "    batch_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_input_sequences,\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    batch_target_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        batch_target_sequences,\n",
    "        maxlen=max_len_target,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    loss = sess.run(\n",
    "        fetches=[train_loss],\n",
    "        feed_dict={\n",
    "            encoder_inputs: batch_input_padded,\n",
    "            input_sequence_length: np.array(batch_input_lengths),\n",
    "            decoder_inputs: batch_target_padded[:, :batch_target_lengths.max()],\n",
    "            target_sequence_length: np.array(batch_target_lengths),\n",
    "            decoder_outputs: batch_target_padded[:, 1:batch_target_lengths.max() + 1],\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def run_validation_loss():\n",
    "    return np.mean([\n",
    "        run_val_batch(ids)\n",
    "        for ids \n",
    "        in np.array_split(val_ids, np.ceil(len(val_ids) / BATCH_SIZE))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-07T21:00:08.884596Z",
     "start_time": "2018-07-07T21:00:08.878102Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    sequenced = europarl.bpe_input.subword_indices(preprocess(sentence))\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [sequenced],\n",
    "        maxlen=max_len_input,\n",
    "        dtype=int,\n",
    "        padding='post'\n",
    "    )\n",
    "    \n",
    "    beam_search_output = sess.run(\n",
    "        fetches=[inference_outputs],\n",
    "        feed_dict={\n",
    "            encoder_inputs: padded,\n",
    "            input_sequence_length: [len(sequenced)],\n",
    "            beam_width: BEAM_WIDTH,\n",
    "        }\n",
    "    )[0]\n",
    "    \n",
    "    return europarl.bpe_target.sentencepiece.DecodePieces([\n",
    "        europarl.bpe_target.tokens[idx] for idx in beam_search_output.predicted_ids[0, :, 0].tolist()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-11T14:02:32.081307Z",
     "start_time": "2018-07-07T21:00:08.885704Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc464afb1435477193e4a2da333e5ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0005, train_loss=3.51415, val_loss=2.52636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfef30042d7a41d690a53ff2ab4342b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0006, train_loss=2.81383, val_loss=2.26058\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7b9e3ed9ba4641b0d8bf83059bc731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0007, train_loss=2.6593, val_loss=2.15732\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16275c7c773841dab05ab6e06a3e6aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0008, train_loss=2.58927, val_loss=2.11433\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b2cac7f78d450d95c6870ee7090e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0009, train_loss=2.5511, val_loss=2.08338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aeeb52f08974f19b6a1409325f2e3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.14742867793506825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1786f0314e4cde9b52c84f7d652062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.24391, val_loss=1.80562\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e52bd37eef04cd2ace688991c75cf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.1181, val_loss=1.75325\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd31ff56fd341ccb2245922d3b8efa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 8', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.07112, val_loss=1.71901\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163c5e2dd6dd459fb35200cd6ba42ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 9', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.04338, val_loss=1.70204\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0986de69c96b4225bf46bd07087e2d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 10', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.02408, val_loss=1.68641\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3579d2c2144bee8a16e11fb576e415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.19216522202581007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97ef5ab2f554ee8937243c1dd3eff05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 11', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.001, train_loss=2.01039, val_loss=1.67712\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3161a24e7e224b07b7e4ae5ed72a6dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 12', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.00097, train_loss=1.99403, val_loss=1.66369\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891adb1a87394285a2037fdf19c15463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 13', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.0009409, train_loss=1.98071, val_loss=1.65155\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5d51af1a954c29b89fad6e9e216bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 14', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000912673, train_loss=1.96839, val_loss=1.6437\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f532db11794fb584537e7d3f227744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 15', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000885293, train_loss=1.95737, val_loss=1.63398\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2177baf50e7849b488d12a012bc03d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.20081519462137531\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b6ef27668f4ee9a235eaaa9b41f390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 16', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000858734, train_loss=1.94778, val_loss=1.62813\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fddaeb870042e4b1f6530ef48777aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 17', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000832972, train_loss=1.93884, val_loss=1.62212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22996be28cce4a4aae077213ca33ae8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 18', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000807983, train_loss=1.93075, val_loss=1.61687\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f6134442af4a7ebfe0aa36c0890fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 19', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000783743, train_loss=1.92314, val_loss=1.60873\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aeb3b45ccc24f18af9e203dfafa2d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 20', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000760231, train_loss=1.9161, val_loss=1.60678\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b977ae874d40c9838eb5606c31aaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.19941596977259465\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d7058ea776490dae4f87cce3c3029b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 21', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000737424, train_loss=1.90963, val_loss=1.60114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b218995b943a49338130986afb0de363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 22', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000715301, train_loss=1.9037, val_loss=1.59932\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc83cfe436c4d68b223015f6cd58e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 23', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000693842, train_loss=1.89771, val_loss=1.58867\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b304bf05be4c10bdbc3b5abfeb090f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 24', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000673027, train_loss=1.89226, val_loss=1.58643\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58ce869ebb643a0b3e314ac3ade0daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 25', max=13112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate=0.000652836, train_loss=1.88718, val_loss=1.58144\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(\n",
    "    allow_soft_placement=True,  # needed as recommendation from https://github.com/tensorflow/tensorflow/issues/2292\n",
    "    log_device_placement=True,\n",
    ")\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batches_per_epoch = np.ceil(len(train_ids) / BATCH_SIZE)\n",
    "for epoch in range(EPOCHS):\n",
    "    shuffled_ids = np.random.permutation(train_ids)\n",
    "    batch_splits = np.array_split(shuffled_ids, batches_per_epoch)\n",
    "    train_losses = []\n",
    "    N = len(batch_splits)\n",
    "    with tqdm(batch_splits, desc=f\"Epoch {epoch+1}\") as t:\n",
    "        for train_batch_ids in t:\n",
    "            batch_loss, lr = run_train_batch(train_batch_ids, epoch=epoch)\n",
    "            train_losses.append(batch_loss)\n",
    "            t.set_postfix(train_loss=np.mean(train_losses))\n",
    "        print(f\"learning rate={lr:.6}, train_loss={np.mean(train_losses):.6}, val_loss={run_validation_loss():.6}\")\n",
    "    \n",
    "    if epoch > 0 and ((epoch+1) % 5 == 0) and (epoch+1 < EPOCHS):\n",
    "        bleu = bleu_scores_europarl(\n",
    "            input_texts=europarl.df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "            target_texts=europarl.df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "            predict=lambda text: predict(text)\n",
    "        )\n",
    "        print(f'average BLEU on test set = {bleu.mean()}')\n",
    "        \n",
    "validation_input_sequences = europarl.df.input_sequences.iloc[val_ids[:BATCH_SIZE]]\n",
    "validation_input_lengths = validation_input_sequences.apply(len)\n",
    "\n",
    "validation_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    validation_input_sequences,\n",
    "    maxlen=max_len_input,\n",
    "    dtype=int,\n",
    "    padding='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-11T14:02:33.629365Z",
     "start_time": "2018-07-11T14:02:32.083041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/tfattentionmodel_2layers_with_weight_decay.ckpt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = f'tfattentionmodel_{LAYERS}layers_with_weight_decay'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, f\"data/{name}.ckpt\")\n",
    "# tfattentionmodel_2layers.ckpt.index https://drive.google.com/open?id=1t5f7vbI6sdBqlTJ3DguUnJwKjx2NInC4 \n",
    "# tfattentionmodel_2layers.cpkt.meta https://drive.google.com/open?id=1Ikp266cw7c93S6mCYHkE0SaBfI1WZtmF \n",
    "# tfattentionmodel_2layers.cpkt.data-00000-of-00001 https://drive.google.com/open?id=1QMT_5nA7dOHe5G8FdCOY0MDvwh5b5_-A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-11T14:02:34.138081Z",
     "start_time": "2018-07-11T14:02:33.631645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello.' --> 'helfen.'\n",
      "'you are welcome.' --> 'sie sind begrüßenswert.'\n",
      "'how do you do?' --> 'wie tun sie?'\n",
      "'i hate mondays.' --> 'ich habe gesprochen.'\n",
      "'i am a programmer.' --> 'ich bin ein programm.'\n",
      "'data is the new oil.' --> 'die daten sind das neue öl.'\n",
      "'it could be worse.' --> 'es könnte schlimmer sein.'\n",
      "'i am on top of it.' --> 'ich bin ganz wichtig.'\n",
      "'n° uno' --> 'nio-oo'\n",
      "'awesome!' --> 'einwände!'\n",
      "'put your feet up!' --> 'lassen sie mich die füße sein!'\n",
      "'from the start till the end!' --> 'aus dem beginn des endes!'\n",
      "'from dusk till dawn.' --> 'aus der dusus till-zulassung.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on some examples:\n",
    "EXAMPLES = [\n",
    "    'Hello.',\n",
    "    'You are welcome.',\n",
    "    'How do you do?',\n",
    "    'I hate mondays.',\n",
    "    'I am a programmer.',\n",
    "    'Data is the new oil.',\n",
    "    'It could be worse.',\n",
    "    \"I am on top of it.\",\n",
    "    \"N° Uno\",\n",
    "    \"Awesome!\",\n",
    "    \"Put your feet up!\",\n",
    "    \"From the start till the end!\",\n",
    "    \"From dusk till dawn.\",\n",
    "]\n",
    "for en in [sentence + '\\n' for sentence in EXAMPLES]:\n",
    "    print(f\"{preprocess(en)!r} --> {predict(en)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-11T14:02:37.180921Z",
     "start_time": "2018-07-11T14:02:34.139377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'i declare resumed the session of the european parliament adjourned on friday 0 december 0, and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.', got 'ich erkläre die sitzungsperiode des europäischen parlaments, der am freitag am freitag am 0. dezember 0 unterbrochen wird, und ich möchte ihnen noch einmal wünschen, dass sie in der hoffnung, dass sie einen freudefähigen zeitraum haben, ein glückliches neues jahr ermöglicht.', exp: 'ich erkläre die am freitag, dem 0. dezember unterbrochene sitzungsperiode des europäischen parlaments für wiederaufgenommen, wünsche ihnen nochmals alles gute zum jahreswechsel und hoffe, daß sie schöne ferien hatten.'\n",
      "Original \"although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\", got 'obwohl sie sehen, daß das schreckliche \"millium-misszettel\" gescheitert ist, hat die bevölkerung in einigen ländern eine reihe von naturkatastrophen leidet, die wirklich schrecklich waren.', exp: 'wie sie feststellen konnten, ist der gefürchtete \"millenium-bug \" nicht eingetreten. doch sind bürger einiger unserer mitgliedstaaten opfer von schrecklichen naturkatastrophen geworden.'\n",
      "Original 'you have requested a debate on this subject in the course of the next few days, during this part-session.', got 'sie haben eine aussprache über dieses thema in den nächsten tagen in dieser sitzung gefordert.', exp: 'im parlament besteht der wunsch nach einer aussprache im verlauf dieser sitzungsperiode in den nächsten tagen.'\n",
      "Original \"in the meantime, i should like to observe a minute' s silence, as a number of members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the european union.\", got 'inzwischen möchte ich eine schweigeminute feststellen, da einige abgeordnete im namen aller opfer, insbesondere denen der schrecklichen stakeholder, in den verschiedenen ländern der europäischen union gefordert wurden.', exp: 'heute möchte ich sie bitten - das ist auch der wunsch einiger kolleginnen und kollegen -, allen opfern der stürme, insbesondere in den verschiedenen ländern der europäischen union, in einer schweigeminute zu gedenken.'\n",
      "Original \"please rise, then, for this minute' s silence.\", got 'bitte bitte ich das schweigen für diese schweigeminute.', exp: 'ich bitte sie, sich zu einer schweigeminute zu erheben.'\n",
      "Original \"(the house rose and observed a minute' s silence)\", got '(das parlament erhebt sich auf eine schweiute.)', exp: '(das parlament erhebt sich zu einer schweigeminute.)'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin! zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka.', got 'sie werden von der presse und fernsehen bekannt sein, dass es eine reihe von bombenanschlägen und töten in sri lanka gibt.', exp: 'wie sie sicher aus der presse und dem fernsehen wissen, gab es in sri lanka mehrere bombenexplosionen mit zahlreichen toten.'\n",
      "Original 'one of the people assassinated very recently in sri lanka was mr kumar ponnambalam, who had visited the european parliament just a few months ago.', got 'eines der menschen, die in sri lanka in jüngster zeit in sri lanka beschlossen wurden, war herr kumaramamamam, der das europäische parlament vor wenigen monaten besucht hatte.', exp: 'zu den attentatsopfern, die es in jüngster zeit in sri lanka zu beklagen gab, zählt auch herr kumar ponnambalam, der dem europäischen parlament erst vor wenigen monaten einen besuch abgestattet hatte.'\n",
      "Original \"would it be appropriate for you, madam president, to write a letter to the sri lankan president expressing parliament's regret at his and the other violent deaths in sri lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\", got 'wäre es für sie angemessen, frau präsidentin, ein schreiben an den sri lanka zu schreiben, das den bedauern des parlaments seinen bedauern und den anderen gewalttetzen in sri lanka zum ausdruck bringt und ihr alles möglich macht, um eine friedliche aussöhnung zu einer sehr schwierigen situation zu suchen?', exp: 'wäre es angemessen, wenn sie, frau präsidentin, der präsidentin von sri lanka in einem schreiben das bedauern des parlaments zum gewaltsamen tod von herrn ponnambalam und anderen bürgern von sri lanka übermitteln und sie auffordern würden, alles in ihrem kräften stehende zu tun, um nach einer friedlichen lösung dieser sehr schwierigen situation zu suchen?'\n",
      "Original 'yes, mr evans, i feel an initiative of the type you have just suggested would be entirely appropriate.', got 'ja, herr evans, ich halte eine initiative der art, die sie gerade vorgeschlagen haben, wäre völlig angemessen.', exp: 'ja, herr evans, ich denke, daß eine derartige initiative durchaus angebracht ist.'\n",
      "Original 'if the house agrees, i shall do as mr evans has suggested.', got 'wenn das parlament einverstanden ist, werde ich, wie herr evans vorgeschlagen hat.', exp: 'wenn das haus damit einverstanden ist, werde ich dem vorschlag von herrn evans folgen.'\n",
      "Original 'madam president, on a point of order.', got 'frau präsidentin! zur geschäftsordnung.', exp: 'frau präsidentin, zur geschäftsordnung.'\n",
      "Original 'i would like your advice about rule 0 concerning inadmissibility.', got 'ich möchte, dass ihre ratspräsidentschaft über artikel 0 bezüglich der unzulässigkeit ist.', exp: 'könnten sie mir eine auskunft zu artikel 0 im zusammenhang mit der unzulässigkeit geben?'\n",
      "Original 'my question relates to something that will come up on thursday and which i will then raise again.', got 'meine frage bezieht sich auf etwas, was am donnerstag stattfinden wird und die ich wiederholen werde.', exp: 'meine frage betrifft eine angelegenheit, die am donnerstag zur sprache kommen wird und auf die ich dann erneut verweisen werde.'\n",
      "Original 'the cunha report on multiannual guidance programmes comes before parliament on thursday and contains a proposal in paragraph 0 that a form of quota penalties should be introduced for countries which fail to meet their fleet reduction targets annually.', got 'der bericht cunha über mehrjährige leitlinien ist vor dem parlament am donnerstag, und enthält einen vorschlag in ziffer 0, dass eine form von quotenstrafen für länder eingeführt werden soll, die ihre flottenziele jährlich nicht erfüllen.', exp: 'das parlament wird sich am donnerstag mit dem cunha-bericht über mehrjährige ausrichtungsprogramme befassen, der in absatz 0 vorschlägt, daß länder, die ihr soll zur flottenverkleinerung nicht erfüllen, jährlich mit einer art quotenstrafe belegt werden sollen.'\n",
      "Original 'it says that this should be done despite the principle of relative stability.', got 'es heißt, dies sollte trotz des grundsatzes der relativen stabilität erfolgen.', exp: 'und zwar sollen derartige strafen trotz des grundsatzes der relativen stabilität verhängt werden.'\n",
      "Original 'i believe that the principle of relative stability is a fundamental legal principle of the common fisheries policy and a proposal to subvert it would be legally inadmissible.', got 'ich glaube, dass der grundsatz der relativen stabilität ein grundlegender rechtsgrundsatz der gemeinsamen fischereipolitik ist und einen vorschlag zur unterbreibung für die werbung darstellt.', exp: 'ich meine, daß der grundsatz der relativen stabilität einen elementaren rechtsgrundsatz der gemeinsamen fischereipolitik darstellt und ein vorschlag, diesen zu unterlaufen, rechtlich unzulässig wäre.'\n",
      "Original 'i want to know whether one can raise an objection of that kind to what is merely a report, not a legislative proposal, and whether that is something i can competently do on thursday.', got 'ich möchte wissen, ob man eine einwände einsetzen kann, die nur einen bericht, nicht einen legislativvorschlag, sondern ob das, was ich am donnerstag zu tun kann.', exp: 'ich möchte wissen, ob es möglich ist, einen einwand gegen ein dokument zu erheben, bei dem es sich lediglich um einen bericht und keinen legislativvorschlag handelt, und ob ich befugt bin, dies am donnerstag zu tun.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on training set:\n",
    "for en, de in europarl.df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-11T14:02:40.296271Z",
     "start_time": "2018-07-11T14:02:37.182434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'it is important not to underestimate the work involved.', got 'es ist wichtig, die arbeitnehmer zu unterschätzen.', exp: 'das sollte man nicht unterschätzen.'\n",
      "Original 'mr vanhanen, you were mr calm, and i think you, mr tuomioja, were mr collected.', got 'herr vanhanen, sie waren herrn calders, und ich denke, sie sind herrn tuomioja, herrn kollege.', exp: 'herr vanhanen, sie waren mr. calm, und, ich denke, sie, herr tuomioja, waren mr. collected.'\n",
      "Original \"most members of this parliament are aware of the commission's efforts to make sure that the european support for the palestinian authority is money that is properly spent, well spent and spent in ways that help to promote pluralism, the rule of law and clean government in the palestinian territories.\", got 'die meisten mitglieder dieses parlaments sind der bemühungen der kommission bekannt, sicherzustellen, dass die europäische unterstützung für die palästinensische autonomiebehörde geld, das in den palästinensischen gebieten in den palästinensischen gebieten angemessen ausgegeben wird, ausgegeben und ausgegeben wird, um den pluralismus, die rechtsstaatlichkeit und saubere regierung zu fördern.', exp: 'die meisten abgeordneten dieses parlaments wissen um die bemühungen der kommission, dafür zu sorgen, dass die hilfe der eu für die palästinensische autonomiebehörde gut angelegt ist, dass die bereitgestellten mittel für zwecke ausgegeben werden, die der förderung des pluralismus, der rechtsstaatlichkeit und der bekämpfung der korruption in den palästinensischen gebieten dienen.'\n",
      "Original 'i therefore advocate that the eu and russia must step up negotiations on a new partnership and cooperation agreement based on the mutual interdependence of the eu and russia.', got 'ich befürworte daher, dass die eu und russland die verhandlungen über ein neues partnerschafts- und kooperationsabkommen, das auf der gegenseitigen wechselseitigen abhängigkeit der eu und russland basiert, verstärken müssen.', exp: 'deswegen befürworte ich, dass die eu und russland ihre verhandlungen über ein neues partnerschafts- und kooperationsabkommen auf der grundlage der gegenseitigen abhängigkeit der eu und russland intensivieren.'\n",
      "Original 'the work to be done on logistics does not involve regulating this rapidly growing sector, but ensuring that it has a sustainable future by allowing it to mobilise the efficiency potential that still exists in the transport business.', got 'die arbeit, die auf logistik getan wird, ist nicht die regelung dieses rasch wachsenden sektors, sondern gewährleistet, dass sie eine nachhaltige zukunft hat, damit sie das effizienzpotenzial, das noch im transportbereich existiert, zu mobilisieren.', exp: 'ziel der anstrengungen auf dem gebiet der logistik ist es nicht, diesen wachstumssektor zu reglementieren, sondern ihm eine nachhaltige zukunft zu sichern, indem er die möglichkeit erhält, das noch im verkehrssektor vorhandene effizienzpotenzial zu mobilisieren.'\n",
      "Original 'nevertheless a comparison of national legislations would be an important factor when we adopt common rules at european level.', got 'dennoch wäre ein vergleich der nationalen rechtsvorschriften ein wichtiger faktor, wenn wir gemeinsame regeln auf europäischer ebene verabschieden.', exp: 'trotzdem wäre ein vergleich der nationalen rechtsvorschriften ein wichtiger faktor, wenn auf europäischer ebene gemeinsame regelungen verabschiedet werden.'\n",
      "Original \"from this point of view, we accept the commission proposal in that - and this should be made very clear here - the genuine political debate will come about when we see what the council makes of the commission's preliminary draft budget.\", got 'aus diesem sicht akzeptieren wir den vorschlag der kommission, daß - und das sollte hier sehr deutlich gemacht werden - die wirkliche politische debatte über die wirkliche politische debatte kommen wird, wenn wir sehen, was der rat den vorentwurf des hausentwurfs der kommission erstellt hat.', exp: 'aus dieser sicht heraus machen wir uns den vorschlag der kommission in dem bewußtsein - und das wollen wir hier klarstellen - zu eigen, daß die wirkliche politische debatte erst dann beginnt, wenn feststeht, wie der rat den haushaltsvorentwurf der kommission aufnimmt.'\n",
      "Original 'this is also the main reason why the council took so long to adopt this framework decision.', got 'dies ist auch der hauptgrund, warum der rat so lange so lange getan hat, diesen rahmenbeschluss zu verabschieden.', exp: 'das ist auch der hauptgrund dafür, weshalb die annahme des rahmenbeschlusses durch den rat so lange gedauert hat.'\n",
      "Original '(the oral amendment was adopted)', got '(der mündliche änderungsantrag wurde angenommen.)', exp: '(der mündliche änderungsantrag wird angenommen)'\n",
      "Original 'the adoption of this text now enables us to express our initial position.', got 'die annahme dieses textes ermöglicht uns, unsere ursprüngliche position zum ausdruck zu bringen.', exp: 'wenn wir den text jetzt verabschieden, können wir unsere anfängliche haltung zum ausdruck bringen.'\n",
      "Original 'if new priorities are to be added each year, further resources must also be provided.', got 'wenn neue prioritäten hinzugefügt werden, müssen auch weitere rourcen bereitgestellt werden.', exp: 'wenn wir jedes jahr neue prioritäten hinzufügen wollen, müssen wir auch neue ressourcen zur verfügung stellen.'\n",
      "Original 'for the sake of peace and for europe, i very much hope that it can be put into practice and that it will be successful.', got 'für den frieden und für europa hoffe ich sehr, dass sie in der praxis umgesetzt werden kann und dass sie erfolgreich sein wird.', exp: 'für den frieden und für europa ist es mein dringlicher wunsch, daß dieses beispiel konkrete gestalt annehmen und zu einem erfolg führen möge.'\n",
      "Original 'sixty per cent of them said that they want the uk to renegotiate our relationship with the european union to a simple free trade agreement and no more than that.', got 'sechzig prozent der ihnen gesagt haben, dass sie das vereinigte königreich verhandeln, um unsere beziehungen zur europäischen union zu einem einfachen freihandelsabkommen zu verhandeln.', exp: 'sechzig prozent der befragten firmen forderten, dass das vereinigte königreich seine beziehungen zur europäischen union dahin gehend ändern soll, dass nur noch ein einfaches freihandelsabkommen mit ihr besteht und nicht mehr.'\n",
      "Original 'over and again, we have said that it needs to be done by relaunching the lisbon strategy, but that strategy will continue to fade away if we do not invest seriously and forcefully in industrial policy.', got 'darüber hinaus haben wir gesagt, dass sie durch die wiederaufnahme der lissabon-strategie getan werden muss, aber diese strategie wird weiterentwickeln, wenn wir in der industriepolitik ernsthaft investieren.', exp: 'wir haben immer wieder betont, dass das durch eine wiederbelebung der lissabon-strategie geschehen muss, doch diese strategie wird weiter dahinschwinden, wenn wir nicht ernsthaft und energisch in die industriepolitik investieren.'\n",
      "Original 'secondly, we gave this matter very careful consideration at the time we tabled the resolution and we are on no account prepared to withdraw it.', got 'zweitens haben wir diese angelegenheit sehr sorgfältig berücksichtigt, da wir die entschließung eingereicht haben und wir nicht bereit sind, ihn zurückzuziehen.', exp: 'zweitens haben wir den zeitpunkt, zu dem wir diese sache eingebracht haben, sorgfältig erwogen und sind auf keinen fall bereit, die entschließung zurückzuziehen.'\n",
      "Original 'this is a country which has enormous potential - and our readiness to engage with iran constructively has been made over and over.', got 'das ist ein land, das enorme potenzial besitzt - und unsere bereitschaft, mit dem iran konstruktiv zu machen - wurde überall abgegeben.', exp: 'dies ist ein land, welches über ein enormes potenzial verfügt - und unsere bereitschaft, uns konstruktiv auf den iran einzulassen, ist immer wieder gezeigt worden.'\n",
      "Original 'sixthly, the rules which mr solana imposed last year in august without any consultation now fall under this regulation and will also be verified against the same.', got 'sechstens, die regelungen, die herr solana im august letzten jahres im august im letzten jahres im august verabschiedet hat, wird jetzt in dieser verordnung unterliegt und auch gegen das gleiche verhandelt.', exp: 'sechstens: die vorschriften, die herr solana im august vergangenen jahres ohne rücksprache durchgesetzt hat, fallen nunmehr unter diese verordnung und werden auch daran geprüft.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'the debate is not therefore whether we are in favour of or opposed to the alternative methods.', got 'die debatte ist nicht, ob wir für die alternativen methoden stimmen oder gegen die alternativen methoden stimmen.', exp: 'es geht also nicht darum, ob wir für oder gegen alternativmethoden sind.'\n",
      "Original 'in conclusion, i would say that only a realistic policy, appropriate to the needs of the population, the environment and an increasingly high-quality market, is capable of achieving the objectives which we think the european union should be aiming at in terms of viticultural policy.', got 'abschließend möchte ich sagen, dass nur eine realistische politik, die den bedürfnissen der bevölkerung, der umwelt und einem zunehmenden qualitativ hochwertigen markt angemessen ist, in der lage ist, die ziele zu erreichen, die wir unserer meinung nach auf die vititikpolitik richten sollten.', exp: 'abschließend möchte ich feststellen, daß nur eine realistische, auf die bedürfnisse der menschen, der umwelt und den zunehmend qualitätsbetonten markt abgestimmte politik geeignet ist, die ziele, die wir im bereich des weinbaus in der europäischen union politisch verankern wollen, zu verwirklichen.'\n"
     ]
    }
   ],
   "source": [
    "# Performance on validation set\n",
    "val_df = europarl.df.iloc[val_ids]\n",
    "for en, de in val_df[['input_texts', 'target_texts']][1:20].values.tolist():\n",
    "    print(f\"Original {en!r}, got {predict(en)!r}, exp: {de!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-11T14:08:30.568635Z",
     "start_time": "2018-07-11T14:02:40.297956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf6123bcb514d298741be6931d8f4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.204563636855427\n"
     ]
    }
   ],
   "source": [
    "bleu = bleu_scores_europarl(\n",
    "    input_texts=europarl.df.input_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    target_texts=europarl.df.target_texts.iloc[val_ids[:TEST_SIZE]],\n",
    "    predict=lambda text: predict(text)\n",
    ")\n",
    "print(f'average BLEU on test set = {bleu.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-11T14:44:10.991971Z",
     "start_time": "2018-07-11T14:13:45.933984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add7bc14aae64b4baa2b2cf187421a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average BLEU on test set = 0.20097956867751898\n"
     ]
    }
   ],
   "source": [
    "# Checking on a bigger test size\n",
    "BIGGER_TEST_SIZE = 12500  # 5 * TEST_SIZE\n",
    "bleu = bleu_scores_europarl(\n",
    "    input_texts=europarl.df.input_texts.iloc[val_ids[:BIGGER_TEST_SIZE]],\n",
    "    target_texts=europarl.df.target_texts.iloc[val_ids[:BIGGER_TEST_SIZE]],\n",
    "    predict=lambda text: predict(text)\n",
    ")\n",
    "print(f'average BLEU on test set = {bleu.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-13T18:17:20.995932Z",
     "start_time": "2018-05-13T18:17:20.994111Z"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "There is only a marginal improvement ($0.202 > 0.199$) while the model needed twice as much time to train. And it's not clear that even the tiny improvement is not a product of the hyperparameter adjustments. I'll need to rerun this experiment with weight decay to check whether 2 layers can be at least a significant (even if it is small improvement).\n",
    "\n",
    "Of course, even small improvements could add to a solid improvement. Google uses for its NMT model 8 layers (that can in parallel on one machine with 8 GPUs), so there might be potential anyway. But of course for a side project with only one GPU access, that would not worth to go on in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 294,
   "position": {
    "height": "40px",
    "left": "553px",
    "right": "192px",
    "top": "132px",
    "width": "615px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
